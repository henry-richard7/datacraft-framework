{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Datacraft Framework","text":""},{"location":"#i-about","title":"\u2139\ufe0f About","text":"<p>Datacraft Framework is a high-performance, modular data processing framework designed to streamline and standardize dataset onboarding for modern data lakehouses. Built on Polars, a blazing-fast DataFrame library, the framework emphasizes speed, scalability, and clarity.</p> <p>It follows the Medallion Architecture with distinct Bronze, Silver, and Gold layers:</p> <ul> <li> <p>Bronze Layer: Ingests raw data from diverse sources (e.g., files, APIs, databases) with minimal processing.</p> </li> <li> <p>Silver Layer: Performs schema alignment, type casting, and robust data quality management (DQM) including null checks, pattern matching, and threshold validations.</p> </li> <li> <p>Gold Layer: Applies business logic, joins, and advanced transformations to produce refined, analytics-ready datasets.</p> </li> </ul>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p>\u2705 Schema-driven configuration using sqlmodel control tables for declarative onboarding.</p> </li> <li> <p>\ud83d\ude80 Ultra-fast processing using Polars, optimized for both in-memory and out-of-core execution.</p> </li> <li> <p>\ud83d\udce6 Plug-and-play connectors for file systems, SFTP, S3, and SQL-based sources.</p> </li> <li> <p>\ud83d\udee1 Built-in DQM engine for enforcing quality rules at the column level.</p> </li> <li> <p>\ud83d\udcca Data lineage tracking and logging using structured audit tables.</p> </li> <li> <p>\ud83d\udd01 Parallel-safe transformations suitable for large-scale distributed workflows.</p> </li> </ul>"},{"location":"#ideal-use-cases","title":"\ud83e\udde9 Ideal Use Cases","text":"<ul> <li>Fast prototyping and standardization of new data sources</li> </ul>"},{"location":"Common/DataProcessor/","title":"Documentation for <code>DataProcessor</code>","text":""},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor","title":"<code>datacraft_framework.Common.DataProcessor</code>","text":""},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.BronzeInboundWriter","title":"<code>BronzeInboundWriter</code>","text":"<p>Writes input data to a CSV or TXT file at the specified location.</p> <p>This class supports writing either a Polars DataFrame or a list of dictionaries to a <code>.csv</code> or <code>.txt</code> file using a specified delimiter. If the input is a dictionary list, it will be automatically converted to a Polars DataFrame before writing.</p> <p>Attributes:</p> Name Type Description <code>input_data</code> <code>Union[DataFrame, List[Dict]]</code> <p>Data to write to file.</p> <code>save_location</code> <code>str</code> <p>Full path including filename where the file should be saved.</p> <code>outbound_file_delimiter</code> <code>str</code> <p>Delimiter used when writing the file (e.g., ',', '       ').</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported file format is provided in <code>save_location</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n&gt;&gt;&gt; writer = BronzeInboundWriter(input_data=data, save_location=\"/path/to/output.csv\", outbound_file_delimiter=\",\")\n</code></pre> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>class BronzeInboundWriter:\n    \"\"\"\n    Writes input data to a CSV or TXT file at the specified location.\n\n    This class supports writing either a Polars DataFrame or a list of dictionaries\n    to a `.csv` or `.txt` file using a specified delimiter. If the input is a dictionary list,\n    it will be automatically converted to a Polars DataFrame before writing.\n\n    Attributes:\n        input_data (Union[polars.DataFrame, List[Dict]]): Data to write to file.\n        save_location (str): Full path including filename where the file should be saved.\n        outbound_file_delimiter (str): Delimiter used when writing the file (e.g., ',', '\\t').\n\n    Raises:\n        ValueError: If an unsupported file format is provided in `save_location`.\n\n    Examples:\n            &gt;&gt;&gt; data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n            &gt;&gt;&gt; writer = BronzeInboundWriter(input_data=data, save_location=\"/path/to/output.csv\", outbound_file_delimiter=\",\")\n    \"\"\"\n\n    def __init__(\n        self,\n        input_data: Union[polars.DataFrame, list[dict]],\n        save_location: str,\n        outbound_file_delimiter: str,\n    ):\n        \"\"\"\n        Write input data to a CSV or TXT file at the specified location.\n\n        This constructor checks the file extension in `save_location` (e.g., .csv or .txt)\n        and writes the provided `input_data` using Polars. If the input is a dictionary list,\n        it will be converted to a DataFrame before writing.\n\n        Args:\n            input_data (Union[polars.DataFrame, list[dict]]): The data to write to file.\n                Can be either a Polars DataFrame or a list of dictionaries.\n            save_location (str): Full path (including filename) where the file should be saved.\n                Supported extensions: `.csv`, `.txt`.\n            outbound_file_delimiter (str): Delimiter to use when writing the file (e.g., ',', '\\t').\n\n        Raises:\n            ValueError: If an unsupported file format is provided in `save_location`.\n\n        Examples:\n            &gt;&gt;&gt; data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n            &gt;&gt;&gt; writer = BronzeInboundWriter(input_data=data, save_location=\"/path/to/output.csv\", outbound_file_delimiter=\",\")\n        \"\"\"\n        if \"csv\" in save_location or \"txt\" in save_location:\n\n            if isinstance(input_data, polars.DataFrame):\n                input_data.write_csv(\n                    file=save_location,\n                    separator=outbound_file_delimiter,\n                    storage_options=storage_options,\n                )\n\n            else:\n                df = polars.DataFrame(input_data)\n                df.write_csv(\n                    file=save_location,\n                    separator=outbound_file_delimiter,\n                    storage_options=storage_options,\n                )\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.BronzeInboundWriter.__init__","title":"<code>__init__(input_data, save_location, outbound_file_delimiter)</code>","text":"<p>Write input data to a CSV or TXT file at the specified location.</p> <p>This constructor checks the file extension in <code>save_location</code> (e.g., .csv or .txt) and writes the provided <code>input_data</code> using Polars. If the input is a dictionary list, it will be converted to a DataFrame before writing.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[DataFrame, list[dict]]</code> <p>The data to write to file. Can be either a Polars DataFrame or a list of dictionaries.</p> required <code>save_location</code> <code>str</code> <p>Full path (including filename) where the file should be saved. Supported extensions: <code>.csv</code>, <code>.txt</code>.</p> required <code>outbound_file_delimiter</code> <code>str</code> <p>Delimiter to use when writing the file (e.g., ',', ' ').</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported file format is provided in <code>save_location</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n&gt;&gt;&gt; writer = BronzeInboundWriter(input_data=data, save_location=\"/path/to/output.csv\", outbound_file_delimiter=\",\")\n</code></pre> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>def __init__(\n    self,\n    input_data: Union[polars.DataFrame, list[dict]],\n    save_location: str,\n    outbound_file_delimiter: str,\n):\n    \"\"\"\n    Write input data to a CSV or TXT file at the specified location.\n\n    This constructor checks the file extension in `save_location` (e.g., .csv or .txt)\n    and writes the provided `input_data` using Polars. If the input is a dictionary list,\n    it will be converted to a DataFrame before writing.\n\n    Args:\n        input_data (Union[polars.DataFrame, list[dict]]): The data to write to file.\n            Can be either a Polars DataFrame or a list of dictionaries.\n        save_location (str): Full path (including filename) where the file should be saved.\n            Supported extensions: `.csv`, `.txt`.\n        outbound_file_delimiter (str): Delimiter to use when writing the file (e.g., ',', '\\t').\n\n    Raises:\n        ValueError: If an unsupported file format is provided in `save_location`.\n\n    Examples:\n        &gt;&gt;&gt; data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n        &gt;&gt;&gt; writer = BronzeInboundWriter(input_data=data, save_location=\"/path/to/output.csv\", outbound_file_delimiter=\",\")\n    \"\"\"\n    if \"csv\" in save_location or \"txt\" in save_location:\n\n        if isinstance(input_data, polars.DataFrame):\n            input_data.write_csv(\n                file=save_location,\n                separator=outbound_file_delimiter,\n                storage_options=storage_options,\n            )\n\n        else:\n            df = polars.DataFrame(input_data)\n            df.write_csv(\n                file=save_location,\n                separator=outbound_file_delimiter,\n                storage_options=storage_options,\n            )\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTableWriter","title":"<code>DeltaTableWriter</code>","text":"Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>class DeltaTableWriter:\n    def __init__(\n        self,\n        input_data: Union[polars.DataFrame, list[dict], str],\n        save_location: str,\n        batch_id: int,\n        partition_columns: str,\n        outbound_file_delimiter: Optional[str] = None,\n        infer_schema: Optional[bool] = False,\n    ):\n        \"\"\"\n        Write data to a Delta Lake table with optional partitioning and batch ID tagging.\n\n        This class supports writing to Delta Lake from the following sources:\n        - Polars DataFrame\n        - List of dictionaries (converted into DataFrame)\n        - CSV/TXT file path (read into DataFrame)\n\n        A `batch_id` column is added to all records for traceability. Data is written in append mode.\n\n        Args:\n            input_data (Union[polars.DataFrame, list[dict], str]): The data to write.\n                Can be a DataFrame, list of dicts, or a file path to CSV/TXT.\n            save_location (str): Target location where the Delta table will be saved.\n                Supports local paths or cloud storage paths (e.g., S3).\n            batch_id (int): Batch ID to tag all rows with for tracking purposes.\n            partition_columns (str): Comma-separated string of columns to partition by.\n            outbound_file_delimiter (Optional[str], optional): Delimiter used in the source file\n                (if `input_data` is a file path). Defaults to None.\n            infer_schema (Optional[bool], optional): Whether to infer schema when reading CSV/TXT.\n                Defaults to False.\n\n        Raises:\n            ValueError: If unsupported file format is provided or required parameters are missing.\n\n        Examples:\n            &gt;&gt;&gt; writer = DeltaTableWriter(\n            ...     input_data=\"data.csv\",\n            ...     save_location=\"s3://bucket/delta_table\",\n            ...     batch_id=123,\n            ...     partition_columns=\"country,date\",\n            ...     outbound_file_delimiter=\",\"\n            ... )\n        \"\"\"\n\n        if isinstance(input_data, polars.DataFrame):\n            input_data = input_data.with_columns(polars.lit(batch_id).alias(\"batch_id\"))\n            input_data.write_delta(\n                save_location,\n                storage_options=storage_options,\n                mode=\"append\",\n                delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n            )\n        elif isinstance(input_data, str):\n            if \"csv\" in input_data or \"txt\" in input_data:\n                df = polars.read_csv(\n                    input_data,\n                    separator=outbound_file_delimiter,\n                    infer_schema=infer_schema,\n                    storage_options={\n                        \"key\": aws_key,\n                        \"secret\": aws_secret,\n                        \"client_kwargs\": {\"endpoint_url\": aws_endpoint},\n                    },\n                )\n                df = df.with_columns(polars.lit(batch_id).alias(\"batch_id\"))\n                df.write_delta(\n                    save_location,\n                    storage_options=storage_options,\n                    mode=\"append\",\n                    delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n                )\n\n        else:\n            df = polars.DataFrame(input_data)\n            df.with_columns(polars.lit(batch_id).alias(\"batch_id\"))\n            df.write_delta(\n                file=save_location,\n                storage_options=storage_options,\n                mode=\"append\",\n                delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n            )\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTableWriter.__init__","title":"<code>__init__(input_data, save_location, batch_id, partition_columns, outbound_file_delimiter=None, infer_schema=False)</code>","text":"<p>Write data to a Delta Lake table with optional partitioning and batch ID tagging.</p> <p>This class supports writing to Delta Lake from the following sources: - Polars DataFrame - List of dictionaries (converted into DataFrame) - CSV/TXT file path (read into DataFrame)</p> <p>A <code>batch_id</code> column is added to all records for traceability. Data is written in append mode.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[DataFrame, list[dict], str]</code> <p>The data to write. Can be a DataFrame, list of dicts, or a file path to CSV/TXT.</p> required <code>save_location</code> <code>str</code> <p>Target location where the Delta table will be saved. Supports local paths or cloud storage paths (e.g., S3).</p> required <code>batch_id</code> <code>int</code> <p>Batch ID to tag all rows with for tracking purposes.</p> required <code>partition_columns</code> <code>str</code> <p>Comma-separated string of columns to partition by.</p> required <code>outbound_file_delimiter</code> <code>Optional[str]</code> <p>Delimiter used in the source file (if <code>input_data</code> is a file path). Defaults to None.</p> <code>None</code> <code>infer_schema</code> <code>Optional[bool]</code> <p>Whether to infer schema when reading CSV/TXT. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unsupported file format is provided or required parameters are missing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; writer = DeltaTableWriter(\n...     input_data=\"data.csv\",\n...     save_location=\"s3://bucket/delta_table\",\n...     batch_id=123,\n...     partition_columns=\"country,date\",\n...     outbound_file_delimiter=\",\"\n... )\n</code></pre> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>def __init__(\n    self,\n    input_data: Union[polars.DataFrame, list[dict], str],\n    save_location: str,\n    batch_id: int,\n    partition_columns: str,\n    outbound_file_delimiter: Optional[str] = None,\n    infer_schema: Optional[bool] = False,\n):\n    \"\"\"\n    Write data to a Delta Lake table with optional partitioning and batch ID tagging.\n\n    This class supports writing to Delta Lake from the following sources:\n    - Polars DataFrame\n    - List of dictionaries (converted into DataFrame)\n    - CSV/TXT file path (read into DataFrame)\n\n    A `batch_id` column is added to all records for traceability. Data is written in append mode.\n\n    Args:\n        input_data (Union[polars.DataFrame, list[dict], str]): The data to write.\n            Can be a DataFrame, list of dicts, or a file path to CSV/TXT.\n        save_location (str): Target location where the Delta table will be saved.\n            Supports local paths or cloud storage paths (e.g., S3).\n        batch_id (int): Batch ID to tag all rows with for tracking purposes.\n        partition_columns (str): Comma-separated string of columns to partition by.\n        outbound_file_delimiter (Optional[str], optional): Delimiter used in the source file\n            (if `input_data` is a file path). Defaults to None.\n        infer_schema (Optional[bool], optional): Whether to infer schema when reading CSV/TXT.\n            Defaults to False.\n\n    Raises:\n        ValueError: If unsupported file format is provided or required parameters are missing.\n\n    Examples:\n        &gt;&gt;&gt; writer = DeltaTableWriter(\n        ...     input_data=\"data.csv\",\n        ...     save_location=\"s3://bucket/delta_table\",\n        ...     batch_id=123,\n        ...     partition_columns=\"country,date\",\n        ...     outbound_file_delimiter=\",\"\n        ... )\n    \"\"\"\n\n    if isinstance(input_data, polars.DataFrame):\n        input_data = input_data.with_columns(polars.lit(batch_id).alias(\"batch_id\"))\n        input_data.write_delta(\n            save_location,\n            storage_options=storage_options,\n            mode=\"append\",\n            delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n        )\n    elif isinstance(input_data, str):\n        if \"csv\" in input_data or \"txt\" in input_data:\n            df = polars.read_csv(\n                input_data,\n                separator=outbound_file_delimiter,\n                infer_schema=infer_schema,\n                storage_options={\n                    \"key\": aws_key,\n                    \"secret\": aws_secret,\n                    \"client_kwargs\": {\"endpoint_url\": aws_endpoint},\n                },\n            )\n            df = df.with_columns(polars.lit(batch_id).alias(\"batch_id\"))\n            df.write_delta(\n                save_location,\n                storage_options=storage_options,\n                mode=\"append\",\n                delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n            )\n\n    else:\n        df = polars.DataFrame(input_data)\n        df.with_columns(polars.lit(batch_id).alias(\"batch_id\"))\n        df.write_delta(\n            file=save_location,\n            storage_options=storage_options,\n            mode=\"append\",\n            delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n        )\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTablePublishWrite","title":"<code>DeltaTablePublishWrite</code>","text":"<p>A class to publish a Polars DataFrame to a Delta Lake table with optional batch ID tagging.</p> <p>This class writes a DataFrame to a Delta Lake table, supporting: - Partitioning by one or more columns - Optional addition of a <code>batch_id</code> column for traceability - Overwrite mode for idempotent publishing</p> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>class DeltaTablePublishWrite:\n    \"\"\"\n    A class to publish a Polars DataFrame to a Delta Lake table with optional batch ID tagging.\n\n    This class writes a DataFrame to a Delta Lake table, supporting:\n    - Partitioning by one or more columns\n    - Optional addition of a `batch_id` column for traceability\n    - Overwrite mode for idempotent publishing\n    \"\"\"\n\n    def __init__(\n        self,\n        input_data: polars.DataFrame,\n        save_location: str,\n        partition_columns: str,\n        batch_id: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize the Delta table write operation.\n\n        Writes the provided DataFrame to the specified Delta Lake location. If a `batch_id`\n        is provided, it adds a new column to the DataFrame before writing.\n\n        Args:\n            input_data (polars.DataFrame): The DataFrame to be written to Delta Lake.\n            save_location (str): Target location where the Delta table will be saved.\n            partition_columns (str): Comma-separated string of columns to partition by.\n            batch_id (Optional[int], optional): Optional batch identifier added as a column.\n                Defaults to None.\n\n        Examples:\n            &gt;&gt;&gt; df = pl.DataFrame({\"name\": [\"Alice\", \"Bob\"], \"age\": [30, 25]})\n            &gt;&gt;&gt; writer = DeltaTablePublishWrite(\n            ...     input_data=df,\n            ...     save_location=\"s3://bucket/published_table\",\n            ...     partition_columns=\"age\",\n            ...     batch_id=123\n            ... )\n        \"\"\"\n        if not batch_id:\n            input_data.write_delta(\n                save_location,\n                storage_options=storage_options,\n                mode=\"overwrite\",\n                delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n            )\n        else:\n            input_data.with_columns(polars.lit(batch_id).alias(\"batch_id\")).write_delta(\n                save_location,\n                storage_options=storage_options,\n                mode=\"overwrite\",\n                delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n            )\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTablePublishWrite.__init__","title":"<code>__init__(input_data, save_location, partition_columns, batch_id=None)</code>","text":"<p>Initialize the Delta table write operation.</p> <p>Writes the provided DataFrame to the specified Delta Lake location. If a <code>batch_id</code> is provided, it adds a new column to the DataFrame before writing.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>DataFrame</code> <p>The DataFrame to be written to Delta Lake.</p> required <code>save_location</code> <code>str</code> <p>Target location where the Delta table will be saved.</p> required <code>partition_columns</code> <code>str</code> <p>Comma-separated string of columns to partition by.</p> required <code>batch_id</code> <code>Optional[int]</code> <p>Optional batch identifier added as a column. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"name\": [\"Alice\", \"Bob\"], \"age\": [30, 25]})\n&gt;&gt;&gt; writer = DeltaTablePublishWrite(\n...     input_data=df,\n...     save_location=\"s3://bucket/published_table\",\n...     partition_columns=\"age\",\n...     batch_id=123\n... )\n</code></pre> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>def __init__(\n    self,\n    input_data: polars.DataFrame,\n    save_location: str,\n    partition_columns: str,\n    batch_id: Optional[int] = None,\n):\n    \"\"\"\n    Initialize the Delta table write operation.\n\n    Writes the provided DataFrame to the specified Delta Lake location. If a `batch_id`\n    is provided, it adds a new column to the DataFrame before writing.\n\n    Args:\n        input_data (polars.DataFrame): The DataFrame to be written to Delta Lake.\n        save_location (str): Target location where the Delta table will be saved.\n        partition_columns (str): Comma-separated string of columns to partition by.\n        batch_id (Optional[int], optional): Optional batch identifier added as a column.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; df = pl.DataFrame({\"name\": [\"Alice\", \"Bob\"], \"age\": [30, 25]})\n        &gt;&gt;&gt; writer = DeltaTablePublishWrite(\n        ...     input_data=df,\n        ...     save_location=\"s3://bucket/published_table\",\n        ...     partition_columns=\"age\",\n        ...     batch_id=123\n        ... )\n    \"\"\"\n    if not batch_id:\n        input_data.write_delta(\n            save_location,\n            storage_options=storage_options,\n            mode=\"overwrite\",\n            delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n        )\n    else:\n        input_data.with_columns(polars.lit(batch_id).alias(\"batch_id\")).write_delta(\n            save_location,\n            storage_options=storage_options,\n            mode=\"overwrite\",\n            delta_write_options={\"partition_by\": partition_columns.split(\",\")},\n        )\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTableRead","title":"<code>DeltaTableRead</code>","text":"<p>A class to read data from a Delta Lake table with optional filtering by batch ID or latest version.</p> <p>This class provides flexible reading capabilities from a Delta Lake table, including: - Reading all data - Filtering by a specific <code>batch_id</code> - Reading only the latest batch of data</p> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>class DeltaTableRead:\n    \"\"\"\n    A class to read data from a Delta Lake table with optional filtering by batch ID or latest version.\n\n    This class provides flexible reading capabilities from a Delta Lake table, including:\n    - Reading all data\n    - Filtering by a specific `batch_id`\n    - Reading only the latest batch of data\n    \"\"\"\n\n    def __init__(\n        self,\n        delta_path: str,\n        batch_id: Optional[int] = None,\n        latest: bool = False,\n    ):\n        \"\"\"\n        Initialize the DeltaTableRead instance with read parameters.\n\n        Args:\n            delta_path (str): Path to the Delta Lake table.\n            batch_id (Optional[int], optional): Specific batch ID to filter data. Defaults to None.\n            latest (bool, optional): Whether to fetch only the latest batch. Defaults to False.\n        \"\"\"\n\n        self.delta_path = delta_path\n        self.batch_id = batch_id\n        self.latest = latest\n\n    def read(self) -&gt; polars.DataFrame:\n        \"\"\"\n        Read data from the Delta Lake table based on the configured parameters.\n\n        If a `batch_id` is provided, it filters the data to only that batch.\n        If `latest` is True, it reads only the latest batch based on the maximum `batch_id`.\n        If neither is specified, it reads the full table.\n\n        Returns:\n            polars.DataFrame: A DataFrame containing the filtered or full Delta Lake table data.\n\n        Raises:\n            FileNotFoundError: If the Delta table at `delta_path` does not exist.\n            Exception: For other generic I/O or Delta table errors during reading.\n        \"\"\"\n\n        if self.batch_id:\n            df = polars.scan_delta(self.delta_path, storage_options=storage_options)\n            df = df.filter(polars.col(\"batch_id\") == self.batch_id)\n            return df.collect()\n        else:\n            if self.latest:\n                df = polars.scan_delta(self.delta_path, storage_options=storage_options)\n                max_batch_id = df.select(polars.col(\"batch_id\").max()).collect().item()\n\n                return df.filter(polars.col(\"batch_id\") == max_batch_id).collect()\n            else:\n                return polars.read_delta(\n                    self.delta_path, storage_options=storage_options\n                )\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTableRead.__init__","title":"<code>__init__(delta_path, batch_id=None, latest=False)</code>","text":"<p>Initialize the DeltaTableRead instance with read parameters.</p> <p>Parameters:</p> Name Type Description Default <code>delta_path</code> <code>str</code> <p>Path to the Delta Lake table.</p> required <code>batch_id</code> <code>Optional[int]</code> <p>Specific batch ID to filter data. Defaults to None.</p> <code>None</code> <code>latest</code> <code>bool</code> <p>Whether to fetch only the latest batch. Defaults to False.</p> <code>False</code> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>def __init__(\n    self,\n    delta_path: str,\n    batch_id: Optional[int] = None,\n    latest: bool = False,\n):\n    \"\"\"\n    Initialize the DeltaTableRead instance with read parameters.\n\n    Args:\n        delta_path (str): Path to the Delta Lake table.\n        batch_id (Optional[int], optional): Specific batch ID to filter data. Defaults to None.\n        latest (bool, optional): Whether to fetch only the latest batch. Defaults to False.\n    \"\"\"\n\n    self.delta_path = delta_path\n    self.batch_id = batch_id\n    self.latest = latest\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTableRead.read","title":"<code>read()</code>","text":"<p>Read data from the Delta Lake table based on the configured parameters.</p> <p>If a <code>batch_id</code> is provided, it filters the data to only that batch. If <code>latest</code> is True, it reads only the latest batch based on the maximum <code>batch_id</code>. If neither is specified, it reads the full table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: A DataFrame containing the filtered or full Delta Lake table data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the Delta table at <code>delta_path</code> does not exist.</p> <code>Exception</code> <p>For other generic I/O or Delta table errors during reading.</p> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>def read(self) -&gt; polars.DataFrame:\n    \"\"\"\n    Read data from the Delta Lake table based on the configured parameters.\n\n    If a `batch_id` is provided, it filters the data to only that batch.\n    If `latest` is True, it reads only the latest batch based on the maximum `batch_id`.\n    If neither is specified, it reads the full table.\n\n    Returns:\n        polars.DataFrame: A DataFrame containing the filtered or full Delta Lake table data.\n\n    Raises:\n        FileNotFoundError: If the Delta table at `delta_path` does not exist.\n        Exception: For other generic I/O or Delta table errors during reading.\n    \"\"\"\n\n    if self.batch_id:\n        df = polars.scan_delta(self.delta_path, storage_options=storage_options)\n        df = df.filter(polars.col(\"batch_id\") == self.batch_id)\n        return df.collect()\n    else:\n        if self.latest:\n            df = polars.scan_delta(self.delta_path, storage_options=storage_options)\n            max_batch_id = df.select(polars.col(\"batch_id\").max()).collect().item()\n\n            return df.filter(polars.col(\"batch_id\") == max_batch_id).collect()\n        else:\n            return polars.read_delta(\n                self.delta_path, storage_options=storage_options\n            )\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTableWriterScdType2","title":"<code>DeltaTableWriterScdType2</code>","text":"<p>A class to perform SCD Type 2 (Slowly Changing Dimension Type 2) upserts into a Delta Lake table.</p> <p>This class writes data from a staging DataFrame into a Delta Lake table using Delta Merge operations. It handles both: - Updating existing records when changes are detected (<code>when_matched_update</code>) - Inserting new records that do not already exist (<code>when_not_matched_insert_all</code>)</p> <p>The implementation assumes the presence of specific columns for versioning: - <code>eff_strt_dt</code>: Effective start date of the record - <code>eff_end_dt</code>: Effective end date of the record (set to '9999-12-31' for active records) - <code>sys_checksum</code>: Column used to detect changes (e.g., hash of relevant fields)</p> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>class DeltaTableWriterScdType2:\n    \"\"\"\n    A class to perform SCD Type 2 (Slowly Changing Dimension Type 2) upserts into a Delta Lake table.\n\n    This class writes data from a staging DataFrame into a Delta Lake table using Delta Merge operations.\n    It handles both:\n    - Updating existing records when changes are detected (`when_matched_update`)\n    - Inserting new records that do not already exist (`when_not_matched_insert_all`)\n\n    The implementation assumes the presence of specific columns for versioning:\n    - `eff_strt_dt`: Effective start date of the record\n    - `eff_end_dt`: Effective end date of the record (set to '9999-12-31' for active records)\n    - `sys_checksum`: Column used to detect changes (e.g., hash of relevant fields)\n    \"\"\"\n\n    def __init__(\n        self, staging_df: polars.DataFrame, delta_path: str, primary_keys: str\n    ):\n        \"\"\"\n        Initialize and execute an SCD Type 2 merge operation on a Delta Lake table.\n\n        Performs two-phase write logic:\n        1. Updates existing active records if they have changed.\n        2. Inserts new records as active entries with `eff_end_dt = '9999-12-31'`.\n\n        Args:\n            staging_df (polars.DataFrame): Incoming data containing staged changes.\n            delta_path (str): Path to the target Delta Lake table.\n            primary_keys (str): SQL-like string expression representing the join condition,\n                e.g., `\"target.id = staging.id\"`.\n\n        Examples:\n            &gt;&gt;&gt; writer = DeltaTableWriterScdType2(\n            ...     staging_df=staging_data,\n            ...     delta_path=\"s3://bucket/table\",\n            ...     primary_keys=\"target.id = staging.id\"\n            ... )\n        \"\"\"\n\n        staging_df.write_delta(\n            delta_path,\n            storage_options=storage_options,\n            mode=\"merge\",\n            delta_merge_options={\n                \"source_alias\": \"staging\",\n                \"target_alias\": \"target\",\n                \"predicate\": f\"target.eff_end_dt == '9999-12-31' AND {primary_keys}\",\n            },\n        ).when_matched_update(\n            predicate=\"target.sys_checksum != staging.sys_checksum\",\n            updates={\n                \"eff_end_dt\": \"staging.eff_strt_dt\",\n                \"sys_del_flg\": \"'Y'\",\n            },\n        ).when_not_matched_insert_all().execute()\n\n        staging_df.write_delta(\n            delta_path,\n            storage_options=storage_options,\n            mode=\"merge\",\n            delta_merge_options={\n                \"source_alias\": \"staging\",\n                \"target_alias\": \"target\",\n                \"predicate\": f\"target.eff_end_dt == '9999-12-31' AND {primary_keys}\",\n            },\n        ).when_not_matched_insert_all().execute()\n</code></pre>"},{"location":"Common/DataProcessor/#datacraft_framework.Common.DataProcessor.DeltaTableWriterScdType2.__init__","title":"<code>__init__(staging_df, delta_path, primary_keys)</code>","text":"<p>Initialize and execute an SCD Type 2 merge operation on a Delta Lake table.</p> <p>Performs two-phase write logic: 1. Updates existing active records if they have changed. 2. Inserts new records as active entries with <code>eff_end_dt = '9999-12-31'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>staging_df</code> <code>DataFrame</code> <p>Incoming data containing staged changes.</p> required <code>delta_path</code> <code>str</code> <p>Path to the target Delta Lake table.</p> required <code>primary_keys</code> <code>str</code> <p>SQL-like string expression representing the join condition, e.g., <code>\"target.id = staging.id\"</code>.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; writer = DeltaTableWriterScdType2(\n...     staging_df=staging_data,\n...     delta_path=\"s3://bucket/table\",\n...     primary_keys=\"target.id = staging.id\"\n... )\n</code></pre> Source code in <code>src/datacraft_framework/Common/DataProcessor.py</code> <pre><code>def __init__(\n    self, staging_df: polars.DataFrame, delta_path: str, primary_keys: str\n):\n    \"\"\"\n    Initialize and execute an SCD Type 2 merge operation on a Delta Lake table.\n\n    Performs two-phase write logic:\n    1. Updates existing active records if they have changed.\n    2. Inserts new records as active entries with `eff_end_dt = '9999-12-31'`.\n\n    Args:\n        staging_df (polars.DataFrame): Incoming data containing staged changes.\n        delta_path (str): Path to the target Delta Lake table.\n        primary_keys (str): SQL-like string expression representing the join condition,\n            e.g., `\"target.id = staging.id\"`.\n\n    Examples:\n        &gt;&gt;&gt; writer = DeltaTableWriterScdType2(\n        ...     staging_df=staging_data,\n        ...     delta_path=\"s3://bucket/table\",\n        ...     primary_keys=\"target.id = staging.id\"\n        ... )\n    \"\"\"\n\n    staging_df.write_delta(\n        delta_path,\n        storage_options=storage_options,\n        mode=\"merge\",\n        delta_merge_options={\n            \"source_alias\": \"staging\",\n            \"target_alias\": \"target\",\n            \"predicate\": f\"target.eff_end_dt == '9999-12-31' AND {primary_keys}\",\n        },\n    ).when_matched_update(\n        predicate=\"target.sys_checksum != staging.sys_checksum\",\n        updates={\n            \"eff_end_dt\": \"staging.eff_strt_dt\",\n            \"sys_del_flg\": \"'Y'\",\n        },\n    ).when_not_matched_insert_all().execute()\n\n    staging_df.write_delta(\n        delta_path,\n        storage_options=storage_options,\n        mode=\"merge\",\n        delta_merge_options={\n            \"source_alias\": \"staging\",\n            \"target_alias\": \"target\",\n            \"predicate\": f\"target.eff_end_dt == '9999-12-31' AND {primary_keys}\",\n        },\n    ).when_not_matched_insert_all().execute()\n</code></pre>"},{"location":"Common/FileNameGenerator/","title":"Documentation for <code>FileNameGenerator</code>","text":""},{"location":"Common/FileNameGenerator/#datacraft_framework.Common.FileNameGenerator.file_name_generator","title":"<code>datacraft_framework.Common.FileNameGenerator.file_name_generator(save_file_name)</code>","text":"<p>Generate a dynamic file name by replacing date placeholders with actual formatted dates.</p> <p>This function replaces specific tokens in the input string with the current date in the corresponding format: - Replaces <code>\"YYYYMMDD\"</code> with today's date in <code>YYYYMMDD</code> format. - Replaces <code>\"YYYYMM\"</code> with today's date in <code>YYYYMM</code> format. - Replaces <code>\"YYYY\"</code> with the current year.</p> <p>The replacement is done in priority order: longest pattern first to avoid partial overlaps.</p> <p>Parameters:</p> Name Type Description Default <code>save_file_name</code> <code>str</code> <p>Original file name that may contain date placeholder tokens.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A new file name with all applicable date tokens replaced by current values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; file_name_generator(\"data_YYYYMMDD.csv\")\n'data_20250405.csv'\n</code></pre> <pre><code>&gt;&gt;&gt; file_name_generator(\"report_YYYYMM.xlsx\")\n'report_202504.xlsx'\n</code></pre> Source code in <code>src/datacraft_framework/Common/FileNameGenerator.py</code> <pre><code>def file_name_generator(save_file_name: str) -&gt; str:\n    \"\"\"\n    Generate a dynamic file name by replacing date placeholders with actual formatted dates.\n\n    This function replaces specific tokens in the input string with the current date in the corresponding format:\n    - Replaces `\"YYYYMMDD\"` with today's date in `YYYYMMDD` format.\n    - Replaces `\"YYYYMM\"` with today's date in `YYYYMM` format.\n    - Replaces `\"YYYY\"` with the current year.\n\n    The replacement is done in priority order: longest pattern first to avoid partial overlaps.\n\n    Args:\n        save_file_name (str): Original file name that may contain date placeholder tokens.\n\n    Returns:\n        str: A new file name with all applicable date tokens replaced by current values.\n\n    Examples:\n        &gt;&gt;&gt; file_name_generator(\"data_YYYYMMDD.csv\")\n        'data_20250405.csv'\n\n        &gt;&gt;&gt; file_name_generator(\"report_YYYYMM.xlsx\")\n        'report_202504.xlsx'\n    \"\"\"\n    today_date = datetime.now()\n\n    if \"YYYYMMDD\" in save_file_name:\n        save_file_name = save_file_name.replace(\n            \"YYYYMMDD\", today_date.strftime(\"%Y%m%d\")\n        )\n    elif \"YYYYMM\" in save_file_name:\n        save_file_name = save_file_name.replace(\n            \"YYYYMM\",\n            today_date.strftime(\"%Y%m\"),\n        )\n    elif \"YYYY\" in save_file_name:\n        save_file_name = save_file_name.replace(\"YYYY\", today_date.strftime(\"%Y\"))\n    else:\n        save_file_name = save_file_name\n\n    return save_file_name\n</code></pre>"},{"location":"Common/JsonDataMapper/","title":"Documentation for <code>JsonDataMapper</code>","text":""},{"location":"Common/JsonDataMapper/#datacraft_framework.Common.JsonDataMapper.JsonDataMapper","title":"<code>datacraft_framework.Common.JsonDataMapper.JsonDataMapper</code>","text":"<p>A class to map and extract data from JSON using JSONPath expressions defined in a mapping.</p> <p>This class takes a JSON data structure and a mapping dictionary where values are JSONPath expressions. It extracts values from the JSON data based on these expressions and transforms the result into a list of dictionaries, suitable for use with DataFrame creation or database insertion.</p> Source code in <code>src/datacraft_framework/Common/JsonDataMapper.py</code> <pre><code>class JsonDataMapper:\n    \"\"\"\n    A class to map and extract data from JSON using JSONPath expressions defined in a mapping.\n\n    This class takes a JSON data structure and a mapping dictionary where values are JSONPath expressions.\n    It extracts values from the JSON data based on these expressions and transforms the result into a list of dictionaries,\n    suitable for use with DataFrame creation or database insertion.\n    \"\"\"\n\n    def __init__(self, mapping: dict, json_data):\n        \"\"\"\n        Initialize the JsonDataMapper with mapping rules and JSON data.\n\n        Args:\n            mapping (Dict[str, str]): Dictionary mapping output keys to JSONPath expressions.\n            json_data (Any): JSON-like data (e.g., dict or list) to be parsed.\n        \"\"\"\n        self.mapping = mapping\n        self.json_data = json_data\n\n    def convert_to_dict(self, data) -&gt; list[dict]:\n        \"\"\"\n        Convert a dictionary of lists into a list of dictionaries.\n\n        This method aligns values by index. If a list is shorter than others, the last value is repeated.\n\n        Args:\n            data (Dict[str, List]): A dictionary where each key maps to a list of extracted values.\n\n        Returns:\n            List[Dict]: A list of dictionaries, each representing one row of aligned data.\n\n        Examples:\n            &gt;&gt;&gt; data = {\"name\": [\"Alice\", \"Bob\"], \"age\": [\"30\", \"25\"]}\n            &gt;&gt;&gt; self.convert_to_dict(data)\n            [{'name': 'Alice', 'age': 30}, {'name': 'Bob', 'age': 25}]\n        \"\"\"\n\n        max_length = max(len(values) for values in data.values())\n        result = []\n        for i in range(max_length):\n            item = {}\n            for key, values in data.items():\n                if i &lt; len(values):\n                    value = values[i]\n                else:\n                    value = values[-1]\n\n                try:\n                    item[key] = int(value)\n                except ValueError:\n                    item[key] = value\n            result.append(item)\n        return result\n\n    def get_mapped_data(self) -&gt; list[dict]:\n        \"\"\"\n        Extract and transform data from JSON based on the provided mapping.\n\n        Uses JSON Path expressions in the mapping to extract values from the JSON data.\n        Converts the result into a list of dictionaries.\n\n        Returns:\n            List[Dict]: A list of dictionaries containing mapped and structured data.\n\n        Examples:\n            &gt;&gt;&gt; mapping = {\"name\": \"person.name\", \"age\": \"person.age\"}\n            &gt;&gt;&gt; json_data = {\"person\": {\"name\": \"Alice\", \"age\": \"30\"}}\n            &gt;&gt;&gt; mapper = JsonDataMapper(mapping, json_data)\n            &gt;&gt;&gt; mapper.get_mapped_data()\n            [{'name': 'Alice', 'age': 30}]\n        \"\"\"\n        parsing_results = dict()\n        for j_key, j_value in self.mapping.items():\n            temp_list = list()\n            jsonpath_expression = parse(j_value)\n\n            for match in jsonpath_expression.find(self.json_data):\n                temp_list.append(match.value)\n            parsing_results[j_key] = temp_list\n\n        parsed_data = self.convert_to_dict(parsing_results)\n        return parsed_data\n</code></pre>"},{"location":"Common/JsonDataMapper/#datacraft_framework.Common.JsonDataMapper.JsonDataMapper.__init__","title":"<code>__init__(mapping, json_data)</code>","text":"<p>Initialize the JsonDataMapper with mapping rules and JSON data.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Dict[str, str]</code> <p>Dictionary mapping output keys to JSONPath expressions.</p> required <code>json_data</code> <code>Any</code> <p>JSON-like data (e.g., dict or list) to be parsed.</p> required Source code in <code>src/datacraft_framework/Common/JsonDataMapper.py</code> <pre><code>def __init__(self, mapping: dict, json_data):\n    \"\"\"\n    Initialize the JsonDataMapper with mapping rules and JSON data.\n\n    Args:\n        mapping (Dict[str, str]): Dictionary mapping output keys to JSONPath expressions.\n        json_data (Any): JSON-like data (e.g., dict or list) to be parsed.\n    \"\"\"\n    self.mapping = mapping\n    self.json_data = json_data\n</code></pre>"},{"location":"Common/JsonDataMapper/#datacraft_framework.Common.JsonDataMapper.JsonDataMapper.convert_to_dict","title":"<code>convert_to_dict(data)</code>","text":"<p>Convert a dictionary of lists into a list of dictionaries.</p> <p>This method aligns values by index. If a list is shorter than others, the last value is repeated.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, List]</code> <p>A dictionary where each key maps to a list of extracted values.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict]: A list of dictionaries, each representing one row of aligned data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = {\"name\": [\"Alice\", \"Bob\"], \"age\": [\"30\", \"25\"]}\n&gt;&gt;&gt; self.convert_to_dict(data)\n[{'name': 'Alice', 'age': 30}, {'name': 'Bob', 'age': 25}]\n</code></pre> Source code in <code>src/datacraft_framework/Common/JsonDataMapper.py</code> <pre><code>def convert_to_dict(self, data) -&gt; list[dict]:\n    \"\"\"\n    Convert a dictionary of lists into a list of dictionaries.\n\n    This method aligns values by index. If a list is shorter than others, the last value is repeated.\n\n    Args:\n        data (Dict[str, List]): A dictionary where each key maps to a list of extracted values.\n\n    Returns:\n        List[Dict]: A list of dictionaries, each representing one row of aligned data.\n\n    Examples:\n        &gt;&gt;&gt; data = {\"name\": [\"Alice\", \"Bob\"], \"age\": [\"30\", \"25\"]}\n        &gt;&gt;&gt; self.convert_to_dict(data)\n        [{'name': 'Alice', 'age': 30}, {'name': 'Bob', 'age': 25}]\n    \"\"\"\n\n    max_length = max(len(values) for values in data.values())\n    result = []\n    for i in range(max_length):\n        item = {}\n        for key, values in data.items():\n            if i &lt; len(values):\n                value = values[i]\n            else:\n                value = values[-1]\n\n            try:\n                item[key] = int(value)\n            except ValueError:\n                item[key] = value\n        result.append(item)\n    return result\n</code></pre>"},{"location":"Common/JsonDataMapper/#datacraft_framework.Common.JsonDataMapper.JsonDataMapper.get_mapped_data","title":"<code>get_mapped_data()</code>","text":"<p>Extract and transform data from JSON based on the provided mapping.</p> <p>Uses JSON Path expressions in the mapping to extract values from the JSON data. Converts the result into a list of dictionaries.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List[Dict]: A list of dictionaries containing mapped and structured data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mapping = {\"name\": \"person.name\", \"age\": \"person.age\"}\n&gt;&gt;&gt; json_data = {\"person\": {\"name\": \"Alice\", \"age\": \"30\"}}\n&gt;&gt;&gt; mapper = JsonDataMapper(mapping, json_data)\n&gt;&gt;&gt; mapper.get_mapped_data()\n[{'name': 'Alice', 'age': 30}]\n</code></pre> Source code in <code>src/datacraft_framework/Common/JsonDataMapper.py</code> <pre><code>def get_mapped_data(self) -&gt; list[dict]:\n    \"\"\"\n    Extract and transform data from JSON based on the provided mapping.\n\n    Uses JSON Path expressions in the mapping to extract values from the JSON data.\n    Converts the result into a list of dictionaries.\n\n    Returns:\n        List[Dict]: A list of dictionaries containing mapped and structured data.\n\n    Examples:\n        &gt;&gt;&gt; mapping = {\"name\": \"person.name\", \"age\": \"person.age\"}\n        &gt;&gt;&gt; json_data = {\"person\": {\"name\": \"Alice\", \"age\": \"30\"}}\n        &gt;&gt;&gt; mapper = JsonDataMapper(mapping, json_data)\n        &gt;&gt;&gt; mapper.get_mapped_data()\n        [{'name': 'Alice', 'age': 30}]\n    \"\"\"\n    parsing_results = dict()\n    for j_key, j_value in self.mapping.items():\n        temp_list = list()\n        jsonpath_expression = parse(j_value)\n\n        for match in jsonpath_expression.find(self.json_data):\n            temp_list.append(match.value)\n        parsing_results[j_key] = temp_list\n\n    parsed_data = self.convert_to_dict(parsing_results)\n    return parsed_data\n</code></pre>"},{"location":"Common/OrchestrationProcess/","title":"Documentation for <code>OrchestrationProcess</code>","text":""},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess","title":"<code>datacraft_framework.Common.OrchestrationProcess</code>","text":""},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.BackendSettings","title":"<code>BackendSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configure backend database settings for the datacraft framework.</p> <p>This class manages configuration parameters related to connecting to a database. It supports MySQL, PostgreSQL, and SQLite databases. If a full SQLAlchemy URL is provided via <code>sqlalchemy_url</code>, it will be used directly. Otherwise, connection parameters like user, password, host, port, etc., are used to construct the URL.</p> <p>Attributes:</p> Name Type Description <code>sqlalchemy_url</code> <code>Optional[str]</code> <p>Full SQLAlchemy connection string. If provided, this takes precedence over other fields.</p> <code>database_type</code> <code>Literal['mysql', 'postgresql', 'sqlite']</code> <p>Type of database to use. Defaults to \"sqlite\".</p> <code>database</code> <code>str</code> <p>Name of the database/schema.</p> <code>user</code> <code>Optional[str]</code> <p>Database user.</p> <code>password</code> <code>Optional[str]</code> <p>Database password.</p> <code>hostname</code> <code>Optional[str]</code> <p>Database host. Defaults to \"localhost\".</p> <code>port</code> <code>Optional[int]</code> <p>Database port. Defaults based on database type: * MySQL: 3306 * PostgreSQL: 5432</p> <code>datacraft_framework_home</code> <code>Optional[str]</code> <p>Root directory for framework files. Defaults to <code>$HOME/datacraft_framework</code>.</p> <code>connection_string</code> <code>str</code> <p>Computed field that returns the final SQLAlchemy connection string based on the configured values.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>class BackendSettings(BaseSettings):\n    \"\"\"Configure backend database settings for the datacraft framework.\n\n    This class manages configuration parameters related to connecting to a database.\n    It supports MySQL, PostgreSQL, and SQLite databases. If a full SQLAlchemy URL is\n    provided via `sqlalchemy_url`, it will be used directly. Otherwise, connection\n    parameters like user, password, host, port, etc., are used to construct the URL.\n\n    Attributes:\n        sqlalchemy_url (Optional[str]): Full SQLAlchemy connection string.\n            If provided, this takes precedence over other fields.\n        database_type (Literal[\"mysql\", \"postgresql\", \"sqlite\"]): Type of database to use.\n            Defaults to \"sqlite\".\n        database (str): Name of the database/schema.\n        user (Optional[str]): Database user.\n        password (Optional[str]): Database password.\n        hostname (Optional[str]): Database host. Defaults to \"localhost\".\n        port (Optional[int]): Database port. Defaults based on database type:\n            * MySQL: 3306\n            * PostgreSQL: 5432\n        datacraft_framework_home (Optional[str]): Root directory for framework files.\n            Defaults to `$HOME/datacraft_framework`.\n        connection_string (str): Computed field that returns the final SQLAlchemy\n            connection string based on the configured values.\n    \"\"\"\n\n    sqlalchemy_url: Optional[str] = Field(default=None, alias=\"database_url\")\n\n    # Database type\n    database_type: Literal[\"mysql\", \"postgresql\", \"sqlite\"] = Field(\n        default=\"sqlite\", alias=\"db_type\"\n    )\n\n    # Common database fields\n    database: str = Field(default=\"nextgen_framework_configuration\", alias=\"db_name\")\n    user: Optional[str] = Field(default=None, alias=\"db_user\")\n    password: Optional[str] = Field(default=None, alias=\"db_password\")\n    hostname: Optional[str] = Field(default=\"localhost\", alias=\"db_host\")\n    port: Optional[int] = Field(default=None, alias=\"db_port\")\n    datacraft_framework_home: Optional[str] = Field(\n        default=str(Path.home() / \"datacraft_framework\")\n    )\n\n    # Port defaults based on database type\n    @model_validator(mode=\"after\")\n    def set_defaults(self):\n        Path(self.datacraft_framework_home).mkdir(parents=True, exist_ok=True)\n\n        if self.port is None:\n            if self.database_type == \"mysql\":\n                self.port = 3306\n            elif self.database_type == \"postgresql\":\n                self.port = 5432\n        return self\n\n    @computed_field\n    @property\n    def connection_string(self) -&gt; str:\n        \"\"\"Generate SQLAlchemy connection string based on database configuration.\n\n        If `sqlalchemy_url` is provided, it is returned as-is. Otherwise,\n        constructs the appropriate connection string using the relevant\n        dialect driver (e.g., pymysql for MySQL, psycopg2 for PostgreSQL),\n        or builds an SQLite path.\n\n        Returns:\n            str: A valid SQLAlchemy connection string.\n        \"\"\"\n        if self.sqlalchemy_url:\n            return self.sqlalchemy_url\n\n        if self.database_type == \"mysql\":\n            driver = \"pymysql\"\n            return f\"mysql+{driver}://{self.user}:{self.password}@{self.hostname}:{self.port}/{self.database}\"\n        elif self.database_type == \"postgresql\":\n            driver = \"psycopg2\"\n            return f\"postgresql+{driver}://{self.user}:{self.password}@{self.hostname}:{self.port}/{self.database}\"\n        elif self.database_type == \"sqlite\":\n            db_path = Path(self.datacraft_framework_home) / f\"{self.database}.db\"\n            return f\"sqlite:///{db_path}\"\n        else:\n            raise ValueError(f\"Unsupported database type: {self.database_type}\")\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.BackendSettings.connection_string","title":"<code>connection_string</code>  <code>property</code>","text":"<p>Generate SQLAlchemy connection string based on database configuration.</p> <p>If <code>sqlalchemy_url</code> is provided, it is returned as-is. Otherwise, constructs the appropriate connection string using the relevant dialect driver (e.g., pymysql for MySQL, psycopg2 for PostgreSQL), or builds an SQLite path.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A valid SQLAlchemy connection string.</p>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess","title":"<code>OrchestrationProcess</code>","text":"<p>A class responsible for orchestrating database operations in the application.</p> <p>This class initializes database connections, ensures necessary tables are created, and provides a session for performing CRUD operations using ORM models.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>class OrchestrationProcess:\n    \"\"\"\n    A class responsible for orchestrating database operations in the application.\n\n    This class initializes database connections, ensures necessary tables are created,\n    and provides a session for performing CRUD operations using ORM models.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the OrchestrationProcess with database connections and session.\n\n        This constructor:\n        - Loads backend settings for database connection\n        - Establishes a database engine connection\n        - Creates all tables defined under `SQLModel.metadata` if they don't exist\n        - Initializes a database session for interaction with the ORM models\n\n        Attributes:\n            orch_settings (BackendSettings): Configuration object containing connection details.\n            connection (Engine): SQLAlchemy engine instance for database connectivity.\n            session (Session): SQLAlchemy session object for interacting with the database.\n        \"\"\"\n        self.orch_settings = BackendSettings()\n        self.connection = create_engine(self.orch_settings.connection_string)\n        SQLModel.metadata.create_all(bind=self.connection)\n\n        self.session = Session(self.connection)\n\n    def get_ctl_column_metadata(self, dataset_id: int) -&gt; list[CtlColumnMetadata]:\n        \"\"\"\n        Retrieve metadata for columns associated with a specific dataset.\n\n        This method queries the database to fetch all column metadata entries\n        for the given dataset ID, sorted by their sequence number.\n\n        Args:\n            dataset_id (int): The unique identifier of the dataset.\n\n        Returns:\n            list[CtlColumnMetadata]: A list of CtlColumnMetadata objects representing\n                the columns associated with the specified dataset.\n        \"\"\"\n        query = (\n            select(CtlColumnMetadata)\n            .where(CtlColumnMetadata.dataset_id == dataset_id)\n            .order_by(CtlColumnMetadata.column_sequence_number)\n        )\n\n        result = self.session.exec(query).all()\n        return result\n\n    def insert_ctl_column_metadata(self, column_metadata: CtlColumnMetadata) -&gt; None:\n        \"\"\"\n        Inserts column metadata details into the database.\n\n        Args:\n            column_metadata (CtlColumnMetadata): Metadata for the column to be inserted.\n\n        Returns:\n            None\n\n        Raises:\n            SQLAlchemyError: If a database error occurs during query execution.\n        \"\"\"\n        self.session.add(column_metadata)\n        self.session.commit()\n\n    def get_ctl_api_connection_details(\n        self, dataset_id: int\n    ) -&gt; list[ctlApiConnectionsDtl]:\n        \"\"\"\n        Retrieves API connection details associated with a specific pre-ingestion dataset ID.\n\n        This method queries the database to fetch all API connection details linked to the provided dataset ID. The results are ordered by sequence number to ensure deterministic ordering.\n\n        Args:\n            dataset_id (int): The pre-ingestion dataset ID used to filter API connection details.\n\n        Returns:\n            list[ctlApiConnectionsDtl]: A list of API connection detail objects, sorted by seq_no in ascending order.\n        \"\"\"\n        query = (\n            select(ctlApiConnectionsDtl)\n            .where(ctlApiConnectionsDtl.pre_ingestion_dataset_id == dataset_id)\n            .order_by(ctlApiConnectionsDtl.seq_no)\n        )\n\n        result = self.session.exec(query).all()\n        return result\n\n    def insert_ctl_api_connection_details(\n        self, api_details: ctlApiConnectionsDtl\n    ) -&gt; None:\n        \"\"\"\n        Inserts API connection details into the database using SQLAlchemy's session.\n\n        Args:\n            api_details (ctlApiConnectionsDtl): An instance of ctlApiConnectionsDtl containing the API connection details to be inserted.\n\n        Returns:\n            None: The function does not return a value.\n        \"\"\"\n        self.session.add(api_details)\n        self.session.commit()\n\n    def get_ctl_data_acquisition_detail(\n        self, process_id: int\n    ) -&gt; list[ctlDataAcquisitionDetail]:\n        \"\"\"\n        Retrieve data acquisition details for a specific process from the database.\n\n        This method queries the database to retrieve all `ctlDataAcquisitionDetail` records associated with the provided `process_id`.\n\n        Args:\n            process_id (int): The unique identifier of the process for which to retrieve data acquisition details.\n\n        Returns:\n            list[ctlDataAcquisitionDetail]: A list of `ctlDataAcquisitionDetail` objects matching the given `process_id`. Returns an empty list if no records are found.\n\n        Raises:\n            SQLAlchemyError: If a database error occurs during query execution.\n        \"\"\"\n        query = select(ctlDataAcquisitionDetail).where(\n            ctlDataAcquisitionDetail.process_id == process_id\n        )\n\n        result = self.session.exec(query).all()\n        return result\n\n    def insert_ctl_data_acquisition_connection_master(\n        self, data_acquisition_connection_detail: ctlDataAcquisitionConnectionMaster\n    ) -&gt; None:\n        \"\"\"\n        Insert a new data acquisition connection master record into the database.\n\n        This method adds the provided `ctlDataAcquisitionConnectionMaster` object to the current\n        database session and commits the transaction, persisting the connection details to the database.\n\n        Args:\n            data_acquisition_connection_detail (ctlDataAcquisitionConnectionMaster):\n                An instance of the `ctlDataAcquisitionConnectionMaster` model containing\n                the data acquisition connection information to be inserted.\n\n        Returns:\n            None\n        \"\"\"\n\n        self.session.add(data_acquisition_connection_detail)\n        self.session.commit()\n\n    def get_ctl_data_acquisition_connection_master(\n        self,\n        outbound_source_platform: str,\n        outbound_source_system: str,\n    ) -&gt; ctlDataAcquisitionConnectionMaster:\n        \"\"\"Retrieve a data acquisition connection master by outbound source platform and system.\n\n        Args:\n            outbound_source_platform (str): The platform of the outbound source.\n            outbound_source_system (str): The system of the outbound source.\n\n        Returns:\n            ctlDataAcquisitionConnectionMaster: An instance of the matching data acquisition\n                connection master, or None if no match is found.\n        \"\"\"\n        query = select(ctlDataAcquisitionConnectionMaster).where(\n            (\n                ctlDataAcquisitionConnectionMaster.outbound_source_platform\n                == outbound_source_platform\n            )\n            &amp; (\n                ctlDataAcquisitionConnectionMaster.outbound_source_system\n                == outbound_source_system\n            )\n        )\n\n        result = self.session.exec(query).first()\n        return result\n\n    def insert_ctl_data_acquisition_detail(\n        self, data_acquisition: ctlDataAcquisitionDetail\n    ) -&gt; None:\n        \"\"\"\n        Insert a new data acquisition detail record into the database.\n\n        This method adds the provided `ctlDataAcquisitionDetail` object to the current database session\n        and commits the transaction, persisting the data acquisition record to the database.\n\n        Args:\n            data_acquisition (ctlDataAcquisitionDetail): An instance of the `ctlDataAcquisitionDetail` model\n                containing the data acquisition details to be inserted.\n\n        Returns:\n            None\n        \"\"\"\n\n        self.session.add(data_acquisition)\n        self.session.commit()\n\n    def get_log_data_acquisition_detail(\n        self,\n        process_id: int,\n        dataset_id: int,\n        status: Literal[\"SUCCEEDED\", \"FAILED\", \"IN-PROGRESS\"],\n    ) -&gt; list[logDataAcquisitionDetail]:\n        \"\"\"\n        Retrieves log details for data acquisition processes based on process ID, dataset ID, and status filter.\n\n        Args:\n            process_id (int): Unique identifier for the data acquisition process.\n            dataset_id (int): Pre-ingestion dataset identifier associated with the log entry.\n            status (Literal[\"SUCCEEDED\", \"FAILED\", \"IN-PROGRESS\"]): Filter by the current status of the data acquisition process.\n\n        Returns:\n            (list[logDataAcquisitionDetail]): List of log entries matching the specified criteria, containing details about data acquisition operations.\n\n        Raises:\n            SQLAlchemyError: If database operation fails during query execution.\n        \"\"\"\n        query = select(logDataAcquisitionDetail).where(\n            (logDataAcquisitionDetail.process_id == process_id)\n            &amp; (logDataAcquisitionDetail.pre_ingestion_dataset_id == dataset_id)\n            &amp; (logDataAcquisitionDetail.status == status)\n        )\n\n        result = self.session.exec(query).all()\n        return result\n\n    def insert_log_data_acquisition_detail(\n        self, log_data_acquisition: logDataAcquisitionDetail\n    ):\n        self.session.add(log_data_acquisition)\n        self.session.commit()\n\n    def get_log_raw_process_dtl(\n        self,\n        process_id: int,\n        dataset_id: int,\n        status: Literal[\"SUCCEEDED\", \"FAILED\", \"IN-PROGRESS\"] = \"SUCCEEDED\",\n    ) -&gt; list[logRawProcessDtl]:\n        \"\"\"Retrieve log raw process details based on process ID, dataset ID, and status.\n\n        Args:\n            process_id (int): Unique identifier for the process.\n            dataset_id (int): Unique identifier for the dataset.\n            status (Literal[\"SUCCEEDED\", \"FAILED\", \"IN-PROGRESS\"], optional):\n                Filter by status. Defaults to \"SUCCEEDED\".\n\n        Returns:\n            List of log raw process details objects, ordered by batch_id in ascending order. Filters records where file_status matches the specified status.\n\n        Examples:\n            &gt;&gt;&gt; get_log_raw_process_dtl(process_id=123, dataset_id=456, status=\"FAILED\")\n        \"\"\"\n        query = (\n            select(logRawProcessDtl)\n            .where(\n                (logRawProcessDtl.process_id == process_id)\n                &amp; (logRawProcessDtl.dataset_id == dataset_id)\n                &amp; (logRawProcessDtl.file_status == status)\n            )\n            .order_by(logRawProcessDtl.batch_id.asc())\n        )\n\n        result = self.session.exec(query).all()\n        return result\n\n    def insert_log_raw_process_detail(self, log_raw_process_dtl: logRawProcessDtl):\n        self.session.add(log_raw_process_dtl)\n        self.session.commit()\n\n    def insert_dataset_master(self, dataset_master: ctlDatasetMaster):\n        self.session.add(dataset_master)\n        self.session.commit()\n\n    def get_dataset_master(\n        self,\n        process_id: int,\n        dataset_type: Literal[\"BRONZE\", \"SILVER\", \"GOLD\"],\n        dataset_id: Optional[int] = None,\n    ) -&gt; Union[list[ctlDatasetMaster], ctlDatasetMaster]:\n        \"\"\"Retrieve dataset master records based on process ID and dataset type.\n\n        Args:\n            process_id (int): The ID of the process associated with the dataset.\n            dataset_type (Literal[\"BRONZE\", \"SILVER\", \"GOLD\"]): The type of dataset to filter by.\n            dataset_id (Optional[int], default=None): Optional dataset ID to fetch a specific record. If provided,\n                a single record is returned; otherwise, all matching records are returned.\n\n        Returns:\n            A list of dataset master records if no dataset_id is provided, or a single dataset master record if dataset_id is specified. The results are ordered by dataset_id in ascending order.\n\n        Note:\n            Uses SQLAlchemy's select statement to query the ctlDatasetMaster table. Results are ordered by dataset_id.asc().\n        \"\"\"\n        if dataset_id is not None:\n            query = (\n                select(ctlDatasetMaster)\n                .where(\n                    (ctlDatasetMaster.process_id == process_id)\n                    &amp; (ctlDatasetMaster.dataset_id == dataset_id)\n                    &amp; (ctlDatasetMaster.dataset_type == dataset_type)\n                )\n                .order_by(ctlDatasetMaster.dataset_id.asc())\n            )\n            result = self.session.exec(query).first()\n            return result\n        else:\n            query = (\n                select(ctlDatasetMaster)\n                .where(\n                    (ctlDatasetMaster.process_id == process_id)\n                    &amp; (ctlDatasetMaster.dataset_type == dataset_type)\n                )\n                .order_by(ctlDatasetMaster.dataset_id.asc())\n            )\n            result = self.session.exec(query).all()\n            return result\n\n    def get_data_standardisation_unprocessed_files(\n        self, process_id: int, dataset_id: int\n    ) -&gt; list[logRawProcessDtl]:\n        \"\"\"Retrieve unprocessed files for data standardisation that have succeeded in raw processing.\n\n        Args:\n            process_id (int): Identifier for the process.\n            dataset_id (int): Identifier for the dataset.\n\n        Returns:\n            (list[logRawProcessDtl]): List of logRawProcessDtl entries where the source_file\n                has not been processed in data standardisation, and the raw processing\n                status is \"SUCCEEDED\", filtered by the given process_id and dataset_id.\n                Results are ordered by batch_id ascending.\n\n        Description:\n            This method queries for files that have completed raw processing successfully\n            but have not yet been handled by data standardisation. It excludes files\n            already marked as succeeded in the logDataStandardisationDtl table. The results\n            are sorted by batch ID to ensure consistent ordering.\n        \"\"\"\n        subquery = select(logDataStandardisationDtl.source_file).where(\n            logDataStandardisationDtl.status == \"SUCCEEDED\"\n        )\n\n        query = (\n            select(logRawProcessDtl)\n            .filter(\n                ~logRawProcessDtl.source_file.in_(subquery),\n                logRawProcessDtl.process_id == process_id,\n                logRawProcessDtl.dataset_id == dataset_id,\n                logRawProcessDtl.file_status == \"SUCCEEDED\",\n            )\n            .order_by(logRawProcessDtl.batch_id.asc())\n        )\n        results = self.session.exec(query).all()\n        return results\n\n    def insert_data_standardisation_log(\n        self, log_data_standardisation: logDataStandardisationDtl\n    ):\n        self.session.add(log_data_standardisation)\n        self.session.commit()\n\n    def get_data_standard_dtl(\n        self,\n        dataset_id: Optional[int] = None,\n    ) -&gt; list[ctlDataStandardisationDtl]:\n        \"\"\"Retrieve standardization details for a specific dataset.\n\n        Queries the database to retrieve all records from the ctlDataStandardisationDtl table where the dataset_id matches the provided value. If no dataset_id is provided, returns all records.\n\n        Args:\n            dataset_id (Optional[int]): The ID of the dataset to filter results. Defaults to None.\n\n        Returns:\n            (list[ctlDataStandardisationDtl]): A list of standardization details objects matching the dataset_id.\n        \"\"\"\n        query = select(ctlDataStandardisationDtl).where(\n            ctlDataStandardisationDtl.dataset_id == dataset_id\n        )\n        result = self.session.exec(query).all()\n        return result\n\n    def get_dqm_unprocessed_files(\n        self, process_id: int, dataset_id: int\n    ) -&gt; list[logDqmDtl]:\n        \"\"\"\n        Retrieve a list of successfully processed data standardization files that have not yet been processed by DQM.\n\n        This method queries the database to find entries from `logDataStandardisationDtl` that:\n        - Belong to the specified `process_id` and `dataset_id`\n        - Have a status of \"SUCCEEDED\"\n        - Do not appear in the `logDqmDtl` table with a status of \"SUCCEEDED\"\n\n        The results are ordered by ascending `batch_id`.\n\n        Args:\n            process_id (int): The ID of the process to filter files.\n            dataset_id (int): The ID of the dataset to filter files.\n\n        Returns:\n            (List[logDqmDtl]): A list of log objects representing unprocessed files in the context of DQM.\n        \"\"\"\n\n        subquery = select(logDqmDtl.source_file).where(logDqmDtl.status == \"SUCCEEDED\")\n\n        query = (\n            select(logDataStandardisationDtl)\n            .filter(\n                ~logDataStandardisationDtl.source_file.in_(subquery),\n                logDataStandardisationDtl.process_id == process_id,\n                logDataStandardisationDtl.dataset_id == dataset_id,\n                logDataStandardisationDtl.status == \"SUCCEEDED\",\n            )\n            .order_by(logDataStandardisationDtl.batch_id.asc())\n        )\n        results = self.session.exec(query).all()\n        return results\n\n    def get_dqm_detail(self, process_id: int, dataset_id: int) -&gt; list[ctlDqmMasterDtl]:\n        \"\"\"\n        Retrieve DQM (Data Quality Management) master detail records for the specified process and dataset.\n\n        This method queries the database to fetch all entries from `ctlDqmMasterDtl` that match the given\n        `process_id` and `dataset_id`.\n\n        Args:\n            process_id (int): The unique identifier of the process.\n            dataset_id (int): The unique identifier of the dataset.\n\n        Returns:\n            (List[ctlDqmMasterDtl]): A list of DQM master detail records matching the provided criteria.\n        \"\"\"\n        query = select(ctlDqmMasterDtl).where(\n            (ctlDqmMasterDtl.dataset_id == dataset_id)\n            &amp; (ctlDqmMasterDtl.process_id == process_id)\n        )\n        result = self.session.exec(query).all()\n        return result\n\n    def insert_log_dqm(self, log_dqm: logDqmDtl) -&gt; None:\n        \"\"\"\n        Insert a new DQM (Data Quality Management) log entry into the database.\n\n        This method adds the provided `logDqmDtl` object to the current database session and commits the transaction,\n        effectively persisting the log record to the database.\n\n        Args:\n            log_dqm (logDqmDtl): An instance of the `logDqmDtl` model containing the DQM log data to be inserted.\n\n        Returns:\n            None\n        \"\"\"\n\n        self.session.add(log_dqm)\n        self.session.commit()\n\n    def get_transformation_dependency_master(\n        self, process_id: int, dataset_id: int\n    ) -&gt; list[ctlTransformationDependencyMaster]:\n        \"\"\"\n        Retrieve transformation dependency master records for the specified process and dataset.\n\n        This method queries the database to fetch all entries from `ctlTransformationDependencyMaster`\n        that match the provided `process_id` and `dataset_id`. The results are ordered by the\n        `transformation_step` field in ascending order.\n\n        Args:\n            process_id (int): The ID of the process associated with the transformation dependencies.\n            dataset_id (int): The ID of the dataset associated with the transformation dependencies.\n\n        Returns:\n            (List[ctlTransformationDependencyMaster]): A list of transformation dependency records\n                                                       sorted by transformation step.\n        \"\"\"\n\n        query = (\n            select(ctlTransformationDependencyMaster)\n            .where(\n                (ctlTransformationDependencyMaster.process_id == process_id)\n                &amp; (ctlTransformationDependencyMaster.dataset_id == dataset_id)\n            )\n            .order_by(ctlTransformationDependencyMaster.transformation_step)\n        )\n        results = self.session.exec(query).all()\n        return results\n\n    def get_unprocessed_transformation_files(\n        self, process_id, dataset_id\n    ) -&gt; list[logTransformationDtl]:\n        \"\"\"\n        Retrieve a list of files that have not yet been successfully processed by the transformation step.\n\n        This method identifies files in the `logDqmDtl` table that:\n        - Belong to the specified `process_id` and `dataset_id`\n        - Have not appeared in the `logTransformationDtl` table with a status of \"SUCCEEDED\"\n\n        These files are considered unprocessed from a transformation perspective and may require further processing.\n\n        Args:\n            process_id (int): The ID of the process to filter files.\n            dataset_id (int): The ID of the dataset to filter files.\n\n        Returns:\n            (list[logTransformationDtl]): A list of transformation detail objects representing unprocessed files.\n        \"\"\"\n\n        subquery = select(logTransformationDtl.source_file).where(\n            logTransformationDtl.status == \"SUCCEEDED\"\n        )\n        query = select(logDqmDtl).filter(\n            ~logDqmDtl.source_file.in_(subquery),\n            logDqmDtl.process_id == process_id,\n            logDqmDtl.dataset_id == dataset_id,\n        )\n        results = self.session.exec(query).all()\n        return results\n\n    def insert_log_transformation(\n        self, log_transformation: logTransformationDtl\n    ) -&gt; None:\n        \"\"\"\n        Insert a new transformation log entry into the database.\n\n        This method adds the provided `logTransformationDtl` object to the current database session\n        and commits the transaction, persisting the transformation log record to the database.\n\n        Args:\n            log_transformation (logTransformationDtl): An instance of the `logTransformationDtl` model\n                containing the transformation log data to be inserted.\n\n        Returns:\n            None\n        \"\"\"\n\n        self.session.add(log_transformation)\n        self.session.commit()\n\n    def get_transformation_dqm_unprocessed_files(\n        self, process_id, dataset_id\n    ) -&gt; list[logTransformationDtl]:\n        \"\"\"\n        Retrieve transformation files that have not yet been successfully processed by DQM.\n\n        This method identifies entries in the `logTransformationDtl` table that:\n        - Belong to the specified `process_id` and `dataset_id`\n        - Have a status of \"SUCCEEDED\"\n        - Do not appear in the `logDqmDtl` table with a status of \"SUCCEEDED\" for the same dataset\n\n        The results are ordered by ascending `batch_id`.\n\n        Args:\n            process_id (int): The ID of the process to filter transformation files.\n            dataset_id (int): The ID of the dataset to filter transformation files.\n\n        Returns:\n            (List[logTransformationDtl]): A list of transformation log records that are unprocessed by DQM.\n        \"\"\"\n\n        subquery = select(logDqmDtl.source_file).where(\n            (logDqmDtl.status == \"SUCCEEDED\") &amp; (logDqmDtl.dataset_id == dataset_id)\n        )\n\n        query = (\n            select(logTransformationDtl)\n            .filter(\n                ~logTransformationDtl.source_file.in_(subquery),\n                logTransformationDtl.process_id == process_id,\n                logTransformationDtl.dataset_id == dataset_id,\n                logTransformationDtl.status == \"SUCCEEDED\",\n            )\n            .order_by(logTransformationDtl.batch_id.asc())\n        )\n        results = self.session.exec(query).all()\n        return results\n\n    def get_gold_datasets(self) -&gt; list[ctlDatasetMaster]:\n        \"\"\"\n        Retrieve all GOLD-type datasets from the dataset master table.\n\n        This method queries the `ctlDatasetMaster` table to fetch records where the\n        `dataset_type` is \"GOLD\". The results are ordered by `dataset_id` in ascending order.\n\n        Returns:\n            (List[ctlDatasetMaster]): A list of dataset master records representing GOLD-type datasets.\n        \"\"\"\n        query = (\n            select(ctlDatasetMaster)\n            .where(ctlDatasetMaster.dataset_type == \"GOLD\")\n            .order_by(ctlDatasetMaster.dataset_id)\n        )\n\n        results = self.session.exec(query).all()\n        return results\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback) -&gt; None:\n        self.session.close()\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the OrchestrationProcess with database connections and session.</p> <p>This constructor: - Loads backend settings for database connection - Establishes a database engine connection - Creates all tables defined under <code>SQLModel.metadata</code> if they don't exist - Initializes a database session for interaction with the ORM models</p> <p>Attributes:</p> Name Type Description <code>orch_settings</code> <code>BackendSettings</code> <p>Configuration object containing connection details.</p> <code>connection</code> <code>Engine</code> <p>SQLAlchemy engine instance for database connectivity.</p> <code>session</code> <code>Session</code> <p>SQLAlchemy session object for interacting with the database.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the OrchestrationProcess with database connections and session.\n\n    This constructor:\n    - Loads backend settings for database connection\n    - Establishes a database engine connection\n    - Creates all tables defined under `SQLModel.metadata` if they don't exist\n    - Initializes a database session for interaction with the ORM models\n\n    Attributes:\n        orch_settings (BackendSettings): Configuration object containing connection details.\n        connection (Engine): SQLAlchemy engine instance for database connectivity.\n        session (Session): SQLAlchemy session object for interacting with the database.\n    \"\"\"\n    self.orch_settings = BackendSettings()\n    self.connection = create_engine(self.orch_settings.connection_string)\n    SQLModel.metadata.create_all(bind=self.connection)\n\n    self.session = Session(self.connection)\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_ctl_column_metadata","title":"<code>get_ctl_column_metadata(dataset_id)</code>","text":"<p>Retrieve metadata for columns associated with a specific dataset.</p> <p>This method queries the database to fetch all column metadata entries for the given dataset ID, sorted by their sequence number.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>int</code> <p>The unique identifier of the dataset.</p> required <p>Returns:</p> Type Description <code>list[CtlColumnMetadata]</code> <p>list[CtlColumnMetadata]: A list of CtlColumnMetadata objects representing the columns associated with the specified dataset.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_ctl_column_metadata(self, dataset_id: int) -&gt; list[CtlColumnMetadata]:\n    \"\"\"\n    Retrieve metadata for columns associated with a specific dataset.\n\n    This method queries the database to fetch all column metadata entries\n    for the given dataset ID, sorted by their sequence number.\n\n    Args:\n        dataset_id (int): The unique identifier of the dataset.\n\n    Returns:\n        list[CtlColumnMetadata]: A list of CtlColumnMetadata objects representing\n            the columns associated with the specified dataset.\n    \"\"\"\n    query = (\n        select(CtlColumnMetadata)\n        .where(CtlColumnMetadata.dataset_id == dataset_id)\n        .order_by(CtlColumnMetadata.column_sequence_number)\n    )\n\n    result = self.session.exec(query).all()\n    return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.insert_ctl_column_metadata","title":"<code>insert_ctl_column_metadata(column_metadata)</code>","text":"<p>Inserts column metadata details into the database.</p> <p>Parameters:</p> Name Type Description Default <code>column_metadata</code> <code>CtlColumnMetadata</code> <p>Metadata for the column to be inserted.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>SQLAlchemyError</code> <p>If a database error occurs during query execution.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def insert_ctl_column_metadata(self, column_metadata: CtlColumnMetadata) -&gt; None:\n    \"\"\"\n    Inserts column metadata details into the database.\n\n    Args:\n        column_metadata (CtlColumnMetadata): Metadata for the column to be inserted.\n\n    Returns:\n        None\n\n    Raises:\n        SQLAlchemyError: If a database error occurs during query execution.\n    \"\"\"\n    self.session.add(column_metadata)\n    self.session.commit()\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_ctl_api_connection_details","title":"<code>get_ctl_api_connection_details(dataset_id)</code>","text":"<p>Retrieves API connection details associated with a specific pre-ingestion dataset ID.</p> <p>This method queries the database to fetch all API connection details linked to the provided dataset ID. The results are ordered by sequence number to ensure deterministic ordering.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>int</code> <p>The pre-ingestion dataset ID used to filter API connection details.</p> required <p>Returns:</p> Type Description <code>list[ctlApiConnectionsDtl]</code> <p>list[ctlApiConnectionsDtl]: A list of API connection detail objects, sorted by seq_no in ascending order.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_ctl_api_connection_details(\n    self, dataset_id: int\n) -&gt; list[ctlApiConnectionsDtl]:\n    \"\"\"\n    Retrieves API connection details associated with a specific pre-ingestion dataset ID.\n\n    This method queries the database to fetch all API connection details linked to the provided dataset ID. The results are ordered by sequence number to ensure deterministic ordering.\n\n    Args:\n        dataset_id (int): The pre-ingestion dataset ID used to filter API connection details.\n\n    Returns:\n        list[ctlApiConnectionsDtl]: A list of API connection detail objects, sorted by seq_no in ascending order.\n    \"\"\"\n    query = (\n        select(ctlApiConnectionsDtl)\n        .where(ctlApiConnectionsDtl.pre_ingestion_dataset_id == dataset_id)\n        .order_by(ctlApiConnectionsDtl.seq_no)\n    )\n\n    result = self.session.exec(query).all()\n    return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.insert_ctl_api_connection_details","title":"<code>insert_ctl_api_connection_details(api_details)</code>","text":"<p>Inserts API connection details into the database using SQLAlchemy's session.</p> <p>Parameters:</p> Name Type Description Default <code>api_details</code> <code>ctlApiConnectionsDtl</code> <p>An instance of ctlApiConnectionsDtl containing the API connection details to be inserted.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function does not return a value.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def insert_ctl_api_connection_details(\n    self, api_details: ctlApiConnectionsDtl\n) -&gt; None:\n    \"\"\"\n    Inserts API connection details into the database using SQLAlchemy's session.\n\n    Args:\n        api_details (ctlApiConnectionsDtl): An instance of ctlApiConnectionsDtl containing the API connection details to be inserted.\n\n    Returns:\n        None: The function does not return a value.\n    \"\"\"\n    self.session.add(api_details)\n    self.session.commit()\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_ctl_data_acquisition_detail","title":"<code>get_ctl_data_acquisition_detail(process_id)</code>","text":"<p>Retrieve data acquisition details for a specific process from the database.</p> <p>This method queries the database to retrieve all <code>ctlDataAcquisitionDetail</code> records associated with the provided <code>process_id</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The unique identifier of the process for which to retrieve data acquisition details.</p> required <p>Returns:</p> Type Description <code>list[ctlDataAcquisitionDetail]</code> <p>list[ctlDataAcquisitionDetail]: A list of <code>ctlDataAcquisitionDetail</code> objects matching the given <code>process_id</code>. Returns an empty list if no records are found.</p> <p>Raises:</p> Type Description <code>SQLAlchemyError</code> <p>If a database error occurs during query execution.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_ctl_data_acquisition_detail(\n    self, process_id: int\n) -&gt; list[ctlDataAcquisitionDetail]:\n    \"\"\"\n    Retrieve data acquisition details for a specific process from the database.\n\n    This method queries the database to retrieve all `ctlDataAcquisitionDetail` records associated with the provided `process_id`.\n\n    Args:\n        process_id (int): The unique identifier of the process for which to retrieve data acquisition details.\n\n    Returns:\n        list[ctlDataAcquisitionDetail]: A list of `ctlDataAcquisitionDetail` objects matching the given `process_id`. Returns an empty list if no records are found.\n\n    Raises:\n        SQLAlchemyError: If a database error occurs during query execution.\n    \"\"\"\n    query = select(ctlDataAcquisitionDetail).where(\n        ctlDataAcquisitionDetail.process_id == process_id\n    )\n\n    result = self.session.exec(query).all()\n    return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.insert_ctl_data_acquisition_connection_master","title":"<code>insert_ctl_data_acquisition_connection_master(data_acquisition_connection_detail)</code>","text":"<p>Insert a new data acquisition connection master record into the database.</p> <p>This method adds the provided <code>ctlDataAcquisitionConnectionMaster</code> object to the current database session and commits the transaction, persisting the connection details to the database.</p> <p>Parameters:</p> Name Type Description Default <code>data_acquisition_connection_detail</code> <code>ctlDataAcquisitionConnectionMaster</code> <p>An instance of the <code>ctlDataAcquisitionConnectionMaster</code> model containing the data acquisition connection information to be inserted.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def insert_ctl_data_acquisition_connection_master(\n    self, data_acquisition_connection_detail: ctlDataAcquisitionConnectionMaster\n) -&gt; None:\n    \"\"\"\n    Insert a new data acquisition connection master record into the database.\n\n    This method adds the provided `ctlDataAcquisitionConnectionMaster` object to the current\n    database session and commits the transaction, persisting the connection details to the database.\n\n    Args:\n        data_acquisition_connection_detail (ctlDataAcquisitionConnectionMaster):\n            An instance of the `ctlDataAcquisitionConnectionMaster` model containing\n            the data acquisition connection information to be inserted.\n\n    Returns:\n        None\n    \"\"\"\n\n    self.session.add(data_acquisition_connection_detail)\n    self.session.commit()\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_ctl_data_acquisition_connection_master","title":"<code>get_ctl_data_acquisition_connection_master(outbound_source_platform, outbound_source_system)</code>","text":"<p>Retrieve a data acquisition connection master by outbound source platform and system.</p> <p>Parameters:</p> Name Type Description Default <code>outbound_source_platform</code> <code>str</code> <p>The platform of the outbound source.</p> required <code>outbound_source_system</code> <code>str</code> <p>The system of the outbound source.</p> required <p>Returns:</p> Name Type Description <code>ctlDataAcquisitionConnectionMaster</code> <code>ctlDataAcquisitionConnectionMaster</code> <p>An instance of the matching data acquisition connection master, or None if no match is found.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_ctl_data_acquisition_connection_master(\n    self,\n    outbound_source_platform: str,\n    outbound_source_system: str,\n) -&gt; ctlDataAcquisitionConnectionMaster:\n    \"\"\"Retrieve a data acquisition connection master by outbound source platform and system.\n\n    Args:\n        outbound_source_platform (str): The platform of the outbound source.\n        outbound_source_system (str): The system of the outbound source.\n\n    Returns:\n        ctlDataAcquisitionConnectionMaster: An instance of the matching data acquisition\n            connection master, or None if no match is found.\n    \"\"\"\n    query = select(ctlDataAcquisitionConnectionMaster).where(\n        (\n            ctlDataAcquisitionConnectionMaster.outbound_source_platform\n            == outbound_source_platform\n        )\n        &amp; (\n            ctlDataAcquisitionConnectionMaster.outbound_source_system\n            == outbound_source_system\n        )\n    )\n\n    result = self.session.exec(query).first()\n    return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.insert_ctl_data_acquisition_detail","title":"<code>insert_ctl_data_acquisition_detail(data_acquisition)</code>","text":"<p>Insert a new data acquisition detail record into the database.</p> <p>This method adds the provided <code>ctlDataAcquisitionDetail</code> object to the current database session and commits the transaction, persisting the data acquisition record to the database.</p> <p>Parameters:</p> Name Type Description Default <code>data_acquisition</code> <code>ctlDataAcquisitionDetail</code> <p>An instance of the <code>ctlDataAcquisitionDetail</code> model containing the data acquisition details to be inserted.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def insert_ctl_data_acquisition_detail(\n    self, data_acquisition: ctlDataAcquisitionDetail\n) -&gt; None:\n    \"\"\"\n    Insert a new data acquisition detail record into the database.\n\n    This method adds the provided `ctlDataAcquisitionDetail` object to the current database session\n    and commits the transaction, persisting the data acquisition record to the database.\n\n    Args:\n        data_acquisition (ctlDataAcquisitionDetail): An instance of the `ctlDataAcquisitionDetail` model\n            containing the data acquisition details to be inserted.\n\n    Returns:\n        None\n    \"\"\"\n\n    self.session.add(data_acquisition)\n    self.session.commit()\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_log_data_acquisition_detail","title":"<code>get_log_data_acquisition_detail(process_id, dataset_id, status)</code>","text":"<p>Retrieves log details for data acquisition processes based on process ID, dataset ID, and status filter.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>Unique identifier for the data acquisition process.</p> required <code>dataset_id</code> <code>int</code> <p>Pre-ingestion dataset identifier associated with the log entry.</p> required <code>status</code> <code>Literal['SUCCEEDED', 'FAILED', 'IN-PROGRESS']</code> <p>Filter by the current status of the data acquisition process.</p> required <p>Returns:</p> Type Description <code>list[logDataAcquisitionDetail]</code> <p>List of log entries matching the specified criteria, containing details about data acquisition operations.</p> <p>Raises:</p> Type Description <code>SQLAlchemyError</code> <p>If database operation fails during query execution.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_log_data_acquisition_detail(\n    self,\n    process_id: int,\n    dataset_id: int,\n    status: Literal[\"SUCCEEDED\", \"FAILED\", \"IN-PROGRESS\"],\n) -&gt; list[logDataAcquisitionDetail]:\n    \"\"\"\n    Retrieves log details for data acquisition processes based on process ID, dataset ID, and status filter.\n\n    Args:\n        process_id (int): Unique identifier for the data acquisition process.\n        dataset_id (int): Pre-ingestion dataset identifier associated with the log entry.\n        status (Literal[\"SUCCEEDED\", \"FAILED\", \"IN-PROGRESS\"]): Filter by the current status of the data acquisition process.\n\n    Returns:\n        (list[logDataAcquisitionDetail]): List of log entries matching the specified criteria, containing details about data acquisition operations.\n\n    Raises:\n        SQLAlchemyError: If database operation fails during query execution.\n    \"\"\"\n    query = select(logDataAcquisitionDetail).where(\n        (logDataAcquisitionDetail.process_id == process_id)\n        &amp; (logDataAcquisitionDetail.pre_ingestion_dataset_id == dataset_id)\n        &amp; (logDataAcquisitionDetail.status == status)\n    )\n\n    result = self.session.exec(query).all()\n    return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_log_raw_process_dtl","title":"<code>get_log_raw_process_dtl(process_id, dataset_id, status='SUCCEEDED')</code>","text":"<p>Retrieve log raw process details based on process ID, dataset ID, and status.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>Unique identifier for the process.</p> required <code>dataset_id</code> <code>int</code> <p>Unique identifier for the dataset.</p> required <code>status</code> <code>Literal['SUCCEEDED', 'FAILED', 'IN-PROGRESS']</code> <p>Filter by status. Defaults to \"SUCCEEDED\".</p> <code>'SUCCEEDED'</code> <p>Returns:</p> Type Description <code>list[logRawProcessDtl]</code> <p>List of log raw process details objects, ordered by batch_id in ascending order. Filters records where file_status matches the specified status.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_log_raw_process_dtl(process_id=123, dataset_id=456, status=\"FAILED\")\n</code></pre> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_log_raw_process_dtl(\n    self,\n    process_id: int,\n    dataset_id: int,\n    status: Literal[\"SUCCEEDED\", \"FAILED\", \"IN-PROGRESS\"] = \"SUCCEEDED\",\n) -&gt; list[logRawProcessDtl]:\n    \"\"\"Retrieve log raw process details based on process ID, dataset ID, and status.\n\n    Args:\n        process_id (int): Unique identifier for the process.\n        dataset_id (int): Unique identifier for the dataset.\n        status (Literal[\"SUCCEEDED\", \"FAILED\", \"IN-PROGRESS\"], optional):\n            Filter by status. Defaults to \"SUCCEEDED\".\n\n    Returns:\n        List of log raw process details objects, ordered by batch_id in ascending order. Filters records where file_status matches the specified status.\n\n    Examples:\n        &gt;&gt;&gt; get_log_raw_process_dtl(process_id=123, dataset_id=456, status=\"FAILED\")\n    \"\"\"\n    query = (\n        select(logRawProcessDtl)\n        .where(\n            (logRawProcessDtl.process_id == process_id)\n            &amp; (logRawProcessDtl.dataset_id == dataset_id)\n            &amp; (logRawProcessDtl.file_status == status)\n        )\n        .order_by(logRawProcessDtl.batch_id.asc())\n    )\n\n    result = self.session.exec(query).all()\n    return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_dataset_master","title":"<code>get_dataset_master(process_id, dataset_type, dataset_id=None)</code>","text":"<p>Retrieve dataset master records based on process ID and dataset type.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The ID of the process associated with the dataset.</p> required <code>dataset_type</code> <code>Literal['BRONZE', 'SILVER', 'GOLD']</code> <p>The type of dataset to filter by.</p> required <code>dataset_id</code> <code>Optional[int], default=None</code> <p>Optional dataset ID to fetch a specific record. If provided, a single record is returned; otherwise, all matching records are returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list[ctlDatasetMaster], ctlDatasetMaster]</code> <p>A list of dataset master records if no dataset_id is provided, or a single dataset master record if dataset_id is specified. The results are ordered by dataset_id in ascending order.</p> Note <p>Uses SQLAlchemy's select statement to query the ctlDatasetMaster table. Results are ordered by dataset_id.asc().</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_dataset_master(\n    self,\n    process_id: int,\n    dataset_type: Literal[\"BRONZE\", \"SILVER\", \"GOLD\"],\n    dataset_id: Optional[int] = None,\n) -&gt; Union[list[ctlDatasetMaster], ctlDatasetMaster]:\n    \"\"\"Retrieve dataset master records based on process ID and dataset type.\n\n    Args:\n        process_id (int): The ID of the process associated with the dataset.\n        dataset_type (Literal[\"BRONZE\", \"SILVER\", \"GOLD\"]): The type of dataset to filter by.\n        dataset_id (Optional[int], default=None): Optional dataset ID to fetch a specific record. If provided,\n            a single record is returned; otherwise, all matching records are returned.\n\n    Returns:\n        A list of dataset master records if no dataset_id is provided, or a single dataset master record if dataset_id is specified. The results are ordered by dataset_id in ascending order.\n\n    Note:\n        Uses SQLAlchemy's select statement to query the ctlDatasetMaster table. Results are ordered by dataset_id.asc().\n    \"\"\"\n    if dataset_id is not None:\n        query = (\n            select(ctlDatasetMaster)\n            .where(\n                (ctlDatasetMaster.process_id == process_id)\n                &amp; (ctlDatasetMaster.dataset_id == dataset_id)\n                &amp; (ctlDatasetMaster.dataset_type == dataset_type)\n            )\n            .order_by(ctlDatasetMaster.dataset_id.asc())\n        )\n        result = self.session.exec(query).first()\n        return result\n    else:\n        query = (\n            select(ctlDatasetMaster)\n            .where(\n                (ctlDatasetMaster.process_id == process_id)\n                &amp; (ctlDatasetMaster.dataset_type == dataset_type)\n            )\n            .order_by(ctlDatasetMaster.dataset_id.asc())\n        )\n        result = self.session.exec(query).all()\n        return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_data_standardisation_unprocessed_files","title":"<code>get_data_standardisation_unprocessed_files(process_id, dataset_id)</code>","text":"<p>Retrieve unprocessed files for data standardisation that have succeeded in raw processing.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>Identifier for the process.</p> required <code>dataset_id</code> <code>int</code> <p>Identifier for the dataset.</p> required <p>Returns:</p> Type Description <code>list[logRawProcessDtl]</code> <p>List of logRawProcessDtl entries where the source_file has not been processed in data standardisation, and the raw processing status is \"SUCCEEDED\", filtered by the given process_id and dataset_id. Results are ordered by batch_id ascending.</p> Description <p>This method queries for files that have completed raw processing successfully but have not yet been handled by data standardisation. It excludes files already marked as succeeded in the logDataStandardisationDtl table. The results are sorted by batch ID to ensure consistent ordering.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_data_standardisation_unprocessed_files(\n    self, process_id: int, dataset_id: int\n) -&gt; list[logRawProcessDtl]:\n    \"\"\"Retrieve unprocessed files for data standardisation that have succeeded in raw processing.\n\n    Args:\n        process_id (int): Identifier for the process.\n        dataset_id (int): Identifier for the dataset.\n\n    Returns:\n        (list[logRawProcessDtl]): List of logRawProcessDtl entries where the source_file\n            has not been processed in data standardisation, and the raw processing\n            status is \"SUCCEEDED\", filtered by the given process_id and dataset_id.\n            Results are ordered by batch_id ascending.\n\n    Description:\n        This method queries for files that have completed raw processing successfully\n        but have not yet been handled by data standardisation. It excludes files\n        already marked as succeeded in the logDataStandardisationDtl table. The results\n        are sorted by batch ID to ensure consistent ordering.\n    \"\"\"\n    subquery = select(logDataStandardisationDtl.source_file).where(\n        logDataStandardisationDtl.status == \"SUCCEEDED\"\n    )\n\n    query = (\n        select(logRawProcessDtl)\n        .filter(\n            ~logRawProcessDtl.source_file.in_(subquery),\n            logRawProcessDtl.process_id == process_id,\n            logRawProcessDtl.dataset_id == dataset_id,\n            logRawProcessDtl.file_status == \"SUCCEEDED\",\n        )\n        .order_by(logRawProcessDtl.batch_id.asc())\n    )\n    results = self.session.exec(query).all()\n    return results\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_data_standard_dtl","title":"<code>get_data_standard_dtl(dataset_id=None)</code>","text":"<p>Retrieve standardization details for a specific dataset.</p> <p>Queries the database to retrieve all records from the ctlDataStandardisationDtl table where the dataset_id matches the provided value. If no dataset_id is provided, returns all records.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>Optional[int]</code> <p>The ID of the dataset to filter results. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ctlDataStandardisationDtl]</code> <p>A list of standardization details objects matching the dataset_id.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_data_standard_dtl(\n    self,\n    dataset_id: Optional[int] = None,\n) -&gt; list[ctlDataStandardisationDtl]:\n    \"\"\"Retrieve standardization details for a specific dataset.\n\n    Queries the database to retrieve all records from the ctlDataStandardisationDtl table where the dataset_id matches the provided value. If no dataset_id is provided, returns all records.\n\n    Args:\n        dataset_id (Optional[int]): The ID of the dataset to filter results. Defaults to None.\n\n    Returns:\n        (list[ctlDataStandardisationDtl]): A list of standardization details objects matching the dataset_id.\n    \"\"\"\n    query = select(ctlDataStandardisationDtl).where(\n        ctlDataStandardisationDtl.dataset_id == dataset_id\n    )\n    result = self.session.exec(query).all()\n    return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_dqm_unprocessed_files","title":"<code>get_dqm_unprocessed_files(process_id, dataset_id)</code>","text":"<p>Retrieve a list of successfully processed data standardization files that have not yet been processed by DQM.</p> <p>This method queries the database to find entries from <code>logDataStandardisationDtl</code> that: - Belong to the specified <code>process_id</code> and <code>dataset_id</code> - Have a status of \"SUCCEEDED\" - Do not appear in the <code>logDqmDtl</code> table with a status of \"SUCCEEDED\"</p> <p>The results are ordered by ascending <code>batch_id</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The ID of the process to filter files.</p> required <code>dataset_id</code> <code>int</code> <p>The ID of the dataset to filter files.</p> required <p>Returns:</p> Type Description <code>List[logDqmDtl]</code> <p>A list of log objects representing unprocessed files in the context of DQM.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_dqm_unprocessed_files(\n    self, process_id: int, dataset_id: int\n) -&gt; list[logDqmDtl]:\n    \"\"\"\n    Retrieve a list of successfully processed data standardization files that have not yet been processed by DQM.\n\n    This method queries the database to find entries from `logDataStandardisationDtl` that:\n    - Belong to the specified `process_id` and `dataset_id`\n    - Have a status of \"SUCCEEDED\"\n    - Do not appear in the `logDqmDtl` table with a status of \"SUCCEEDED\"\n\n    The results are ordered by ascending `batch_id`.\n\n    Args:\n        process_id (int): The ID of the process to filter files.\n        dataset_id (int): The ID of the dataset to filter files.\n\n    Returns:\n        (List[logDqmDtl]): A list of log objects representing unprocessed files in the context of DQM.\n    \"\"\"\n\n    subquery = select(logDqmDtl.source_file).where(logDqmDtl.status == \"SUCCEEDED\")\n\n    query = (\n        select(logDataStandardisationDtl)\n        .filter(\n            ~logDataStandardisationDtl.source_file.in_(subquery),\n            logDataStandardisationDtl.process_id == process_id,\n            logDataStandardisationDtl.dataset_id == dataset_id,\n            logDataStandardisationDtl.status == \"SUCCEEDED\",\n        )\n        .order_by(logDataStandardisationDtl.batch_id.asc())\n    )\n    results = self.session.exec(query).all()\n    return results\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_dqm_detail","title":"<code>get_dqm_detail(process_id, dataset_id)</code>","text":"<p>Retrieve DQM (Data Quality Management) master detail records for the specified process and dataset.</p> <p>This method queries the database to fetch all entries from <code>ctlDqmMasterDtl</code> that match the given <code>process_id</code> and <code>dataset_id</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The unique identifier of the process.</p> required <code>dataset_id</code> <code>int</code> <p>The unique identifier of the dataset.</p> required <p>Returns:</p> Type Description <code>List[ctlDqmMasterDtl]</code> <p>A list of DQM master detail records matching the provided criteria.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_dqm_detail(self, process_id: int, dataset_id: int) -&gt; list[ctlDqmMasterDtl]:\n    \"\"\"\n    Retrieve DQM (Data Quality Management) master detail records for the specified process and dataset.\n\n    This method queries the database to fetch all entries from `ctlDqmMasterDtl` that match the given\n    `process_id` and `dataset_id`.\n\n    Args:\n        process_id (int): The unique identifier of the process.\n        dataset_id (int): The unique identifier of the dataset.\n\n    Returns:\n        (List[ctlDqmMasterDtl]): A list of DQM master detail records matching the provided criteria.\n    \"\"\"\n    query = select(ctlDqmMasterDtl).where(\n        (ctlDqmMasterDtl.dataset_id == dataset_id)\n        &amp; (ctlDqmMasterDtl.process_id == process_id)\n    )\n    result = self.session.exec(query).all()\n    return result\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.insert_log_dqm","title":"<code>insert_log_dqm(log_dqm)</code>","text":"<p>Insert a new DQM (Data Quality Management) log entry into the database.</p> <p>This method adds the provided <code>logDqmDtl</code> object to the current database session and commits the transaction, effectively persisting the log record to the database.</p> <p>Parameters:</p> Name Type Description Default <code>log_dqm</code> <code>logDqmDtl</code> <p>An instance of the <code>logDqmDtl</code> model containing the DQM log data to be inserted.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def insert_log_dqm(self, log_dqm: logDqmDtl) -&gt; None:\n    \"\"\"\n    Insert a new DQM (Data Quality Management) log entry into the database.\n\n    This method adds the provided `logDqmDtl` object to the current database session and commits the transaction,\n    effectively persisting the log record to the database.\n\n    Args:\n        log_dqm (logDqmDtl): An instance of the `logDqmDtl` model containing the DQM log data to be inserted.\n\n    Returns:\n        None\n    \"\"\"\n\n    self.session.add(log_dqm)\n    self.session.commit()\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_transformation_dependency_master","title":"<code>get_transformation_dependency_master(process_id, dataset_id)</code>","text":"<p>Retrieve transformation dependency master records for the specified process and dataset.</p> <p>This method queries the database to fetch all entries from <code>ctlTransformationDependencyMaster</code> that match the provided <code>process_id</code> and <code>dataset_id</code>. The results are ordered by the <code>transformation_step</code> field in ascending order.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The ID of the process associated with the transformation dependencies.</p> required <code>dataset_id</code> <code>int</code> <p>The ID of the dataset associated with the transformation dependencies.</p> required <p>Returns:</p> Type Description <code>List[ctlTransformationDependencyMaster]</code> <p>A list of transformation dependency records                                        sorted by transformation step.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_transformation_dependency_master(\n    self, process_id: int, dataset_id: int\n) -&gt; list[ctlTransformationDependencyMaster]:\n    \"\"\"\n    Retrieve transformation dependency master records for the specified process and dataset.\n\n    This method queries the database to fetch all entries from `ctlTransformationDependencyMaster`\n    that match the provided `process_id` and `dataset_id`. The results are ordered by the\n    `transformation_step` field in ascending order.\n\n    Args:\n        process_id (int): The ID of the process associated with the transformation dependencies.\n        dataset_id (int): The ID of the dataset associated with the transformation dependencies.\n\n    Returns:\n        (List[ctlTransformationDependencyMaster]): A list of transformation dependency records\n                                                   sorted by transformation step.\n    \"\"\"\n\n    query = (\n        select(ctlTransformationDependencyMaster)\n        .where(\n            (ctlTransformationDependencyMaster.process_id == process_id)\n            &amp; (ctlTransformationDependencyMaster.dataset_id == dataset_id)\n        )\n        .order_by(ctlTransformationDependencyMaster.transformation_step)\n    )\n    results = self.session.exec(query).all()\n    return results\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_unprocessed_transformation_files","title":"<code>get_unprocessed_transformation_files(process_id, dataset_id)</code>","text":"<p>Retrieve a list of files that have not yet been successfully processed by the transformation step.</p> <p>This method identifies files in the <code>logDqmDtl</code> table that: - Belong to the specified <code>process_id</code> and <code>dataset_id</code> - Have not appeared in the <code>logTransformationDtl</code> table with a status of \"SUCCEEDED\"</p> <p>These files are considered unprocessed from a transformation perspective and may require further processing.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The ID of the process to filter files.</p> required <code>dataset_id</code> <code>int</code> <p>The ID of the dataset to filter files.</p> required <p>Returns:</p> Type Description <code>list[logTransformationDtl]</code> <p>A list of transformation detail objects representing unprocessed files.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_unprocessed_transformation_files(\n    self, process_id, dataset_id\n) -&gt; list[logTransformationDtl]:\n    \"\"\"\n    Retrieve a list of files that have not yet been successfully processed by the transformation step.\n\n    This method identifies files in the `logDqmDtl` table that:\n    - Belong to the specified `process_id` and `dataset_id`\n    - Have not appeared in the `logTransformationDtl` table with a status of \"SUCCEEDED\"\n\n    These files are considered unprocessed from a transformation perspective and may require further processing.\n\n    Args:\n        process_id (int): The ID of the process to filter files.\n        dataset_id (int): The ID of the dataset to filter files.\n\n    Returns:\n        (list[logTransformationDtl]): A list of transformation detail objects representing unprocessed files.\n    \"\"\"\n\n    subquery = select(logTransformationDtl.source_file).where(\n        logTransformationDtl.status == \"SUCCEEDED\"\n    )\n    query = select(logDqmDtl).filter(\n        ~logDqmDtl.source_file.in_(subquery),\n        logDqmDtl.process_id == process_id,\n        logDqmDtl.dataset_id == dataset_id,\n    )\n    results = self.session.exec(query).all()\n    return results\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.insert_log_transformation","title":"<code>insert_log_transformation(log_transformation)</code>","text":"<p>Insert a new transformation log entry into the database.</p> <p>This method adds the provided <code>logTransformationDtl</code> object to the current database session and commits the transaction, persisting the transformation log record to the database.</p> <p>Parameters:</p> Name Type Description Default <code>log_transformation</code> <code>logTransformationDtl</code> <p>An instance of the <code>logTransformationDtl</code> model containing the transformation log data to be inserted.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def insert_log_transformation(\n    self, log_transformation: logTransformationDtl\n) -&gt; None:\n    \"\"\"\n    Insert a new transformation log entry into the database.\n\n    This method adds the provided `logTransformationDtl` object to the current database session\n    and commits the transaction, persisting the transformation log record to the database.\n\n    Args:\n        log_transformation (logTransformationDtl): An instance of the `logTransformationDtl` model\n            containing the transformation log data to be inserted.\n\n    Returns:\n        None\n    \"\"\"\n\n    self.session.add(log_transformation)\n    self.session.commit()\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_transformation_dqm_unprocessed_files","title":"<code>get_transformation_dqm_unprocessed_files(process_id, dataset_id)</code>","text":"<p>Retrieve transformation files that have not yet been successfully processed by DQM.</p> <p>This method identifies entries in the <code>logTransformationDtl</code> table that: - Belong to the specified <code>process_id</code> and <code>dataset_id</code> - Have a status of \"SUCCEEDED\" - Do not appear in the <code>logDqmDtl</code> table with a status of \"SUCCEEDED\" for the same dataset</p> <p>The results are ordered by ascending <code>batch_id</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The ID of the process to filter transformation files.</p> required <code>dataset_id</code> <code>int</code> <p>The ID of the dataset to filter transformation files.</p> required <p>Returns:</p> Type Description <code>List[logTransformationDtl]</code> <p>A list of transformation log records that are unprocessed by DQM.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_transformation_dqm_unprocessed_files(\n    self, process_id, dataset_id\n) -&gt; list[logTransformationDtl]:\n    \"\"\"\n    Retrieve transformation files that have not yet been successfully processed by DQM.\n\n    This method identifies entries in the `logTransformationDtl` table that:\n    - Belong to the specified `process_id` and `dataset_id`\n    - Have a status of \"SUCCEEDED\"\n    - Do not appear in the `logDqmDtl` table with a status of \"SUCCEEDED\" for the same dataset\n\n    The results are ordered by ascending `batch_id`.\n\n    Args:\n        process_id (int): The ID of the process to filter transformation files.\n        dataset_id (int): The ID of the dataset to filter transformation files.\n\n    Returns:\n        (List[logTransformationDtl]): A list of transformation log records that are unprocessed by DQM.\n    \"\"\"\n\n    subquery = select(logDqmDtl.source_file).where(\n        (logDqmDtl.status == \"SUCCEEDED\") &amp; (logDqmDtl.dataset_id == dataset_id)\n    )\n\n    query = (\n        select(logTransformationDtl)\n        .filter(\n            ~logTransformationDtl.source_file.in_(subquery),\n            logTransformationDtl.process_id == process_id,\n            logTransformationDtl.dataset_id == dataset_id,\n            logTransformationDtl.status == \"SUCCEEDED\",\n        )\n        .order_by(logTransformationDtl.batch_id.asc())\n    )\n    results = self.session.exec(query).all()\n    return results\n</code></pre>"},{"location":"Common/OrchestrationProcess/#datacraft_framework.Common.OrchestrationProcess.OrchestrationProcess.get_gold_datasets","title":"<code>get_gold_datasets()</code>","text":"<p>Retrieve all GOLD-type datasets from the dataset master table.</p> <p>This method queries the <code>ctlDatasetMaster</code> table to fetch records where the <code>dataset_type</code> is \"GOLD\". The results are ordered by <code>dataset_id</code> in ascending order.</p> <p>Returns:</p> Type Description <code>List[ctlDatasetMaster]</code> <p>A list of dataset master records representing GOLD-type datasets.</p> Source code in <code>src/datacraft_framework/Common/OrchestrationProcess.py</code> <pre><code>def get_gold_datasets(self) -&gt; list[ctlDatasetMaster]:\n    \"\"\"\n    Retrieve all GOLD-type datasets from the dataset master table.\n\n    This method queries the `ctlDatasetMaster` table to fetch records where the\n    `dataset_type` is \"GOLD\". The results are ordered by `dataset_id` in ascending order.\n\n    Returns:\n        (List[ctlDatasetMaster]): A list of dataset master records representing GOLD-type datasets.\n    \"\"\"\n    query = (\n        select(ctlDatasetMaster)\n        .where(ctlDatasetMaster.dataset_type == \"GOLD\")\n        .order_by(ctlDatasetMaster.dataset_id)\n    )\n\n    results = self.session.exec(query).all()\n    return results\n</code></pre>"},{"location":"Common/PatternValidator/","title":"Documentation for <code>PatternValidator</code>","text":""},{"location":"Common/PatternValidator/#datacraft_framework.Common.PatternValidator.validate_pattern","title":"<code>datacraft_framework.Common.PatternValidator.validate_pattern(file_pattern, file_name, custom=False)</code>","text":"<p>Validate whether a given file name matches a specified file naming pattern.</p> <p>This function supports dynamic date-based patterns (<code>YYYY</code>, <code>YYYYMM</code>, <code>YYYYMMDD</code>) and wildcards (<code>*</code>). If <code>custom=True</code>, it treats the pattern as a raw regular expression and matches directly.</p> Supported replacements <ul> <li><code>YYYYMMDD</code> \u2192 <code>[0-9]{8}</code> (e.g., 20250405)</li> <li><code>YYYYMM</code>   \u2192 <code>[0-9]{6}</code> (e.g., 202504)</li> <li><code>YYYY</code>     \u2192 <code>[0-9]{4}</code> (e.g., 2025)</li> </ul> Wildcard support <ul> <li><code>*</code> is replaced with <code>.*</code> in regex for flexible matching</li> </ul> <p>Parameters:</p> Name Type Description Default <code>file_pattern</code> <code>str</code> <p>The pattern to validate against. May contain date placeholders or be a regex.</p> required <code>file_name</code> <code>str</code> <p>The actual file name to validate.</p> required <code>custom</code> <code>bool</code> <p>If True, treat <code>file_pattern</code> as a full regex string. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file name matches the pattern, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validate_pattern(\"data_YYYYMMDD.csv\", \"data_20250405.csv\")\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; validate_pattern(\"report_*.csv\", \"report_20250405.csv\")\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; validate_pattern(r\"data_\\d{8}\\.csv\", \"data_20250405.csv\", custom=True)\nTrue\n</code></pre> Source code in <code>src/datacraft_framework/Common/PatternValidator.py</code> <pre><code>def validate_pattern(file_pattern: str, file_name: str, custom: bool = False) -&gt; bool:\n    \"\"\"\n    Validate whether a given file name matches a specified file naming pattern.\n\n    This function supports dynamic date-based patterns (`YYYY`, `YYYYMM`, `YYYYMMDD`) and wildcards (`*`).\n    If `custom=True`, it treats the pattern as a raw regular expression and matches directly.\n\n    Supported replacements:\n        - `YYYYMMDD` \u2192 `[0-9]{8}` (e.g., 20250405)\n        - `YYYYMM`   \u2192 `[0-9]{6}` (e.g., 202504)\n        - `YYYY`     \u2192 `[0-9]{4}` (e.g., 2025)\n\n    Wildcard support:\n        - `*` is replaced with `.*` in regex for flexible matching\n\n    Args:\n        file_pattern (str): The pattern to validate against. May contain date placeholders or be a regex.\n        file_name (str): The actual file name to validate.\n        custom (bool, optional): If True, treat `file_pattern` as a full regex string. Defaults to False.\n\n    Returns:\n        bool: True if the file name matches the pattern, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; validate_pattern(\"data_YYYYMMDD.csv\", \"data_20250405.csv\")\n        True\n\n        &gt;&gt;&gt; validate_pattern(\"report_*.csv\", \"report_20250405.csv\")\n        True\n\n        &gt;&gt;&gt; validate_pattern(r\"data_\\\\d{8}\\\\.csv\", \"data_20250405.csv\", custom=True)\n        True\n    \"\"\"\n    if not custom:\n        if \"YYYYMMDD\" in file_pattern:\n            if \"*\" in file_pattern:\n                regex_pattern_, other_pattern = file_pattern.split(\"*\")\n                regex_pattern = (\n                    regex_pattern_\n                    + \".*\"\n                    + other_pattern.replace(\"YYYYMMDD\", \"[0-9]{8}\")\n                )\n                if regex_pattern_matches(regex_pattern, file_name):\n                    return True\n                else:\n                    return False\n\n            else:\n                regex_pattern = file_pattern.replace(\"YYYYMMDD\", \"[0-9]{8}\")\n\n                if regex_pattern_matches(regex_pattern, file_name):\n                    return True\n                else:\n                    return False\n\n        elif \"YYYYMM\" in file_pattern:\n            if \"*\" in file_pattern:\n                regex_pattern_, other_pattern = file_pattern.split(\"*\")\n                regex_pattern = (\n                    regex_pattern_ + \".*\" + other_pattern.replace(\"YYYYMM\", \"[0-9]{6}\")\n                )\n                if regex_pattern_matches(regex_pattern, file_name):\n                    return True\n                else:\n                    return False\n\n            else:\n                regex_pattern = file_pattern.replace(\"YYYYMM\", \"[0-9]{6}\")\n\n                if regex_pattern_matches(regex_pattern, file_name):\n                    return True\n                else:\n                    return False\n\n        elif \"YYYY\" in file_pattern:\n            if \"*\" in file_pattern:\n                regex_pattern_, other_pattern = file_pattern.split(\"*\")\n                regex_pattern = (\n                    regex_pattern_ + \".*\" + other_pattern.replace(\"YYYY\", \"[0-9]{4}\")\n                )\n                if regex_pattern_matches(regex_pattern, file_name):\n                    return True\n                else:\n                    return False\n\n            else:\n                regex_pattern = file_pattern.replace(\"YYYY\", \"[0-9]{4}\")\n\n                if regex_pattern_matches(regex_pattern, file_name):\n                    return True\n                else:\n                    return False\n\n    else:\n        if regex_pattern_matches(file_pattern, file_name):\n            return True\n        else:\n            return False\n</code></pre>"},{"location":"Common/RegexDateFormats/","title":"Documentation for <code>RegexDateFormats</code>","text":""},{"location":"Common/RegexDateFormats/#datacraft_framework.Common.RegexDateFormats.get_date_regex","title":"<code>datacraft_framework.Common.RegexDateFormats.get_date_regex(qc_param)</code>","text":"<p>Return a regex pattern corresponding to the specified date/time format.</p> <p>This function maps common date and datetime format strings to their corresponding regular expression patterns for validation or parsing purposes. It supports multiple datetime formats including ISO 8601 variants, US date formats, and custom timestamp formats.</p> Supported Formats and Their Regex Equivalents <ul> <li><code>%Y-%m-%dT%H:%M:%S+0000</code> \u2192 ISO 8601 basic datetime with offset</li> <li><code>%Y</code> \u2192 4-digit year</li> <li><code>%Y-%m-%dT%H:%M:%S.%f+0000</code> \u2192 ISO 8601 datetime with milliseconds and offset</li> <li><code>MM/DD/YYYY</code> \u2192 U.S. date format</li> <li><code>YYYY-MM-DD HH24:MI:SS</code> \u2192 Standard SQL date-time format</li> <li><code>%Y-%m-%dT%H:%M:%S.000Z</code> \u2192 UTC Zulu time format</li> <li><code>YYYYMMDD</code> \u2192 Compact 8-digit date</li> <li><code>yyyy-MM-dd HH:mm:ss.nnnnnnn {+|-}hh:mm</code> \u2192 Extended .NET datetime format</li> </ul> <p>Parameters:</p> Name Type Description Default <code>qc_param</code> <code>str</code> <p>The date format string to match against known patterns.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A regex pattern that matches the provided date/time format.  Defaults to <code>MM/DD/YYYY</code> if no match is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_date_regex(\"%Y-%m-%dT%H:%M:%S+0000\")\n'([0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\+[0-9]{4})'\n</code></pre> <pre><code>&gt;&gt;&gt; get_date_regex(\"MM/DD/YYYY\")\n'([0-9]{2}/[0-9]{2}/[0-9]{4})'\n</code></pre> Source code in <code>src/datacraft_framework/Common/RegexDateFormats.py</code> <pre><code>def get_date_regex(qc_param: str) -&gt; str:\n    \"\"\"\n    Return a regex pattern corresponding to the specified date/time format.\n\n    This function maps common date and datetime format strings to their corresponding\n    regular expression patterns for validation or parsing purposes. It supports multiple\n    datetime formats including ISO 8601 variants, US date formats, and custom timestamp formats.\n\n    Supported Formats and Their Regex Equivalents:\n        - `%Y-%m-%dT%H:%M:%S+0000` \u2192 ISO 8601 basic datetime with offset\n        - `%Y` \u2192 4-digit year\n        - `%Y-%m-%dT%H:%M:%S.%f+0000` \u2192 ISO 8601 datetime with milliseconds and offset\n        - `MM/DD/YYYY` \u2192 U.S. date format\n        - `YYYY-MM-DD HH24:MI:SS` \u2192 Standard SQL date-time format\n        - `%Y-%m-%dT%H:%M:%S.000Z` \u2192 UTC Zulu time format\n        - `YYYYMMDD` \u2192 Compact 8-digit date\n        - `yyyy-MM-dd HH:mm:ss.nnnnnnn {+|-}hh:mm` \u2192 Extended .NET datetime format\n\n    Args:\n        qc_param (str): The date format string to match against known patterns.\n\n    Returns:\n        str: A regex pattern that matches the provided date/time format.\n             Defaults to `MM/DD/YYYY` if no match is found.\n\n    Examples:\n        &gt;&gt;&gt; get_date_regex(\"%Y-%m-%dT%H:%M:%S+0000\")\n        '([0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\\\+[0-9]{4})'\n\n        &gt;&gt;&gt; get_date_regex(\"MM/DD/YYYY\")\n        '([0-9]{2}/[0-9]{2}/[0-9]{4})'\n    \"\"\"\n    DATE_TIME_FORMAT_1 = r\"%Y-%m-%dT%H:%M:%S+0000\"\n    DATE_TIME_FORMAT_1_REGEX = (\n        r\"([0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\+[0-9]{4})\"\n    )\n\n    DATE_TIME_FORMAT_2 = \"%Y\"\n    DATE_TIME_FORMAT_2_REGEX = r\"([0-9]{4})\"\n\n    DATE_TIME_FORMAT_3 = r\"%Y-%m-%dT%H:%M:%S.%f+0000\"\n    DATE_TIME_FORMAT_3_REGEX = (\n        r\"([0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}.[0-9]{3}\\+[0-9]{4})\"\n    )\n\n    DATE_TIME_FORMAT_4 = \"MM/DD/YYYY\"\n    DATE_TIME_FORMAT_4_REGEX = r\"([0-9]{2}/[0-9]{2}/[0-9]{4})\"\n\n    DATE_TIME_FORMAT_5 = \"YYYY-MM-DD HH24:MI:SS\"\n    DATE_TIME_FORMAT_5_REGEX = \"([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})\"\n\n    DATE_TIME_FORMAT_6 = \"%Y-%m-%dT%H:%M:%S.000Z\"\n    DATE_TIME_FORMAT_6_REGEX = (\n        r\"([0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}.[0-9]{3}Z)\"\n    )\n\n    DATE_TIME_FORMAT_7 = \"YYYYMMDD\"\n    DATE_TIME_FORMAT_7_REGEX = r\"([0-9]{4})([0-9]{2})([0-9]{2})\"\n\n    DATE_TIME_FORMAT_8 = \"yyyy-MM-dd HH:mm:ss.nnnnnnn {+|-}hh:mm\"\n    DATE_TIME_FORMAT_8_REGEX = r\"([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9]{1,7}? [+-][0-9]{2}:[0-9]{2})\"\n\n    DATE_TIME_FORMAT_DEFAULT = \"MM/DD/YYYY\"\n    DATE_TIME_FORMAT_DEFAULT_REGEX = r\"([0-9]{2}/[0-9]{2}/[0-9]{4})\"\n\n    # INTEGER_REGEX = \"(^-?\\d+$)\"\n    # DECIMAL_REGEX = \"(^-?\\d*[.]?\\d+$)\"\n\n    if qc_param == DATE_TIME_FORMAT_1:\n        regex = DATE_TIME_FORMAT_1_REGEX\n    elif qc_param == DATE_TIME_FORMAT_2:\n        regex = DATE_TIME_FORMAT_2_REGEX\n    elif qc_param == DATE_TIME_FORMAT_3:\n        regex = DATE_TIME_FORMAT_3_REGEX\n    elif qc_param == DATE_TIME_FORMAT_4:\n        regex = DATE_TIME_FORMAT_4_REGEX\n    elif qc_param == DATE_TIME_FORMAT_5:\n        regex = DATE_TIME_FORMAT_5_REGEX\n    elif qc_param == DATE_TIME_FORMAT_6:\n        regex = DATE_TIME_FORMAT_6_REGEX\n    elif qc_param == DATE_TIME_FORMAT_7:\n        regex = DATE_TIME_FORMAT_7_REGEX\n    elif qc_param == DATE_TIME_FORMAT_8:\n        regex = DATE_TIME_FORMAT_8_REGEX\n    else:\n        regex = DATE_TIME_FORMAT_DEFAULT_REGEX\n        return regex\n</code></pre>"},{"location":"Common/S3Process/","title":"Documentation for <code>S3Process</code>","text":""},{"location":"Common/S3Process/#datacraft_framework.Common.S3Process.S3Process","title":"<code>datacraft_framework.Common.S3Process.S3Process</code>","text":"<p>A class to interact with Amazon S3 for file operations such as upload and listing files.</p> <p>This class initializes a connection to S3 using provided AWS credentials and endpoint, and provides methods to upload files and list objects in a specified bucket.</p> <p>Attributes:</p> Name Type Description <code>s3_client</code> <code>client</code> <p>A low-level client representing Amazon S3.</p> Source code in <code>src/datacraft_framework/Common/S3Process.py</code> <pre><code>class S3Process:\n    \"\"\"\n    A class to interact with Amazon S3 for file operations such as upload and listing files.\n\n    This class initializes a connection to S3 using provided AWS credentials and endpoint,\n    and provides methods to upload files and list objects in a specified bucket.\n\n    Attributes:\n        s3_client (boto3.client): A low-level client representing Amazon S3.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize an S3 client using global AWS credentials and endpoint.\n\n        Uses the following global variables:\n            - aws_key: AWS access key ID\n            - aws_secret: AWS secret access key\n            - aws_endpoint: Custom endpoint URL for S3-compatible services\n\n        Examples:\n            &gt;&gt;&gt; s3 = S3Process()\n        \"\"\"\n        self.s3_client = boto3.client(\n            \"s3\",\n            aws_access_key_id=aws_key,\n            aws_secret_access_key=aws_secret,\n            endpoint_url=aws_endpoint,\n        )\n\n    def s3_raw_file_write(self, file_object, bucket, file_name) -&gt; None:\n        \"\"\"\n        Upload a file-like object to an S3 bucket.\n\n        Args:\n            file_object (Any): A file-like object to upload. Must be readable and seekable.\n            bucket (str): Name of the S3 bucket to upload to.\n            file_name (str): The destination key (path/filename) inside the bucket.\n\n        Returns:\n            None\n\n        Raises:\n            botocore.exceptions.ClientError: If the upload fails due to network issues or permissions.\n\n        Examples:\n            &gt;&gt;&gt; with open(\"local_file.csv\", \"rb\") as f:\n            ...     s3.s3_raw_file_write(f, \"my-bucket\", \"path/to/remote_file.csv\")\n        \"\"\"\n\n        self.s3_client.upload_fileobj(\n            Fileobj=file_object,\n            Bucket=bucket,\n            Key=file_name,\n        )\n\n    def s3_list_files(self, bucket, file_name) -&gt; Union[List[str], bool]:\n        \"\"\"\n        List all files under a specific prefix in an S3 bucket.\n\n        Args:\n            bucket (str): Name of the S3 bucket to search in.\n            file_name (str): Prefix used to filter objects (e.g., directory path or filename pattern).\n\n        Returns:\n            list[str] | bool: A list of matching object keys if found; False otherwise.\n\n        Examples:\n            &gt;&gt;&gt; s3.s3_list_files(\"my-bucket\", \"data/input/\")\n            ['data/input/file1.csv', 'data/input/file2.csv']\n        \"\"\"\n        files = self.s3_client.list_objects_v2(Bucket=bucket, Prefix=file_name).get(\n            \"Contents\"\n        )\n        if files:\n            return [x[\"Key\"] for x in files]\n        else:\n            return False\n</code></pre>"},{"location":"Common/S3Process/#datacraft_framework.Common.S3Process.S3Process.__init__","title":"<code>__init__()</code>","text":"<p>Initialize an S3 client using global AWS credentials and endpoint.</p> Uses the following global variables <ul> <li>aws_key: AWS access key ID</li> <li>aws_secret: AWS secret access key</li> <li>aws_endpoint: Custom endpoint URL for S3-compatible services</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s3 = S3Process()\n</code></pre> Source code in <code>src/datacraft_framework/Common/S3Process.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize an S3 client using global AWS credentials and endpoint.\n\n    Uses the following global variables:\n        - aws_key: AWS access key ID\n        - aws_secret: AWS secret access key\n        - aws_endpoint: Custom endpoint URL for S3-compatible services\n\n    Examples:\n        &gt;&gt;&gt; s3 = S3Process()\n    \"\"\"\n    self.s3_client = boto3.client(\n        \"s3\",\n        aws_access_key_id=aws_key,\n        aws_secret_access_key=aws_secret,\n        endpoint_url=aws_endpoint,\n    )\n</code></pre>"},{"location":"Common/S3Process/#datacraft_framework.Common.S3Process.S3Process.s3_raw_file_write","title":"<code>s3_raw_file_write(file_object, bucket, file_name)</code>","text":"<p>Upload a file-like object to an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>file_object</code> <code>Any</code> <p>A file-like object to upload. Must be readable and seekable.</p> required <code>bucket</code> <code>str</code> <p>Name of the S3 bucket to upload to.</p> required <code>file_name</code> <code>str</code> <p>The destination key (path/filename) inside the bucket.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>If the upload fails due to network issues or permissions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with open(\"local_file.csv\", \"rb\") as f:\n...     s3.s3_raw_file_write(f, \"my-bucket\", \"path/to/remote_file.csv\")\n</code></pre> Source code in <code>src/datacraft_framework/Common/S3Process.py</code> <pre><code>def s3_raw_file_write(self, file_object, bucket, file_name) -&gt; None:\n    \"\"\"\n    Upload a file-like object to an S3 bucket.\n\n    Args:\n        file_object (Any): A file-like object to upload. Must be readable and seekable.\n        bucket (str): Name of the S3 bucket to upload to.\n        file_name (str): The destination key (path/filename) inside the bucket.\n\n    Returns:\n        None\n\n    Raises:\n        botocore.exceptions.ClientError: If the upload fails due to network issues or permissions.\n\n    Examples:\n        &gt;&gt;&gt; with open(\"local_file.csv\", \"rb\") as f:\n        ...     s3.s3_raw_file_write(f, \"my-bucket\", \"path/to/remote_file.csv\")\n    \"\"\"\n\n    self.s3_client.upload_fileobj(\n        Fileobj=file_object,\n        Bucket=bucket,\n        Key=file_name,\n    )\n</code></pre>"},{"location":"Common/S3Process/#datacraft_framework.Common.S3Process.S3Process.s3_list_files","title":"<code>s3_list_files(bucket, file_name)</code>","text":"<p>List all files under a specific prefix in an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the S3 bucket to search in.</p> required <code>file_name</code> <code>str</code> <p>Prefix used to filter objects (e.g., directory path or filename pattern).</p> required <p>Returns:</p> Type Description <code>Union[List[str], bool]</code> <p>list[str] | bool: A list of matching object keys if found; False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s3.s3_list_files(\"my-bucket\", \"data/input/\")\n['data/input/file1.csv', 'data/input/file2.csv']\n</code></pre> Source code in <code>src/datacraft_framework/Common/S3Process.py</code> <pre><code>def s3_list_files(self, bucket, file_name) -&gt; Union[List[str], bool]:\n    \"\"\"\n    List all files under a specific prefix in an S3 bucket.\n\n    Args:\n        bucket (str): Name of the S3 bucket to search in.\n        file_name (str): Prefix used to filter objects (e.g., directory path or filename pattern).\n\n    Returns:\n        list[str] | bool: A list of matching object keys if found; False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; s3.s3_list_files(\"my-bucket\", \"data/input/\")\n        ['data/input/file1.csv', 'data/input/file2.csv']\n    \"\"\"\n    files = self.s3_client.list_objects_v2(Bucket=bucket, Prefix=file_name).get(\n        \"Contents\"\n    )\n    if files:\n        return [x[\"Key\"] for x in files]\n    else:\n        return False\n</code></pre>"},{"location":"Common/SchemaCaster/","title":"Documentation for <code>SchemaCaster</code>","text":""},{"location":"Common/SchemaCaster/#datacraft_framework.Common.SchemaCaster.SchemaCaster","title":"<code>datacraft_framework.Common.SchemaCaster.SchemaCaster</code>","text":"<p>A class to cast columns of a Polars DataFrame to specified data types based on metadata.</p> <p>This class takes a DataFrame and a list of column metadata objects that define the desired data type (and optionally date format) for each column. It applies casting operations to transform the DataFrame schema accordingly.</p> Source code in <code>src/datacraft_framework/Common/SchemaCaster.py</code> <pre><code>class SchemaCaster:\n    \"\"\"\n    A class to cast columns of a Polars DataFrame to specified data types based on metadata.\n\n    This class takes a DataFrame and a list of column metadata objects that define the desired\n    data type (and optionally date format) for each column. It applies casting operations to\n    transform the DataFrame schema accordingly.\n    \"\"\"\n\n    def __init__(self, df: polars.DataFrame, column_metadata: list[CtlColumnMetadata]):\n        \"\"\"\n        Initialize the SchemaCaster with a DataFrame and column metadata.\n\n        Args:\n            df (polars.DataFrame): The DataFrame whose columns will be cast.\n            column_metadata (List[CtlColumnMetadata]): A list of metadata objects where each object\n                contains at least `column_name` and `column_data_type`. Optionally includes\n                `column_date_format` if the type is 'date'.\n        \"\"\"\n        self.df = df\n        self.column_metadata = column_metadata\n\n    def start(self) -&gt; polars.DataFrame:\n        \"\"\"\n        Apply schema casting operations to the DataFrame based on metadata.\n\n        Iterates through the column metadata and performs the following casts:\n            - `\"integer\"` \u2192 `Int32`\n            - `\"float\"` \u2192 `Float32`\n            - `\"double\"` \u2192 `Float64`\n            - `\"long\"` \u2192 `Int64`\n            - `\"string\"` \u2192 `String`\n            - `\"boolean\"` \u2192 `Boolean`\n            - `\"date\"` \u2192 Converts string to `Date` using the specified format\n\n        Returns:\n            polars.DataFrame: A new DataFrame with columns cast to the specified data types.\n\n        Raises:\n            polars.exceptions.PolarsException: If casting fails due to incompatible data.\n        \"\"\"\n        for metadata in self.column_metadata:\n            if metadata.column_data_type == \"integer\":\n                self.df = self.df.with_columns(\n                    polars.col(metadata.column_name).cast(polars.Int32)\n                )\n            elif metadata.column_data_type == \"float\":\n                self.df = self.df.with_columns(\n                    polars.col(metadata.column_name).cast(polars.Float32)\n                )\n            elif metadata.column_data_type == \"double\":\n                self.df = self.df.with_columns(\n                    polars.col(metadata.column_name).cast(polars.Float64)\n                )\n            elif metadata.column_data_type == \"long\":\n                self.df = self.df.with_columns(\n                    polars.col(metadata.column_name).cast(polars.Int64)\n                )\n            elif metadata.column_data_type == \"string\":\n                self.df = self.df.with_columns(\n                    polars.col(metadata.column_name).cast(polars.String)\n                )\n            elif metadata.column_data_type == \"boolean\":\n                self.df = self.df.with_columns(\n                    polars.col(metadata.column_name).cast(polars.Boolean)\n                )\n\n            elif metadata.column_data_type == \"date\":\n                self.df = self.df.with_columns(\n                    polars.col(metadata.column_name).str.to_date(\n                        format=metadata.column_date_format\n                    )\n                )\n        return self.df\n</code></pre>"},{"location":"Common/SchemaCaster/#datacraft_framework.Common.SchemaCaster.SchemaCaster.__init__","title":"<code>__init__(df, column_metadata)</code>","text":"<p>Initialize the SchemaCaster with a DataFrame and column metadata.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame whose columns will be cast.</p> required <code>column_metadata</code> <code>List[CtlColumnMetadata]</code> <p>A list of metadata objects where each object contains at least <code>column_name</code> and <code>column_data_type</code>. Optionally includes <code>column_date_format</code> if the type is 'date'.</p> required Source code in <code>src/datacraft_framework/Common/SchemaCaster.py</code> <pre><code>def __init__(self, df: polars.DataFrame, column_metadata: list[CtlColumnMetadata]):\n    \"\"\"\n    Initialize the SchemaCaster with a DataFrame and column metadata.\n\n    Args:\n        df (polars.DataFrame): The DataFrame whose columns will be cast.\n        column_metadata (List[CtlColumnMetadata]): A list of metadata objects where each object\n            contains at least `column_name` and `column_data_type`. Optionally includes\n            `column_date_format` if the type is 'date'.\n    \"\"\"\n    self.df = df\n    self.column_metadata = column_metadata\n</code></pre>"},{"location":"Common/SchemaCaster/#datacraft_framework.Common.SchemaCaster.SchemaCaster.start","title":"<code>start()</code>","text":"<p>Apply schema casting operations to the DataFrame based on metadata.</p> Iterates through the column metadata and performs the following casts <ul> <li><code>\"integer\"</code> \u2192 <code>Int32</code></li> <li><code>\"float\"</code> \u2192 <code>Float32</code></li> <li><code>\"double\"</code> \u2192 <code>Float64</code></li> <li><code>\"long\"</code> \u2192 <code>Int64</code></li> <li><code>\"string\"</code> \u2192 <code>String</code></li> <li><code>\"boolean\"</code> \u2192 <code>Boolean</code></li> <li><code>\"date\"</code> \u2192 Converts string to <code>Date</code> using the specified format</li> </ul> <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: A new DataFrame with columns cast to the specified data types.</p> <p>Raises:</p> Type Description <code>PolarsException</code> <p>If casting fails due to incompatible data.</p> Source code in <code>src/datacraft_framework/Common/SchemaCaster.py</code> <pre><code>def start(self) -&gt; polars.DataFrame:\n    \"\"\"\n    Apply schema casting operations to the DataFrame based on metadata.\n\n    Iterates through the column metadata and performs the following casts:\n        - `\"integer\"` \u2192 `Int32`\n        - `\"float\"` \u2192 `Float32`\n        - `\"double\"` \u2192 `Float64`\n        - `\"long\"` \u2192 `Int64`\n        - `\"string\"` \u2192 `String`\n        - `\"boolean\"` \u2192 `Boolean`\n        - `\"date\"` \u2192 Converts string to `Date` using the specified format\n\n    Returns:\n        polars.DataFrame: A new DataFrame with columns cast to the specified data types.\n\n    Raises:\n        polars.exceptions.PolarsException: If casting fails due to incompatible data.\n    \"\"\"\n    for metadata in self.column_metadata:\n        if metadata.column_data_type == \"integer\":\n            self.df = self.df.with_columns(\n                polars.col(metadata.column_name).cast(polars.Int32)\n            )\n        elif metadata.column_data_type == \"float\":\n            self.df = self.df.with_columns(\n                polars.col(metadata.column_name).cast(polars.Float32)\n            )\n        elif metadata.column_data_type == \"double\":\n            self.df = self.df.with_columns(\n                polars.col(metadata.column_name).cast(polars.Float64)\n            )\n        elif metadata.column_data_type == \"long\":\n            self.df = self.df.with_columns(\n                polars.col(metadata.column_name).cast(polars.Int64)\n            )\n        elif metadata.column_data_type == \"string\":\n            self.df = self.df.with_columns(\n                polars.col(metadata.column_name).cast(polars.String)\n            )\n        elif metadata.column_data_type == \"boolean\":\n            self.df = self.df.with_columns(\n                polars.col(metadata.column_name).cast(polars.Boolean)\n            )\n\n        elif metadata.column_data_type == \"date\":\n            self.df = self.df.with_columns(\n                polars.col(metadata.column_name).str.to_date(\n                    format=metadata.column_date_format\n                )\n            )\n    return self.df\n</code></pre>"},{"location":"Examples/Connection%20Config/","title":"Usage for <code>Connection Config</code>","text":""},{"location":"Examples/Connection%20Config/#description","title":"Description","text":"<p>The <code>connection_config</code> field in the configuration table expects a dictionary (JSON format) depending on the type of connection.</p>"},{"location":"Examples/Connection%20Config/#sftp","title":"\ud83d\udd10 SFTP","text":"<p>Example:</p> <pre><code>{\n  \"host\": \"your-sftp-host.com\",\n  \"port\": \"22\",\n  \"user\": \"sftp-username\"\n}\n</code></pre>"},{"location":"Examples/Connection%20Config/#s3","title":"\u2601\ufe0f S3","text":"<p>Example:</p> <pre><code>{\n  \"client_id\": \"your-aws-client-id\",\n  \"client_secret\": \"your-aws-secret-key\",\n  \"endpoint_url\": \"https://s3.custom-endpoint.com\",\n  \"signature_version\": \"s3v4\"\n}\n</code></pre> <p><code>endpoint_url</code> and <code>signature_version</code> are optional.</p>"},{"location":"Examples/Connection%20Config/#database","title":"\ud83d\uddc4\ufe0f Database","text":"<p>Example:</p> <pre><code>{\n  \"url\": \"jdbc:mysql://host:3306\",\n  \"database\": \"your-database-name\",\n  \"user\": \"db-username\",\n  \"password\": \"db-password\",\n  \"driver\": \"com.mysql.cj.jdbc.Driver\"\n}\n</code></pre>"},{"location":"Examples/Connection%20Config/#veeva-salesforce","title":"\ud83e\uddea Veeva / Salesforce","text":"<p>Example:</p> <pre><code>{\n  \"domain\": \"your-salesforce-domain.com\",\n  \"client_id\": \"your-client-id\",\n  \"client_secret\": \"your-client-secret\"\n}\n</code></pre> <p>\ud83d\udca1 All values should be strings. Optional fields can be omitted based on your setup.</p>"},{"location":"Examples/CtlColumnMetadata/","title":"Usage for <code>CtlColumnMetadata</code>","text":""},{"location":"Examples/CtlColumnMetadata/#column-json-mapping","title":"Column Json Mapping","text":""},{"location":"Examples/CtlColumnMetadata/#description","title":"Description","text":"<p>Defines the JSON path used to extract a list of values from nested JSON response from API.</p>"},{"location":"Examples/CtlColumnMetadata/#example-json-path","title":"Example JSON Path","text":""},{"location":"Examples/CtlColumnMetadata/#sample-json","title":"Sample JSON","text":"<pre><code>{\n  \"analytics\": {\n    \"sections\": [\n      {\n        \"items\": [\n          {\n            \"dailyStats\": [\n              { \"date\": \"2024-06-01\", \"views\": 100 },\n              { \"date\": \"2024-06-02\", \"views\": 150 }\n            ]\n          },\n          {\n            \"dailyStats\": [{ \"date\": \"2024-06-03\", \"views\": 130 }]\n          }\n        ]\n      },\n      {\n        \"items\": [\n          {\n            \"dailyStats\": [{ \"date\": \"2024-06-04\", \"views\": 170 }]\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"Examples/CtlColumnMetadata/#extracted-values-using-json-path","title":"Extracted Values Using JSON Path","text":"<pre><code>[\"2024-06-01\", \"2024-06-02\", \"2024-06-03\", \"2024-06-04\"]\n</code></pre>"},{"location":"Examples/CtlColumnMetadata/#explanation-of-path","title":"Explanation of Path","text":"<ul> <li> <p>analytics.sections[*]: Loops through each section in the analytics data.</p> </li> <li> <p>items[*]: Accesses each item within a section.</p> </li> <li> <p>dailyStats[*].date: Extracts the date field from each daily stat record.</p> </li> </ul>"},{"location":"Examples/ctlApiConnectionsDtl/","title":"Usage for <code>ctlApiConnectionsDtl</code>","text":""},{"location":"Examples/ctlApiConnectionsDtl/#description","title":"Description","text":"<p>The <code>body_values</code> column should contain a list of key-value mappings where each key is a placeholder (wrapped in <code>$&lt;...&gt;$</code>) and the value is a list of items that will replace that placeholder.</p>"},{"location":"Examples/ctlApiConnectionsDtl/#format","title":"\ud83d\udccc Format","text":"<pre><code>[\n  {\n    \"$&lt;placeholder1&gt;$\": [\"value1\", \"value2\"]\n  },\n  {\n    \"$&lt;placeholder2&gt;$\": [\"valueA\", \"valueB\"]\n  }\n]\n</code></pre>"},{"location":"Examples/ctlApiConnectionsDtl/#example","title":"\ud83d\udccb Example","text":"<pre><code>[\n  {\n    \"$campaign_id$\": [\"CMP001\", \"CMP002\"]\n  },\n  {\n    \"$region$\": [\"US\", \"EU\"]\n  }\n]\n</code></pre> <p>This format is typically used for dynamically constructing API request bodies where placeholders need to be replaced with actual data during execution.</p> <p>\ud83d\udca1 Placeholders must be wrapped in <code>$...$</code>. Values must be provided as arrays, even for single replacements.</p>"},{"location":"Extractors/ApiExtractor/","title":"Documentation for <code>ApiExtractor</code>","text":""},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor","title":"<code>datacraft_framework.Extractors.ApiExtractor</code>","text":""},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIAutomation","title":"<code>APIAutomation</code>","text":"<p>A class used to automate API requests based on a configuration dictionary.</p> <p>This class supports authentication via multiple mechanisms and dynamic request body generation. It allows workflow execution over multiple API steps and handles token-based authentication.</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>class APIAutomation:\n    \"\"\"\n    A class used to automate API requests based on a configuration dictionary.\n\n    This class supports authentication via multiple mechanisms and dynamic request body generation.\n    It allows workflow execution over multiple API steps and handles token-based authentication.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initialize the API automation instance with a configuration dictionary.\n\n        Args:\n            config (dict): A dictionary containing configurations for the API.\n                Must contain required keys like 'method', 'url', etc., depending on the step type.\n        \"\"\"\n\n        self.token = None\n        self.config = config\n        self.headers = {}\n        self.params = {}\n        self.data = {}\n        self.json_body = {}\n\n    def _replace_date(self, date_match) -&gt; str:\n        \"\"\"\n        Replace date placeholders like `$current_date-7$` with actual dates.\n\n        Args:\n            date_match (str): Placeholder string like \"$current_date-7:%Y-%m$\".\n\n        Returns:\n            date: Formatted date string.\n        \"\"\"\n\n        if \":\" in date_match:\n            date_part, date_format = date_match.split(\":\")\n            date_format = date_format.replace(\"$\", \"\")\n        else:\n            date_part, date_format = date_match, \"%Y-%m-%d\"\n\n        date_generation_match = re.search(r\"-\\d+\", date_part)\n        days_to_subtract = (\n            int(date_generation_match.group()) if date_generation_match else 0\n        )\n\n        new_date = (datetime.today() + timedelta(days=days_to_subtract)).strftime(\n            date_format\n        )\n        return new_date\n\n    def date_parse_changer(self, body: dict) -&gt; dict:\n        \"\"\"\n        Replace all date placeholders in the request body with real values.\n\n        Args:\n            body (dict): The original request body possibly containing date placeholders.\n\n        Returns:\n            dict: Updated body with placeholders replaced.\n        \"\"\"\n\n        body = json.dumps(body)\n        date_matches = re.findall(r\"\\$current_date(?:-\\d+)?(?::[^$]+)?\\$\", body)\n\n        for date_match in date_matches:\n            body = body.replace(date_match, self._replace_date(date_match))\n\n        return json.loads(body)\n\n    def fetch_token(self, step: dict):\n        \"\"\"\n        Fetch an authentication token from the given endpoint using the specified auth method.\n\n        Supported auth types:\n            - oauth\n            - service_account\n            - basic_auth\n            - custom\n\n        Sets the result in `self.headers`.\n\n        Args:\n            step (dict): Step configuration with token-related details.\n\n        Raises:\n            ValueError: If the provided `auth_type` is not supported.\n        \"\"\"\n\n        auth_type = step.get(\"auth_type\")\n\n        if auth_type == \"oauth\":\n            response = niquests.request(\n                method=step.get(\"method\", \"GET\"),\n                url=step[\"token_url\"],\n                data={\n                    \"grant_type\": \"client_credentials\",\n                    \"client_id\": step[\"client_id\"],\n                    \"client_secret\": step[\"client_secret\"],\n                },\n            )\n            response.raise_for_status()\n            self.headers = {\n                \"Authorization\": f\"{step['token_type']} {response.json().get(step.get('token_path', 'access_token'))}\"\n            }\n\n        elif auth_type == \"service_account\":\n            private_key = step[\"private_key\"]\n            payload = {\n                \"iss\": step[\"issuer\"],\n                \"scope\": step[\"scope\"],\n                \"aud\": step[\"token_url\"],\n                \"exp\": datetime.now(timezone.utc) + timedelta(minutes=60),\n                \"iat\": datetime.now(timezone.utc),\n            }\n            jwt_token = jwt.encode(payload, private_key, algorithm=\"RS256\")\n            response = niquests.post(\n                step[\"token_url\"],\n                data={\n                    \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n                    \"assertion\": jwt_token,\n                },\n            )\n            response.raise_for_status()\n            self.headers = {\n                \"Authorization\": f\"Bearer {response.json().get(step.get('token_path', 'access_token'))}\"\n            }\n\n        elif auth_type == \"basic_auth\":\n            username, password = (\n                step[\"basic_auth\"][\"username\"],\n                step[\"basic_auth\"][\"password\"],\n            )\n            self.headers = {\n                \"Authorization\": \"Basic \"\n                + base64.b64encode(f\"{username}:{password}\".encode()).decode()\n            }\n\n        elif auth_type == \"custom\":\n            response = niquests.request(\n                method=step[\"method\"].upper(),\n                url=step[\"token_url\"],\n            ).json()\n            self.headers = {\n                \"Authorization\": \"Bearer \" + response.get(step[\"token_path\"])\n            }\n\n        else:\n            raise ValueError(f\"Unsupported auth_type: {auth_type}\")\n\n    def execute_request(\n        self,\n        method: str,\n        url: str,\n        headers: dict,\n        params: dict,\n        data: dict,\n        json_body: dict,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"\n        Execute an API request and return the processed response.\n\n        Args:\n            method (str): HTTP method (e.g., GET, POST).\n            url (str): Full URL for the API request.\n            headers (dict): Request headers.\n            params (dict): Query parameters.\n            data (dict): Form-encoded data.\n            json_body (dict): JSON payload.\n\n        Returns:\n            JSON response parsed into a dict or list of dicts.\n        \"\"\"\n\n        response = niquests.request(\n            method=method,\n            url=url,\n            headers=headers if headers else None,\n            params=params if params else None,\n            data=data if data else None,\n            json=json_body if json_body else None,\n            verify=False,\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def make_request(self, step) -&gt; Union[dict, List[dict]]:\n        \"\"\"\n        Process and execute a single API request step.\n\n        Handles dynamic replacement of placeholder values and multiplexed requests.\n\n        Args:\n            step (dict): API step configuration including URL, method, headers, etc.\n\n        Returns:\n            Union[dict, List[dict]]: Parsed JSON response from the API.\n        \"\"\"\n\n        url, method = step[\"url\"], step.get(\"method\", \"GET\").upper()\n        headers = {**self.headers, **step.get(\"headers\", {})}\n        params, data, json_body = map(\n            lambda k: {**getattr(self, k), **step.get(k, {})},\n            [\"params\", \"data\", \"json_body\"],\n        )\n\n        # Replace $current_date in data and json_body\n        if \"$current_date\" in str(data):\n            data = self.date_parse_changer(data)\n        if \"$current_date\" in str(json_body):\n            json_body = self.date_parse_changer(json_body)\n\n        if not step.get(\"body_values\"):\n            return self.execute_request(method, url, headers, params, data, json_body)\n\n        # Process multiple body values\n        responses = []\n        to_perform_requests = []\n\n        for body_value in step[\"body_values\"]:\n            keys = list(body_value.keys())\n            values = list(body_value.values())\n\n            # Create all combinations of the placeholder values\n            for combination in itertools.product(*values):\n\n                temp_json_body = json.dumps(\n                    json_body\n                )  # Make a copy of the original JSON string\n                temp_params = json.dumps(data)\n\n                # Replace each placeholder with the corresponding value from the combination\n                for key, val in zip(keys, combination):\n                    temp_json_body = temp_json_body.replace(key, val)\n                    temp_params = temp_params.replace(key, val)\n\n                to_perform_requests.append(\n                    json.loads(temp_json_body)\n                    if temp_json_body\n                    else json.loads(temp_params)\n                )\n\n        # Making use of niquests multiplexed feature.\n        with niquests.Session(multiplexed=True) as s:\n            for to_perform_request in to_perform_requests:\n                responses.append(\n                    s.request(\n                        method=method,\n                        url=url,\n                        headers=headers if headers else None,\n                        params=params if params else None,\n                        data=to_perform_request if data else None,\n                        json=to_perform_request if json_body else None,\n                        verify=False,\n                    )\n                )\n\n        return {\"values_based_response\": [r.json() for r in responses]}\n\n    def execute_workflow(self) -&gt; Union[dict, List[dict]]:\n        \"\"\"\n        Execute the entire configured API workflow step-by-step.\n\n        Iterates through the `config` list and executes each step in sequence.\n\n        Returns:\n            (Union[dict, List[dict]]): Result of the final API call.\n        \"\"\"\n        for step in self.config:\n            if step[\"type\"] == \"TOKEN\":\n                self.fetch_token(step)\n            else:\n                return self.make_request(step)\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIAutomation.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the API automation instance with a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing configurations for the API. Must contain required keys like 'method', 'url', etc., depending on the step type.</p> required Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def __init__(self, config):\n    \"\"\"\n    Initialize the API automation instance with a configuration dictionary.\n\n    Args:\n        config (dict): A dictionary containing configurations for the API.\n            Must contain required keys like 'method', 'url', etc., depending on the step type.\n    \"\"\"\n\n    self.token = None\n    self.config = config\n    self.headers = {}\n    self.params = {}\n    self.data = {}\n    self.json_body = {}\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIAutomation._replace_date","title":"<code>_replace_date(date_match)</code>","text":"<p>Replace date placeholders like <code>$current_date-7$</code> with actual dates.</p> <p>Parameters:</p> Name Type Description Default <code>date_match</code> <code>str</code> <p>Placeholder string like \"$current_date-7:%Y-%m$\".</p> required <p>Returns:</p> Name Type Description <code>date</code> <code>str</code> <p>Formatted date string.</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def _replace_date(self, date_match) -&gt; str:\n    \"\"\"\n    Replace date placeholders like `$current_date-7$` with actual dates.\n\n    Args:\n        date_match (str): Placeholder string like \"$current_date-7:%Y-%m$\".\n\n    Returns:\n        date: Formatted date string.\n    \"\"\"\n\n    if \":\" in date_match:\n        date_part, date_format = date_match.split(\":\")\n        date_format = date_format.replace(\"$\", \"\")\n    else:\n        date_part, date_format = date_match, \"%Y-%m-%d\"\n\n    date_generation_match = re.search(r\"-\\d+\", date_part)\n    days_to_subtract = (\n        int(date_generation_match.group()) if date_generation_match else 0\n    )\n\n    new_date = (datetime.today() + timedelta(days=days_to_subtract)).strftime(\n        date_format\n    )\n    return new_date\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIAutomation.date_parse_changer","title":"<code>date_parse_changer(body)</code>","text":"<p>Replace all date placeholders in the request body with real values.</p> <p>Parameters:</p> Name Type Description Default <code>body</code> <code>dict</code> <p>The original request body possibly containing date placeholders.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated body with placeholders replaced.</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def date_parse_changer(self, body: dict) -&gt; dict:\n    \"\"\"\n    Replace all date placeholders in the request body with real values.\n\n    Args:\n        body (dict): The original request body possibly containing date placeholders.\n\n    Returns:\n        dict: Updated body with placeholders replaced.\n    \"\"\"\n\n    body = json.dumps(body)\n    date_matches = re.findall(r\"\\$current_date(?:-\\d+)?(?::[^$]+)?\\$\", body)\n\n    for date_match in date_matches:\n        body = body.replace(date_match, self._replace_date(date_match))\n\n    return json.loads(body)\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIAutomation.fetch_token","title":"<code>fetch_token(step)</code>","text":"<p>Fetch an authentication token from the given endpoint using the specified auth method.</p> Supported auth types <ul> <li>oauth</li> <li>service_account</li> <li>basic_auth</li> <li>custom</li> </ul> <p>Sets the result in <code>self.headers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>dict</code> <p>Step configuration with token-related details.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>auth_type</code> is not supported.</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def fetch_token(self, step: dict):\n    \"\"\"\n    Fetch an authentication token from the given endpoint using the specified auth method.\n\n    Supported auth types:\n        - oauth\n        - service_account\n        - basic_auth\n        - custom\n\n    Sets the result in `self.headers`.\n\n    Args:\n        step (dict): Step configuration with token-related details.\n\n    Raises:\n        ValueError: If the provided `auth_type` is not supported.\n    \"\"\"\n\n    auth_type = step.get(\"auth_type\")\n\n    if auth_type == \"oauth\":\n        response = niquests.request(\n            method=step.get(\"method\", \"GET\"),\n            url=step[\"token_url\"],\n            data={\n                \"grant_type\": \"client_credentials\",\n                \"client_id\": step[\"client_id\"],\n                \"client_secret\": step[\"client_secret\"],\n            },\n        )\n        response.raise_for_status()\n        self.headers = {\n            \"Authorization\": f\"{step['token_type']} {response.json().get(step.get('token_path', 'access_token'))}\"\n        }\n\n    elif auth_type == \"service_account\":\n        private_key = step[\"private_key\"]\n        payload = {\n            \"iss\": step[\"issuer\"],\n            \"scope\": step[\"scope\"],\n            \"aud\": step[\"token_url\"],\n            \"exp\": datetime.now(timezone.utc) + timedelta(minutes=60),\n            \"iat\": datetime.now(timezone.utc),\n        }\n        jwt_token = jwt.encode(payload, private_key, algorithm=\"RS256\")\n        response = niquests.post(\n            step[\"token_url\"],\n            data={\n                \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n                \"assertion\": jwt_token,\n            },\n        )\n        response.raise_for_status()\n        self.headers = {\n            \"Authorization\": f\"Bearer {response.json().get(step.get('token_path', 'access_token'))}\"\n        }\n\n    elif auth_type == \"basic_auth\":\n        username, password = (\n            step[\"basic_auth\"][\"username\"],\n            step[\"basic_auth\"][\"password\"],\n        )\n        self.headers = {\n            \"Authorization\": \"Basic \"\n            + base64.b64encode(f\"{username}:{password}\".encode()).decode()\n        }\n\n    elif auth_type == \"custom\":\n        response = niquests.request(\n            method=step[\"method\"].upper(),\n            url=step[\"token_url\"],\n        ).json()\n        self.headers = {\n            \"Authorization\": \"Bearer \" + response.get(step[\"token_path\"])\n        }\n\n    else:\n        raise ValueError(f\"Unsupported auth_type: {auth_type}\")\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIAutomation.execute_request","title":"<code>execute_request(method, url, headers, params, data, json_body)</code>","text":"<p>Execute an API request and return the processed response.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>HTTP method (e.g., GET, POST).</p> required <code>url</code> <code>str</code> <p>Full URL for the API request.</p> required <code>headers</code> <code>dict</code> <p>Request headers.</p> required <code>params</code> <code>dict</code> <p>Query parameters.</p> required <code>data</code> <code>dict</code> <p>Form-encoded data.</p> required <code>json_body</code> <code>dict</code> <p>JSON payload.</p> required <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>JSON response parsed into a dict or list of dicts.</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def execute_request(\n    self,\n    method: str,\n    url: str,\n    headers: dict,\n    params: dict,\n    data: dict,\n    json_body: dict,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"\n    Execute an API request and return the processed response.\n\n    Args:\n        method (str): HTTP method (e.g., GET, POST).\n        url (str): Full URL for the API request.\n        headers (dict): Request headers.\n        params (dict): Query parameters.\n        data (dict): Form-encoded data.\n        json_body (dict): JSON payload.\n\n    Returns:\n        JSON response parsed into a dict or list of dicts.\n    \"\"\"\n\n    response = niquests.request(\n        method=method,\n        url=url,\n        headers=headers if headers else None,\n        params=params if params else None,\n        data=data if data else None,\n        json=json_body if json_body else None,\n        verify=False,\n    )\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIAutomation.make_request","title":"<code>make_request(step)</code>","text":"<p>Process and execute a single API request step.</p> <p>Handles dynamic replacement of placeholder values and multiplexed requests.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>dict</code> <p>API step configuration including URL, method, headers, etc.</p> required <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Parsed JSON response from the API.</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def make_request(self, step) -&gt; Union[dict, List[dict]]:\n    \"\"\"\n    Process and execute a single API request step.\n\n    Handles dynamic replacement of placeholder values and multiplexed requests.\n\n    Args:\n        step (dict): API step configuration including URL, method, headers, etc.\n\n    Returns:\n        Union[dict, List[dict]]: Parsed JSON response from the API.\n    \"\"\"\n\n    url, method = step[\"url\"], step.get(\"method\", \"GET\").upper()\n    headers = {**self.headers, **step.get(\"headers\", {})}\n    params, data, json_body = map(\n        lambda k: {**getattr(self, k), **step.get(k, {})},\n        [\"params\", \"data\", \"json_body\"],\n    )\n\n    # Replace $current_date in data and json_body\n    if \"$current_date\" in str(data):\n        data = self.date_parse_changer(data)\n    if \"$current_date\" in str(json_body):\n        json_body = self.date_parse_changer(json_body)\n\n    if not step.get(\"body_values\"):\n        return self.execute_request(method, url, headers, params, data, json_body)\n\n    # Process multiple body values\n    responses = []\n    to_perform_requests = []\n\n    for body_value in step[\"body_values\"]:\n        keys = list(body_value.keys())\n        values = list(body_value.values())\n\n        # Create all combinations of the placeholder values\n        for combination in itertools.product(*values):\n\n            temp_json_body = json.dumps(\n                json_body\n            )  # Make a copy of the original JSON string\n            temp_params = json.dumps(data)\n\n            # Replace each placeholder with the corresponding value from the combination\n            for key, val in zip(keys, combination):\n                temp_json_body = temp_json_body.replace(key, val)\n                temp_params = temp_params.replace(key, val)\n\n            to_perform_requests.append(\n                json.loads(temp_json_body)\n                if temp_json_body\n                else json.loads(temp_params)\n            )\n\n    # Making use of niquests multiplexed feature.\n    with niquests.Session(multiplexed=True) as s:\n        for to_perform_request in to_perform_requests:\n            responses.append(\n                s.request(\n                    method=method,\n                    url=url,\n                    headers=headers if headers else None,\n                    params=params if params else None,\n                    data=to_perform_request if data else None,\n                    json=to_perform_request if json_body else None,\n                    verify=False,\n                )\n            )\n\n    return {\"values_based_response\": [r.json() for r in responses]}\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIAutomation.execute_workflow","title":"<code>execute_workflow()</code>","text":"<p>Execute the entire configured API workflow step-by-step.</p> <p>Iterates through the <code>config</code> list and executes each step in sequence.</p> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Result of the final API call.</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def execute_workflow(self) -&gt; Union[dict, List[dict]]:\n    \"\"\"\n    Execute the entire configured API workflow step-by-step.\n\n    Iterates through the `config` list and executes each step in sequence.\n\n    Returns:\n        (Union[dict, List[dict]]): Result of the final API call.\n    \"\"\"\n    for step in self.config:\n        if step[\"type\"] == \"TOKEN\":\n            self.fetch_token(step)\n        else:\n            return self.make_request(step)\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIExtractor","title":"<code>APIExtractor</code>","text":"<p>Class responsible for extracting data from APIs and optionally writing it to storage.</p> <p>Uses metadata from control tables to authenticate, map, and structure the resulting output.</p> <p>Attributes:</p> Name Type Description <code>result</code> <p>The extracted and mapped API result.</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>class APIExtractor:\n    \"\"\"\n    Class responsible for extracting data from APIs and optionally writing it to storage.\n\n    Uses metadata from control tables to authenticate, map, and structure the resulting output.\n\n    Attributes:\n        result: The extracted and mapped API result.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_connection_dtls: list[ctlApiConnectionsDtl],\n        column_meta_data: list[CtlColumnMetadata],\n        pre_ingestion_logs: list[logDataAcquisitionDetail],\n        orch_process: OrchestrationProcess.OrchestrationProcess,\n        data_acquisition_detail: ctlDataAcquisitionDetail,\n        write_data=True,\n    ):\n        \"\"\"\n        Initialize and execute the API extraction.\n\n        Validates input configuration, builds the request config, fetches and maps the API response,\n        and optionally writes the result to S3 or logs ingestion status.\n\n        Args:\n            api_connection_dtls (List[ctlApiConnectionsDtl]): List of API connection definitions.\n            column_meta_data (List[CtlColumnMetadata]): Column schema/mapping definitions.\n            pre_ingestion_logs (List[logDataAcquisitionDetail]): Logs for duplicate prevention.\n            orch_process (OrchestrationProcess): Instance for logging process events.\n            data_acquisition_detail (ctlDataAcquisitionDetail): Ingestion configuration.\n            write_data (bool): Whether to persist output to file/store.\n\n        Raises:\n            Exception: If file has already been processed (duplicate protection).\n        \"\"\"\n\n        logger = logging.getLogger(__name__)\n\n        file_name = file_name_generator(\n            data_acquisition_detail.outbound_source_file_pattern\n        )\n        save_location_s3 = path_to_s3(\n            location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n            env=env,\n        )\n        save_location_ = save_location_s3[\"s3_location\"]\n\n        if save_location_ in [x.inbound_file_location for x in pre_ingestion_logs]:\n            logger.error(f\"The file {file_name} is already processed to Bronze Layer.\")\n\n            raise Exception(\n                f\"The file {file_name} is already processed to Bronze Layer.\"\n            )\n\n        start_time = datetime.now()\n        batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n        json_mapping = {\n            x.source_column_name: x.column_json_mapping for x in column_meta_data\n        }\n\n        config = list()\n\n        for api_connection_dtl in api_connection_dtls:\n            temp_dict = dict()\n            temp_dict[\"method\"] = api_connection_dtl.method\n            temp_dict[\"type\"] = api_connection_dtl.type\n\n            if api_connection_dtl.type == \"TOKEN\":\n                temp_dict[\"token_url\"] = api_connection_dtl.token_url\n                temp_dict[\"auth_type\"] = api_connection_dtl.auth_type\n                temp_dict[\"token_type\"] = api_connection_dtl.token_type\n                temp_dict[\"token_path\"] = api_connection_dtl.token_path\n\n                if api_connection_dtl.client_id is not None:\n                    temp_dict[\"client_id\"] = api_connection_dtl.client_id\n                if api_connection_dtl.client_secret is not None:\n                    temp_dict[\"client_secret\"] = api_connection_dtl.client_secret\n\n                if api_connection_dtl.username is not None:\n                    temp_dict[\"username\"] = api_connection_dtl.username\n                if api_connection_dtl.password is not None:\n                    temp_dict[\"password\"] = api_connection_dtl.password\n\n                if api_connection_dtl.issuer is not None:\n                    temp_dict[\"issuer\"] = api_connection_dtl.issuer\n                if api_connection_dtl.scope is not None:\n                    temp_dict[\"scope\"] = api_connection_dtl.scope\n                if api_connection_dtl.private_key is not None:\n                    temp_dict[\"private_key\"] = api_connection_dtl.private_key\n\n            else:\n                temp_dict[\"url\"] = api_connection_dtl.url\n\n                if api_connection_dtl.headers is not None:\n                    temp_dict[\"headers\"] = json_loads(api_connection_dtl.headers)\n                if api_connection_dtl.params is not None:\n                    temp_dict[\"params\"] = json_loads(api_connection_dtl.params)\n                if api_connection_dtl.data is not None:\n                    temp_dict[\"data\"] = json_loads(api_connection_dtl.data)\n                if api_connection_dtl.json_body is not None:\n                    temp_dict[\"json_body\"] = json_loads(api_connection_dtl.json_body)\n                if api_connection_dtl.body_values is not None:\n                    temp_dict[\"body_values\"] = json_loads(\n                        api_connection_dtl.body_values\n                    )\n\n            config.append(temp_dict)\n\n        try:\n            api_response = APIAutomation(config=config).execute_workflow()\n            if not api_response.get(\"values_based_response\"):\n                mapped_data = JsonDataMapper.JsonDataMapper(\n                    mapping=json_mapping, json_data=api_response\n                ).get_mapped_data()\n            else:\n                final_results = list()\n                for response in api_response.get(\"values_based_response\"):\n                    mapped_data = JsonDataMapper.JsonDataMapper(\n                        mapping=json_mapping,\n                        json_data=response,\n                    ).get_mapped_data()\n\n                    final_results.extend(mapped_data)\n                mapped_data = final_results\n\n            if write_data:\n                file_name = file_name_generator(file_name)\n\n                path_to_s3_ = path_to_s3(\n                    location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n                    env=env,\n                )[\"s3_location\"]\n                save_location = f\"{path_to_s3_}/{file_name}\"\n\n                BronzeInboundWriter(\n                    input_data=mapped_data,\n                    save_location=save_location,\n                    outbound_file_delimiter=data_acquisition_detail.outbound_file_delimiter,\n                )\n\n                orch_process.insert_log_data_acquisition_detail(\n                    log_data_acquisition=logDataAcquisitionDetail(\n                        batch_id=batch_id,\n                        run_date=start_time.date(),\n                        process_id=data_acquisition_detail.process_id,\n                        pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                        outbound_source_location=\"API\",\n                        inbound_file_location=save_location,\n                        status=\"SUCCEEDED\",\n                        start_time=start_time,\n                        end_time=datetime.now(),\n                    )\n                )\n\n            self.result = mapped_data\n        except Exception as e:\n            orch_process.insert_log_data_acquisition_detail(\n                log_data_acquisition=logDataAcquisitionDetail(\n                    batch_id=batch_id,\n                    run_date=start_time.date(),\n                    process_id=data_acquisition_detail.process_id,\n                    pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                    outbound_source_location=\"API\",\n                    inbound_file_location=None,\n                    status=\"FAILED\",\n                    exception_details=traceback.format_exc(),\n                    start_time=start_time,\n                    end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                traceback.format_exc(),\n            )\n            raise\n\n    def get_json(self) -&gt; Union[dict, List[dict]]:\n        \"\"\"\n        Get the final extracted and mapped result.\n\n        Returns:\n            Extracted structured data (typically a list of dictionaries).\n        \"\"\"\n\n        return self.result\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIExtractor.__init__","title":"<code>__init__(api_connection_dtls, column_meta_data, pre_ingestion_logs, orch_process, data_acquisition_detail, write_data=True)</code>","text":"<p>Initialize and execute the API extraction.</p> <p>Validates input configuration, builds the request config, fetches and maps the API response, and optionally writes the result to S3 or logs ingestion status.</p> <p>Parameters:</p> Name Type Description Default <code>api_connection_dtls</code> <code>List[ctlApiConnectionsDtl]</code> <p>List of API connection definitions.</p> required <code>column_meta_data</code> <code>List[CtlColumnMetadata]</code> <p>Column schema/mapping definitions.</p> required <code>pre_ingestion_logs</code> <code>List[logDataAcquisitionDetail]</code> <p>Logs for duplicate prevention.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance for logging process events.</p> required <code>data_acquisition_detail</code> <code>ctlDataAcquisitionDetail</code> <p>Ingestion configuration.</p> required <code>write_data</code> <code>bool</code> <p>Whether to persist output to file/store.</p> <code>True</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If file has already been processed (duplicate protection).</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def __init__(\n    self,\n    api_connection_dtls: list[ctlApiConnectionsDtl],\n    column_meta_data: list[CtlColumnMetadata],\n    pre_ingestion_logs: list[logDataAcquisitionDetail],\n    orch_process: OrchestrationProcess.OrchestrationProcess,\n    data_acquisition_detail: ctlDataAcquisitionDetail,\n    write_data=True,\n):\n    \"\"\"\n    Initialize and execute the API extraction.\n\n    Validates input configuration, builds the request config, fetches and maps the API response,\n    and optionally writes the result to S3 or logs ingestion status.\n\n    Args:\n        api_connection_dtls (List[ctlApiConnectionsDtl]): List of API connection definitions.\n        column_meta_data (List[CtlColumnMetadata]): Column schema/mapping definitions.\n        pre_ingestion_logs (List[logDataAcquisitionDetail]): Logs for duplicate prevention.\n        orch_process (OrchestrationProcess): Instance for logging process events.\n        data_acquisition_detail (ctlDataAcquisitionDetail): Ingestion configuration.\n        write_data (bool): Whether to persist output to file/store.\n\n    Raises:\n        Exception: If file has already been processed (duplicate protection).\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n\n    file_name = file_name_generator(\n        data_acquisition_detail.outbound_source_file_pattern\n    )\n    save_location_s3 = path_to_s3(\n        location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n        env=env,\n    )\n    save_location_ = save_location_s3[\"s3_location\"]\n\n    if save_location_ in [x.inbound_file_location for x in pre_ingestion_logs]:\n        logger.error(f\"The file {file_name} is already processed to Bronze Layer.\")\n\n        raise Exception(\n            f\"The file {file_name} is already processed to Bronze Layer.\"\n        )\n\n    start_time = datetime.now()\n    batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n    json_mapping = {\n        x.source_column_name: x.column_json_mapping for x in column_meta_data\n    }\n\n    config = list()\n\n    for api_connection_dtl in api_connection_dtls:\n        temp_dict = dict()\n        temp_dict[\"method\"] = api_connection_dtl.method\n        temp_dict[\"type\"] = api_connection_dtl.type\n\n        if api_connection_dtl.type == \"TOKEN\":\n            temp_dict[\"token_url\"] = api_connection_dtl.token_url\n            temp_dict[\"auth_type\"] = api_connection_dtl.auth_type\n            temp_dict[\"token_type\"] = api_connection_dtl.token_type\n            temp_dict[\"token_path\"] = api_connection_dtl.token_path\n\n            if api_connection_dtl.client_id is not None:\n                temp_dict[\"client_id\"] = api_connection_dtl.client_id\n            if api_connection_dtl.client_secret is not None:\n                temp_dict[\"client_secret\"] = api_connection_dtl.client_secret\n\n            if api_connection_dtl.username is not None:\n                temp_dict[\"username\"] = api_connection_dtl.username\n            if api_connection_dtl.password is not None:\n                temp_dict[\"password\"] = api_connection_dtl.password\n\n            if api_connection_dtl.issuer is not None:\n                temp_dict[\"issuer\"] = api_connection_dtl.issuer\n            if api_connection_dtl.scope is not None:\n                temp_dict[\"scope\"] = api_connection_dtl.scope\n            if api_connection_dtl.private_key is not None:\n                temp_dict[\"private_key\"] = api_connection_dtl.private_key\n\n        else:\n            temp_dict[\"url\"] = api_connection_dtl.url\n\n            if api_connection_dtl.headers is not None:\n                temp_dict[\"headers\"] = json_loads(api_connection_dtl.headers)\n            if api_connection_dtl.params is not None:\n                temp_dict[\"params\"] = json_loads(api_connection_dtl.params)\n            if api_connection_dtl.data is not None:\n                temp_dict[\"data\"] = json_loads(api_connection_dtl.data)\n            if api_connection_dtl.json_body is not None:\n                temp_dict[\"json_body\"] = json_loads(api_connection_dtl.json_body)\n            if api_connection_dtl.body_values is not None:\n                temp_dict[\"body_values\"] = json_loads(\n                    api_connection_dtl.body_values\n                )\n\n        config.append(temp_dict)\n\n    try:\n        api_response = APIAutomation(config=config).execute_workflow()\n        if not api_response.get(\"values_based_response\"):\n            mapped_data = JsonDataMapper.JsonDataMapper(\n                mapping=json_mapping, json_data=api_response\n            ).get_mapped_data()\n        else:\n            final_results = list()\n            for response in api_response.get(\"values_based_response\"):\n                mapped_data = JsonDataMapper.JsonDataMapper(\n                    mapping=json_mapping,\n                    json_data=response,\n                ).get_mapped_data()\n\n                final_results.extend(mapped_data)\n            mapped_data = final_results\n\n        if write_data:\n            file_name = file_name_generator(file_name)\n\n            path_to_s3_ = path_to_s3(\n                location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n                env=env,\n            )[\"s3_location\"]\n            save_location = f\"{path_to_s3_}/{file_name}\"\n\n            BronzeInboundWriter(\n                input_data=mapped_data,\n                save_location=save_location,\n                outbound_file_delimiter=data_acquisition_detail.outbound_file_delimiter,\n            )\n\n            orch_process.insert_log_data_acquisition_detail(\n                log_data_acquisition=logDataAcquisitionDetail(\n                    batch_id=batch_id,\n                    run_date=start_time.date(),\n                    process_id=data_acquisition_detail.process_id,\n                    pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                    outbound_source_location=\"API\",\n                    inbound_file_location=save_location,\n                    status=\"SUCCEEDED\",\n                    start_time=start_time,\n                    end_time=datetime.now(),\n                )\n            )\n\n        self.result = mapped_data\n    except Exception as e:\n        orch_process.insert_log_data_acquisition_detail(\n            log_data_acquisition=logDataAcquisitionDetail(\n                batch_id=batch_id,\n                run_date=start_time.date(),\n                process_id=data_acquisition_detail.process_id,\n                pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                outbound_source_location=\"API\",\n                inbound_file_location=None,\n                status=\"FAILED\",\n                exception_details=traceback.format_exc(),\n                start_time=start_time,\n                end_time=datetime.now(),\n            )\n        )\n        logger.error(\n            traceback.format_exc(),\n        )\n        raise\n</code></pre>"},{"location":"Extractors/ApiExtractor/#datacraft_framework.Extractors.ApiExtractor.APIExtractor.get_json","title":"<code>get_json()</code>","text":"<p>Get the final extracted and mapped result.</p> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Extracted structured data (typically a list of dictionaries).</p> Source code in <code>src/datacraft_framework/Extractors/ApiExtractor.py</code> <pre><code>def get_json(self) -&gt; Union[dict, List[dict]]:\n    \"\"\"\n    Get the final extracted and mapped result.\n\n    Returns:\n        Extracted structured data (typically a list of dictionaries).\n    \"\"\"\n\n    return self.result\n</code></pre>"},{"location":"Extractors/DatabaseExtractor/","title":"Documentation for <code>DatabaseExtractor</code>","text":""},{"location":"Extractors/DatabaseExtractor/#datacraft_framework.Extractors.DatabaseExtractor","title":"<code>datacraft_framework.Extractors.DatabaseExtractor</code>","text":""},{"location":"Extractors/DatabaseExtractor/#datacraft_framework.Extractors.DatabaseExtractor.DatabaseExtractor","title":"<code>DatabaseExtractor</code>","text":"<p>A class to extract data from a JDBC-compatible database and write it to a structured format (e.g., CSV) or cloud storage (e.g., S3).</p> <p>This class connects to databases using <code>jaydebeapi</code>, executes queries, and writes results using <code>BronzeInboundWriter</code>. It also logs ingestion details into a control table via <code>OrchestrationProcess</code>.</p> Source code in <code>src/datacraft_framework/Extractors/DatabaseExtractor.py</code> <pre><code>class DatabaseExtractor:\n    \"\"\"\n    A class to extract data from a JDBC-compatible database and write it to a structured format (e.g., CSV) or cloud storage (e.g., S3).\n\n    This class connects to databases using `jaydebeapi`, executes queries, and writes results using `BronzeInboundWriter`.\n    It also logs ingestion details into a control table via `OrchestrationProcess`.\n    \"\"\"\n\n    def connect_via_jdbc(self, config: dict) -&gt; jaydebeapi.Connection:\n        \"\"\"\n        Connect to a JDBC-compatible database dynamically based on the config dictionary.\n\n        Supports dynamic query parameters and URL construction for various databases like MySQL, PostgreSQL, Snowflake, etc.\n\n        Args:\n            config (dict): Connection configuration containing:\n                - driver (str): JDBC driver class name.\n                - url (str): Base JDBC URL (e.g., jdbc:mysql://localhost:3306).\n                - user (str): Username for authentication.\n                - password (str): Password for authentication.\n                - Optional keys: database, schema, warehouse, jar, and other query parameters.\n\n        Returns:\n            jaydebeapi.Connection: An active connection object to the database.\n\n        Raises:\n            jaydebeapi.DatabaseError: If connection fails due to invalid credentials or unreachable server.\n        \"\"\"\n        base_url = config[\"url\"]\n        driver = config[\"driver\"]\n        user = config[\"user\"]\n        password = config[\"password\"]\n\n        # Adjust URL based on database type\n        jdbc_url = base_url\n\n        # Extract database if applicable (for MySQL/Postgres-style URLs)\n        db = config.get(\"database\")\n\n        if \"mysql\" in driver or \"postgresql\" in driver or \"mariadb\" in driver:\n            # Append database to path, if not already present\n            if db and not base_url.rstrip(\"/\").endswith(f\"/{db}\"):\n                jdbc_url = base_url.rstrip(\"/\") + f\"/{db}\"\n\n        # Collect query parameters for all other keys\n        query_params = []\n        for key, value in config.items():\n            if key not in {\"url\", \"user\", \"password\", \"driver\", \"jar\", \"database\"}:\n                query_params.append(f\"{key}={value}\")\n\n        if query_params:\n            sep = \"&amp;\" if \"?\" in jdbc_url else \"?\"\n            jdbc_url += sep + \"&amp;\".join(query_params)\n\n        # Connect\n        conn = jaydebeapi.connect(driver, jdbc_url, [user, password], jars)\n        return conn\n\n    def __init__(\n        self,\n        data_acquisition_connection_master: ctlDataAcquisitionConnectionMaster,\n        pre_ingestion_logs: list[logDataAcquisitionDetail],\n        orch_process: OrchestrationProcess.OrchestrationProcess,\n        data_acquisition_detail: ctlDataAcquisitionDetail,\n    ):\n        \"\"\"\n        Initialize and execute the database extraction process.\n\n        Validates input configuration, connects to the database, executes query, writes result to S3,\n        and logs the ingestion status.\n\n        Args:\n            data_acquisition_connection_master (ctlDataAcquisitionConnectionMaster): Connection metadata object.\n            pre_ingestion_logs (list[logDataAcquisitionDetail]): List of previously processed files to avoid duplication.\n            orch_process (OrchestrationProcess): Instance used for logging process events.\n            data_acquisition_detail (ctlDataAcquisitionDetail): Configuration for this acquisition step.\n\n        Raises:\n            Exception: If the file has already been processed to prevent duplication.\n        \"\"\"\n\n        connection_config: dict = json_loads(\n            data_acquisition_connection_master.connection_config\n        )\n\n        pre_ingestion_processed_files = [\n            x.inbound_file_location for x in pre_ingestion_logs\n        ]\n\n        file_name = file_name_generator(\n            data_acquisition_detail.outbound_source_file_pattern\n        )\n        save_location_s3 = path_to_s3(\n            location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n            env=env,\n        )[\"s3_location\"]\n        save_location_ = f\"{save_location_s3}/{file_name}\"\n\n        if save_location_ in pre_ingestion_processed_files:\n            raise Exception(\n                f\"The file {file_name} is already processed to Bronze Layer.\"\n            )\n\n        start_time = datetime.now()\n        batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n        try:\n            connection = self.connect_via_jdbc(config=connection_config)\n            cursor = connection.cursor()\n            cursor.execute(data_acquisition_detail.query)\n            column_names = [desc[0] for desc in cursor.description]\n            data = cursor.fetchall()\n            cursor.close()\n            connection.close()\n\n            df = polars.DataFrame(data=data, schema=column_names)\n            BronzeInboundWriter(\n                input_data=df,\n                save_location=save_location_,\n                outbound_file_delimiter=data_acquisition_detail.outbound_file_delimiter,\n            )\n            orch_process.insert_log_data_acquisition_detail(\n                log_data_acquisition=logDataAcquisitionDetail(\n                    batch_id=batch_id,\n                    run_date=start_time.date(),\n                    process_id=data_acquisition_detail.process_id,\n                    pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                    outbound_source_location=\"DATABASE\",\n                    inbound_file_location=save_location_,\n                    status=\"SUCCEEDED\",\n                    start_time=start_time,\n                    end_time=datetime.now(),\n                )\n            )\n        except Exception as e:\n            orch_process.insert_log_data_acquisition_detail(\n                log_data_acquisition=logDataAcquisitionDetail(\n                    batch_id=batch_id,\n                    run_date=start_time.date(),\n                    process_id=data_acquisition_detail.process_id,\n                    pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                    outbound_source_location=\"DATABASE\",\n                    inbound_file_location=None,\n                    status=\"FAILED\",\n                    exception_details=traceback.format_exc(),\n                    start_time=start_time,\n                    end_time=datetime.now(),\n                )\n            )\n            logger.error(traceback.format_exc())\n            raise\n</code></pre>"},{"location":"Extractors/DatabaseExtractor/#datacraft_framework.Extractors.DatabaseExtractor.DatabaseExtractor.connect_via_jdbc","title":"<code>connect_via_jdbc(config)</code>","text":"<p>Connect to a JDBC-compatible database dynamically based on the config dictionary.</p> <p>Supports dynamic query parameters and URL construction for various databases like MySQL, PostgreSQL, Snowflake, etc.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Connection configuration containing: - driver (str): JDBC driver class name. - url (str): Base JDBC URL (e.g., jdbc:mysql://localhost:3306). - user (str): Username for authentication. - password (str): Password for authentication. - Optional keys: database, schema, warehouse, jar, and other query parameters.</p> required <p>Returns:</p> Type Description <code>Connection</code> <p>jaydebeapi.Connection: An active connection object to the database.</p> <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If connection fails due to invalid credentials or unreachable server.</p> Source code in <code>src/datacraft_framework/Extractors/DatabaseExtractor.py</code> <pre><code>def connect_via_jdbc(self, config: dict) -&gt; jaydebeapi.Connection:\n    \"\"\"\n    Connect to a JDBC-compatible database dynamically based on the config dictionary.\n\n    Supports dynamic query parameters and URL construction for various databases like MySQL, PostgreSQL, Snowflake, etc.\n\n    Args:\n        config (dict): Connection configuration containing:\n            - driver (str): JDBC driver class name.\n            - url (str): Base JDBC URL (e.g., jdbc:mysql://localhost:3306).\n            - user (str): Username for authentication.\n            - password (str): Password for authentication.\n            - Optional keys: database, schema, warehouse, jar, and other query parameters.\n\n    Returns:\n        jaydebeapi.Connection: An active connection object to the database.\n\n    Raises:\n        jaydebeapi.DatabaseError: If connection fails due to invalid credentials or unreachable server.\n    \"\"\"\n    base_url = config[\"url\"]\n    driver = config[\"driver\"]\n    user = config[\"user\"]\n    password = config[\"password\"]\n\n    # Adjust URL based on database type\n    jdbc_url = base_url\n\n    # Extract database if applicable (for MySQL/Postgres-style URLs)\n    db = config.get(\"database\")\n\n    if \"mysql\" in driver or \"postgresql\" in driver or \"mariadb\" in driver:\n        # Append database to path, if not already present\n        if db and not base_url.rstrip(\"/\").endswith(f\"/{db}\"):\n            jdbc_url = base_url.rstrip(\"/\") + f\"/{db}\"\n\n    # Collect query parameters for all other keys\n    query_params = []\n    for key, value in config.items():\n        if key not in {\"url\", \"user\", \"password\", \"driver\", \"jar\", \"database\"}:\n            query_params.append(f\"{key}={value}\")\n\n    if query_params:\n        sep = \"&amp;\" if \"?\" in jdbc_url else \"?\"\n        jdbc_url += sep + \"&amp;\".join(query_params)\n\n    # Connect\n    conn = jaydebeapi.connect(driver, jdbc_url, [user, password], jars)\n    return conn\n</code></pre>"},{"location":"Extractors/DatabaseExtractor/#datacraft_framework.Extractors.DatabaseExtractor.DatabaseExtractor.__init__","title":"<code>__init__(data_acquisition_connection_master, pre_ingestion_logs, orch_process, data_acquisition_detail)</code>","text":"<p>Initialize and execute the database extraction process.</p> <p>Validates input configuration, connects to the database, executes query, writes result to S3, and logs the ingestion status.</p> <p>Parameters:</p> Name Type Description Default <code>data_acquisition_connection_master</code> <code>ctlDataAcquisitionConnectionMaster</code> <p>Connection metadata object.</p> required <code>pre_ingestion_logs</code> <code>list[logDataAcquisitionDetail]</code> <p>List of previously processed files to avoid duplication.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging process events.</p> required <code>data_acquisition_detail</code> <code>ctlDataAcquisitionDetail</code> <p>Configuration for this acquisition step.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the file has already been processed to prevent duplication.</p> Source code in <code>src/datacraft_framework/Extractors/DatabaseExtractor.py</code> <pre><code>def __init__(\n    self,\n    data_acquisition_connection_master: ctlDataAcquisitionConnectionMaster,\n    pre_ingestion_logs: list[logDataAcquisitionDetail],\n    orch_process: OrchestrationProcess.OrchestrationProcess,\n    data_acquisition_detail: ctlDataAcquisitionDetail,\n):\n    \"\"\"\n    Initialize and execute the database extraction process.\n\n    Validates input configuration, connects to the database, executes query, writes result to S3,\n    and logs the ingestion status.\n\n    Args:\n        data_acquisition_connection_master (ctlDataAcquisitionConnectionMaster): Connection metadata object.\n        pre_ingestion_logs (list[logDataAcquisitionDetail]): List of previously processed files to avoid duplication.\n        orch_process (OrchestrationProcess): Instance used for logging process events.\n        data_acquisition_detail (ctlDataAcquisitionDetail): Configuration for this acquisition step.\n\n    Raises:\n        Exception: If the file has already been processed to prevent duplication.\n    \"\"\"\n\n    connection_config: dict = json_loads(\n        data_acquisition_connection_master.connection_config\n    )\n\n    pre_ingestion_processed_files = [\n        x.inbound_file_location for x in pre_ingestion_logs\n    ]\n\n    file_name = file_name_generator(\n        data_acquisition_detail.outbound_source_file_pattern\n    )\n    save_location_s3 = path_to_s3(\n        location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n        env=env,\n    )[\"s3_location\"]\n    save_location_ = f\"{save_location_s3}/{file_name}\"\n\n    if save_location_ in pre_ingestion_processed_files:\n        raise Exception(\n            f\"The file {file_name} is already processed to Bronze Layer.\"\n        )\n\n    start_time = datetime.now()\n    batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n    try:\n        connection = self.connect_via_jdbc(config=connection_config)\n        cursor = connection.cursor()\n        cursor.execute(data_acquisition_detail.query)\n        column_names = [desc[0] for desc in cursor.description]\n        data = cursor.fetchall()\n        cursor.close()\n        connection.close()\n\n        df = polars.DataFrame(data=data, schema=column_names)\n        BronzeInboundWriter(\n            input_data=df,\n            save_location=save_location_,\n            outbound_file_delimiter=data_acquisition_detail.outbound_file_delimiter,\n        )\n        orch_process.insert_log_data_acquisition_detail(\n            log_data_acquisition=logDataAcquisitionDetail(\n                batch_id=batch_id,\n                run_date=start_time.date(),\n                process_id=data_acquisition_detail.process_id,\n                pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                outbound_source_location=\"DATABASE\",\n                inbound_file_location=save_location_,\n                status=\"SUCCEEDED\",\n                start_time=start_time,\n                end_time=datetime.now(),\n            )\n        )\n    except Exception as e:\n        orch_process.insert_log_data_acquisition_detail(\n            log_data_acquisition=logDataAcquisitionDetail(\n                batch_id=batch_id,\n                run_date=start_time.date(),\n                process_id=data_acquisition_detail.process_id,\n                pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                outbound_source_location=\"DATABASE\",\n                inbound_file_location=None,\n                status=\"FAILED\",\n                exception_details=traceback.format_exc(),\n                start_time=start_time,\n                end_time=datetime.now(),\n            )\n        )\n        logger.error(traceback.format_exc())\n        raise\n</code></pre>"},{"location":"Extractors/S3Extractor/","title":"Documentation for <code>S3Extractor</code>","text":""},{"location":"Extractors/S3Extractor/#datacraft_framework.Extractors.S3Extractor","title":"<code>datacraft_framework.Extractors.S3Extractor</code>","text":""},{"location":"Extractors/S3Extractor/#datacraft_framework.Extractors.S3Extractor.S3Extractor","title":"<code>S3Extractor</code>","text":"<p>A class to extract data from Amazon S3 or S3-compatible storage services.</p> <p>This class handles listing objects in a given S3 bucket/prefix, downloading files, validating file names using pattern matching, and writing them to a target location. It also logs ingestion details into a control table via <code>OrchestrationProcess</code>.</p> Source code in <code>src/datacraft_framework/Extractors/S3Extractor.py</code> <pre><code>class S3Extractor:\n    \"\"\"\n    A class to extract data from Amazon S3 or S3-compatible storage services.\n\n    This class handles listing objects in a given S3 bucket/prefix, downloading files,\n    validating file names using pattern matching, and writing them to a target location.\n    It also logs ingestion details into a control table via `OrchestrationProcess`.\n\n    \"\"\"\n\n    def parse_location(self, outbound_location: str) -&gt; dict:\n        \"\"\"\n        Convert a raw S3 path into a structured dictionary containing bucket and prefix.\n\n        Args:\n            outbound_location (str): Raw S3 path like 'bucket-name/path/to/files/'.\n\n        Returns:\n            dict: Dictionary with keys:\n                - 'Bucket': Bucket name.\n                - 'Prefix': Object key prefix used for filtering.\n\n        Examples:\n            &gt;&gt;&gt; self.parse_location(\"my-bucket/data/input/\")\n            {'Bucket': 'my-bucket', 'Prefix': 'data/input/'}\n        \"\"\"\n        splited_path = outbound_location.rstrip(\"/\").split(\"/\")\n        if splited_path[0] == \"\":\n            splited_path.pop(0)\n        bucket_name = splited_path[0]\n        prefix = \"/\".join(splited_path[1:]) + \"/\"\n\n        return {\n            \"Bucket\": bucket_name,\n            \"Prefix\": prefix,\n        }\n\n    def __init__(\n        self,\n        data_acquisition_connection_master: ctlDataAcquisitionConnectionMaster,\n        pre_ingestion_logs: list[logDataAcquisitionDetail],\n        orch_process: OrchestrationProcess.OrchestrationProcess,\n        data_acquisition_detail: ctlDataAcquisitionDetail,\n    ):\n        \"\"\"\n        Initialize and execute the S3 extraction process.\n\n        Validates input configuration, connects to S3, lists and downloads matching files,\n        and logs ingestion status. Skips already processed files.\n\n        Args:\n            data_acquisition_connection_master (ctlDataAcquisitionConnectionMaster): Connection metadata object.\n            pre_ingestion_logs (list[logDataAcquisitionDetail]): List of previously processed files to avoid duplication.\n            orch_process (OrchestrationProcess): Instance for logging process events.\n            data_acquisition_detail (ctlDataAcquisitionDetail): Ingestion configuration.\n\n        Raises:\n            Exception: If no new unprocessed files are found or an error occurs during download.\n        \"\"\"\n\n        connection_config: dict = json_loads(\n            data_acquisition_connection_master.connection_config\n        )\n\n        client_id = connection_config[\"client_id\"]\n        client_secret = connection_config[\"client_secret\"]\n\n        endpoint_url = connection_config.get(\"endpoint_url\", False)\n        region = connection_config.get(\"region\", False)\n        signature_version = connection_config.get(\"signature_version\", False)\n\n        config_ = dict()\n\n        config_[\"aws_access_key_id\"] = client_id\n        config_[\"aws_secret_access_key\"] = client_secret\n\n        if endpoint_url:\n            config_[\"endpoint_url\"] = endpoint_url\n\n        if region:\n            config_[\"region\"] = region\n\n        if signature_version:\n            config_[\"config\"] = Config(signature_version=signature_version)\n\n        s3_client = boto3.client(\n            \"s3\",\n            **config_,\n        )\n\n        s3_object_config = self.parse_location(\n            outbound_location=data_acquisition_detail.outbound_source_location\n        )\n\n        files = s3_client.list_objects_v2(**s3_object_config).get(\"Contents\")\n\n        pre_ingestion_processed_files = [\n            x.inbound_file_location for x in pre_ingestion_logs\n        ]\n\n        new_files = list()\n        if files:\n            for file in files:\n                file_ = file.get(\"Key\")\n                file_name_s3 = file_.split(\"/\")[-1]\n\n                if PatternValidator.validate_pattern(\n                    file_pattern=data_acquisition_detail.outbound_source_file_pattern,\n                    file_name=file_name_s3,\n                    custom=(\n                        True\n                        if data_acquisition_detail.outbound_source_file_pattern_static\n                        == \"Y\"\n                        else False\n                    ),\n                ):\n                    path_s3 = path_to_s3(\n                        location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n                        env=env,\n                    )\n                    file_save_name = f\"{path_s3['s3_location']}/{file_name_s3}\"\n\n                    if file_save_name not in pre_ingestion_processed_files:\n                        new_files.append(file_save_name)\n\n                        start_time = datetime.now()\n                        batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n                        try:\n                            buffer = BytesIO()\n                            s3_client.download_fileobj(\n                                Bucket=s3_object_config[\"Bucket\"],\n                                Key=file.get(\"Key\"),\n                                Fileobj=buffer,\n                            )\n\n                            buffer.seek(0)\n\n                            splited_ = data_acquisition_detail.inbound_location.split(\n                                \"/\"\n                            )\n                            bucket_name = path_s3[\"bucket\"]\n                            splited_.pop(0)\n                            aws_file_key = (\n                                \"/\".join(splited_) + file_name_s3.split(\"/\")[-1]\n                            )\n                            S3Process.S3Process().s3_raw_file_write(\n                                file_object=buffer,\n                                bucket=bucket_name,\n                                file_name=aws_file_key,\n                            )\n                            orch_process.insert_log_data_acquisition_detail(\n                                log_data_acquisition=logDataAcquisitionDetail(\n                                    batch_id=batch_id,\n                                    run_date=start_time.date(),\n                                    process_id=data_acquisition_detail.process_id,\n                                    pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                                    outbound_source_location=\"S3\",\n                                    inbound_file_location=file_save_name,\n                                    status=\"SUCCEEDED\",\n                                    start_time=start_time,\n                                    end_time=datetime.now(),\n                                )\n                            )\n\n                        except Exception as e:\n                            orch_process.insert_log_data_acquisition_detail(\n                                log_data_acquisition=logDataAcquisitionDetail(\n                                    batch_id=batch_id,\n                                    run_date=start_time.date(),\n                                    process_id=data_acquisition_detail.process_id,\n                                    pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                                    outbound_source_location=\"S3\",\n                                    inbound_file_location=None,\n                                    status=\"FAILED\",\n                                    exception_details=traceback.format_exc(),\n                                    start_time=start_time,\n                                    end_time=datetime.now(),\n                                )\n                            )\n                            logger.error(traceback.format_exc())\n                            raise\n\n            if len(new_files) == 0:\n                raise Exception(f\"No unprocessed files are found.\")\n</code></pre>"},{"location":"Extractors/S3Extractor/#datacraft_framework.Extractors.S3Extractor.S3Extractor.parse_location","title":"<code>parse_location(outbound_location)</code>","text":"<p>Convert a raw S3 path into a structured dictionary containing bucket and prefix.</p> <p>Parameters:</p> Name Type Description Default <code>outbound_location</code> <code>str</code> <p>Raw S3 path like 'bucket-name/path/to/files/'.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with keys: - 'Bucket': Bucket name. - 'Prefix': Object key prefix used for filtering.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; self.parse_location(\"my-bucket/data/input/\")\n{'Bucket': 'my-bucket', 'Prefix': 'data/input/'}\n</code></pre> Source code in <code>src/datacraft_framework/Extractors/S3Extractor.py</code> <pre><code>def parse_location(self, outbound_location: str) -&gt; dict:\n    \"\"\"\n    Convert a raw S3 path into a structured dictionary containing bucket and prefix.\n\n    Args:\n        outbound_location (str): Raw S3 path like 'bucket-name/path/to/files/'.\n\n    Returns:\n        dict: Dictionary with keys:\n            - 'Bucket': Bucket name.\n            - 'Prefix': Object key prefix used for filtering.\n\n    Examples:\n        &gt;&gt;&gt; self.parse_location(\"my-bucket/data/input/\")\n        {'Bucket': 'my-bucket', 'Prefix': 'data/input/'}\n    \"\"\"\n    splited_path = outbound_location.rstrip(\"/\").split(\"/\")\n    if splited_path[0] == \"\":\n        splited_path.pop(0)\n    bucket_name = splited_path[0]\n    prefix = \"/\".join(splited_path[1:]) + \"/\"\n\n    return {\n        \"Bucket\": bucket_name,\n        \"Prefix\": prefix,\n    }\n</code></pre>"},{"location":"Extractors/S3Extractor/#datacraft_framework.Extractors.S3Extractor.S3Extractor.__init__","title":"<code>__init__(data_acquisition_connection_master, pre_ingestion_logs, orch_process, data_acquisition_detail)</code>","text":"<p>Initialize and execute the S3 extraction process.</p> <p>Validates input configuration, connects to S3, lists and downloads matching files, and logs ingestion status. Skips already processed files.</p> <p>Parameters:</p> Name Type Description Default <code>data_acquisition_connection_master</code> <code>ctlDataAcquisitionConnectionMaster</code> <p>Connection metadata object.</p> required <code>pre_ingestion_logs</code> <code>list[logDataAcquisitionDetail]</code> <p>List of previously processed files to avoid duplication.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance for logging process events.</p> required <code>data_acquisition_detail</code> <code>ctlDataAcquisitionDetail</code> <p>Ingestion configuration.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no new unprocessed files are found or an error occurs during download.</p> Source code in <code>src/datacraft_framework/Extractors/S3Extractor.py</code> <pre><code>def __init__(\n    self,\n    data_acquisition_connection_master: ctlDataAcquisitionConnectionMaster,\n    pre_ingestion_logs: list[logDataAcquisitionDetail],\n    orch_process: OrchestrationProcess.OrchestrationProcess,\n    data_acquisition_detail: ctlDataAcquisitionDetail,\n):\n    \"\"\"\n    Initialize and execute the S3 extraction process.\n\n    Validates input configuration, connects to S3, lists and downloads matching files,\n    and logs ingestion status. Skips already processed files.\n\n    Args:\n        data_acquisition_connection_master (ctlDataAcquisitionConnectionMaster): Connection metadata object.\n        pre_ingestion_logs (list[logDataAcquisitionDetail]): List of previously processed files to avoid duplication.\n        orch_process (OrchestrationProcess): Instance for logging process events.\n        data_acquisition_detail (ctlDataAcquisitionDetail): Ingestion configuration.\n\n    Raises:\n        Exception: If no new unprocessed files are found or an error occurs during download.\n    \"\"\"\n\n    connection_config: dict = json_loads(\n        data_acquisition_connection_master.connection_config\n    )\n\n    client_id = connection_config[\"client_id\"]\n    client_secret = connection_config[\"client_secret\"]\n\n    endpoint_url = connection_config.get(\"endpoint_url\", False)\n    region = connection_config.get(\"region\", False)\n    signature_version = connection_config.get(\"signature_version\", False)\n\n    config_ = dict()\n\n    config_[\"aws_access_key_id\"] = client_id\n    config_[\"aws_secret_access_key\"] = client_secret\n\n    if endpoint_url:\n        config_[\"endpoint_url\"] = endpoint_url\n\n    if region:\n        config_[\"region\"] = region\n\n    if signature_version:\n        config_[\"config\"] = Config(signature_version=signature_version)\n\n    s3_client = boto3.client(\n        \"s3\",\n        **config_,\n    )\n\n    s3_object_config = self.parse_location(\n        outbound_location=data_acquisition_detail.outbound_source_location\n    )\n\n    files = s3_client.list_objects_v2(**s3_object_config).get(\"Contents\")\n\n    pre_ingestion_processed_files = [\n        x.inbound_file_location for x in pre_ingestion_logs\n    ]\n\n    new_files = list()\n    if files:\n        for file in files:\n            file_ = file.get(\"Key\")\n            file_name_s3 = file_.split(\"/\")[-1]\n\n            if PatternValidator.validate_pattern(\n                file_pattern=data_acquisition_detail.outbound_source_file_pattern,\n                file_name=file_name_s3,\n                custom=(\n                    True\n                    if data_acquisition_detail.outbound_source_file_pattern_static\n                    == \"Y\"\n                    else False\n                ),\n            ):\n                path_s3 = path_to_s3(\n                    location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n                    env=env,\n                )\n                file_save_name = f\"{path_s3['s3_location']}/{file_name_s3}\"\n\n                if file_save_name not in pre_ingestion_processed_files:\n                    new_files.append(file_save_name)\n\n                    start_time = datetime.now()\n                    batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n                    try:\n                        buffer = BytesIO()\n                        s3_client.download_fileobj(\n                            Bucket=s3_object_config[\"Bucket\"],\n                            Key=file.get(\"Key\"),\n                            Fileobj=buffer,\n                        )\n\n                        buffer.seek(0)\n\n                        splited_ = data_acquisition_detail.inbound_location.split(\n                            \"/\"\n                        )\n                        bucket_name = path_s3[\"bucket\"]\n                        splited_.pop(0)\n                        aws_file_key = (\n                            \"/\".join(splited_) + file_name_s3.split(\"/\")[-1]\n                        )\n                        S3Process.S3Process().s3_raw_file_write(\n                            file_object=buffer,\n                            bucket=bucket_name,\n                            file_name=aws_file_key,\n                        )\n                        orch_process.insert_log_data_acquisition_detail(\n                            log_data_acquisition=logDataAcquisitionDetail(\n                                batch_id=batch_id,\n                                run_date=start_time.date(),\n                                process_id=data_acquisition_detail.process_id,\n                                pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                                outbound_source_location=\"S3\",\n                                inbound_file_location=file_save_name,\n                                status=\"SUCCEEDED\",\n                                start_time=start_time,\n                                end_time=datetime.now(),\n                            )\n                        )\n\n                    except Exception as e:\n                        orch_process.insert_log_data_acquisition_detail(\n                            log_data_acquisition=logDataAcquisitionDetail(\n                                batch_id=batch_id,\n                                run_date=start_time.date(),\n                                process_id=data_acquisition_detail.process_id,\n                                pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                                outbound_source_location=\"S3\",\n                                inbound_file_location=None,\n                                status=\"FAILED\",\n                                exception_details=traceback.format_exc(),\n                                start_time=start_time,\n                                end_time=datetime.now(),\n                            )\n                        )\n                        logger.error(traceback.format_exc())\n                        raise\n\n        if len(new_files) == 0:\n            raise Exception(f\"No unprocessed files are found.\")\n</code></pre>"},{"location":"Extractors/SalesforceExtractor/","title":"Documentation for <code>SalesforceExtractor</code>","text":""},{"location":"Extractors/SalesforceExtractor/#datacraft_framework.Extractors.SalesforceExtractor","title":"<code>datacraft_framework.Extractors.SalesforceExtractor</code>","text":""},{"location":"Extractors/SalesforceExtractor/#datacraft_framework.Extractors.SalesforceExtractor.SalesForce","title":"<code>SalesForce</code>","text":"<p>A class for connecting to Salesforce using OAuth2 and querying data from datasets.</p> <p>This class handles authentication via OAuth2 client credentials flow and provides a method to query Salesforce objects with pagination support.</p> <p>Attributes:</p> Name Type Description <code>domain</code> <code>str</code> <p>Base URL of the Salesforce instance.</p> <code>headers</code> <code>dict</code> <p>HTTP headers including access token after successful authentication.</p> Source code in <code>src/datacraft_framework/Extractors/SalesforceExtractor.py</code> <pre><code>class SalesForce:\n    \"\"\"\n    A class for connecting to Salesforce using OAuth2 and querying data from datasets.\n\n    This class handles authentication via OAuth2 client credentials flow and provides\n    a method to query Salesforce objects with pagination support.\n\n    Attributes:\n        domain (str): Base URL of the Salesforce instance.\n        headers (dict): HTTP headers including access token after successful authentication.\n    \"\"\"\n\n    def __init__(\n        self,\n        connection_config: dict,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Salesforce connection using provided configuration.\n\n        Args:\n            connection_config (dict): Dictionary containing:\n                - 'domain': Salesforce instance base URL.\n                - 'client_id': OAuth2 client ID.\n                - 'client_secret': OAuth2 client secret.\n\n        Raises:\n            Exception: If authentication fails or no access token is received.\n        \"\"\"\n\n        self.domain = connection_config[\"domain\"]\n        client_id = connection_config[\"client_id\"]\n        client_secret = connection_config[\"client_secret\"]\n        oauth_endpoint = \"/services/oauth2/token\"\n\n        payload = {\n            \"grant_type\": \"client_credentials\",\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n        }\n\n        response = niquests.post(url=f\"{self.domain}{oauth_endpoint}\", data=payload)\n\n        if response.status_code == 200:\n            access_token = response.json()[\"access_token\"]\n            self.headers = {\"Authorization\": \"Bearer \" + access_token}\n        else:\n            raise Exception(f\"Failed In Getting Access Token:\\n {response.text}\")\n\n    def query(self, columns: list[str], dataset_name: str) -&gt; list[dict]:\n        \"\"\"\n        Query records from a specified Salesforce dataset (object).\n\n        This method supports paginated responses via `nextRecordsUrl`.\n\n        Args:\n            columns (list[str]): List of column names to retrieve.\n            dataset_name (str): The name of the Salesforce object (e.g., `Account`, `Contact`).\n\n        Returns:\n            list[dict]: A list of dictionaries representing the queried records.\n\n        Examples:\n            &gt;&gt;&gt; sf = SalesForce(config)\n            &gt;&gt;&gt; records = sf.query(columns=[\"Id\", \"Name\"], dataset_name=\"Account\")\n        \"\"\"\n\n        query_ = f\"select {','.join(columns)} FROM {dataset_name}\"\n\n        endpoint = \"/services/data/v62.0/queryAll\"\n        response = niquests.get(\n            f\"{self.domain}{endpoint}\",\n            headers=self.headers,\n            params={\"q\": query_},\n        ).json()\n\n        records = response[\"records\"]\n        more_results = list()\n\n        results = list()\n        for record in records:\n            results.append({column: record[column] for column in columns})\n\n        while not response[\"done\"]:\n            response = niquests.get(\n                f\"{self.domain}{response['nextRecordsUrl']}\",\n                headers=self.headers,\n            ).json()\n            records_ = response[\"records\"]\n\n            for record in records_:\n                more_results.append({column: record[column] for column in columns})\n\n        if len(more_results) != 0:\n            results = results + more_results\n\n        return results\n</code></pre>"},{"location":"Extractors/SalesforceExtractor/#datacraft_framework.Extractors.SalesforceExtractor.SalesForce.__init__","title":"<code>__init__(connection_config)</code>","text":"<p>Initialize the Salesforce connection using provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>connection_config</code> <code>dict</code> <p>Dictionary containing: - 'domain': Salesforce instance base URL. - 'client_id': OAuth2 client ID. - 'client_secret': OAuth2 client secret.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If authentication fails or no access token is received.</p> Source code in <code>src/datacraft_framework/Extractors/SalesforceExtractor.py</code> <pre><code>def __init__(\n    self,\n    connection_config: dict,\n) -&gt; None:\n    \"\"\"\n    Initialize the Salesforce connection using provided configuration.\n\n    Args:\n        connection_config (dict): Dictionary containing:\n            - 'domain': Salesforce instance base URL.\n            - 'client_id': OAuth2 client ID.\n            - 'client_secret': OAuth2 client secret.\n\n    Raises:\n        Exception: If authentication fails or no access token is received.\n    \"\"\"\n\n    self.domain = connection_config[\"domain\"]\n    client_id = connection_config[\"client_id\"]\n    client_secret = connection_config[\"client_secret\"]\n    oauth_endpoint = \"/services/oauth2/token\"\n\n    payload = {\n        \"grant_type\": \"client_credentials\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n    }\n\n    response = niquests.post(url=f\"{self.domain}{oauth_endpoint}\", data=payload)\n\n    if response.status_code == 200:\n        access_token = response.json()[\"access_token\"]\n        self.headers = {\"Authorization\": \"Bearer \" + access_token}\n    else:\n        raise Exception(f\"Failed In Getting Access Token:\\n {response.text}\")\n</code></pre>"},{"location":"Extractors/SalesforceExtractor/#datacraft_framework.Extractors.SalesforceExtractor.SalesForce.query","title":"<code>query(columns, dataset_name)</code>","text":"<p>Query records from a specified Salesforce dataset (object).</p> <p>This method supports paginated responses via <code>nextRecordsUrl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str]</code> <p>List of column names to retrieve.</p> required <code>dataset_name</code> <code>str</code> <p>The name of the Salesforce object (e.g., <code>Account</code>, <code>Contact</code>).</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of dictionaries representing the queried records.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sf = SalesForce(config)\n&gt;&gt;&gt; records = sf.query(columns=[\"Id\", \"Name\"], dataset_name=\"Account\")\n</code></pre> Source code in <code>src/datacraft_framework/Extractors/SalesforceExtractor.py</code> <pre><code>def query(self, columns: list[str], dataset_name: str) -&gt; list[dict]:\n    \"\"\"\n    Query records from a specified Salesforce dataset (object).\n\n    This method supports paginated responses via `nextRecordsUrl`.\n\n    Args:\n        columns (list[str]): List of column names to retrieve.\n        dataset_name (str): The name of the Salesforce object (e.g., `Account`, `Contact`).\n\n    Returns:\n        list[dict]: A list of dictionaries representing the queried records.\n\n    Examples:\n        &gt;&gt;&gt; sf = SalesForce(config)\n        &gt;&gt;&gt; records = sf.query(columns=[\"Id\", \"Name\"], dataset_name=\"Account\")\n    \"\"\"\n\n    query_ = f\"select {','.join(columns)} FROM {dataset_name}\"\n\n    endpoint = \"/services/data/v62.0/queryAll\"\n    response = niquests.get(\n        f\"{self.domain}{endpoint}\",\n        headers=self.headers,\n        params={\"q\": query_},\n    ).json()\n\n    records = response[\"records\"]\n    more_results = list()\n\n    results = list()\n    for record in records:\n        results.append({column: record[column] for column in columns})\n\n    while not response[\"done\"]:\n        response = niquests.get(\n            f\"{self.domain}{response['nextRecordsUrl']}\",\n            headers=self.headers,\n        ).json()\n        records_ = response[\"records\"]\n\n        for record in records_:\n            more_results.append({column: record[column] for column in columns})\n\n    if len(more_results) != 0:\n        results = results + more_results\n\n    return results\n</code></pre>"},{"location":"Extractors/SalesforceExtractor/#datacraft_framework.Extractors.SalesforceExtractor.SalesforceExtractor","title":"<code>SalesforceExtractor</code>","text":"<p>A class that extracts data from Salesforce and writes it to an S3-compatible storage.</p> <p>This class uses <code>SalesForce</code> to fetch data, maps the result to a Polars DataFrame, and writes it to cloud storage using <code>BronzeInboundWriter</code>. It also logs ingestion status via <code>OrchestrationProcess</code>.</p> Source code in <code>src/datacraft_framework/Extractors/SalesforceExtractor.py</code> <pre><code>class SalesforceExtractor:\n    \"\"\"\n    A class that extracts data from Salesforce and writes it to an S3-compatible storage.\n\n    This class uses `SalesForce` to fetch data, maps the result to a Polars DataFrame,\n    and writes it to cloud storage using `BronzeInboundWriter`.\n    It also logs ingestion status via `OrchestrationProcess`.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_acquisition_connection_master: ctlDataAcquisitionConnectionMaster,\n        pre_ingestion_logs: list[logDataAcquisitionDetail],\n        orch_process: OrchestrationProcess.OrchestrationProcess,\n        data_acquisition_detail: ctlDataAcquisitionDetail,\n    ):\n        \"\"\"\n        Initialize and execute the Salesforce extraction process.\n\n        Validates input configuration, connects to Salesforce, executes query, writes result to S3,\n        and logs the ingestion status.\n\n        Args:\n            data_acquisition_connection_master (ctlDataAcquisitionConnectionMaster): Connection metadata object.\n            pre_ingestion_logs (list[logDataAcquisitionDetail]): List of previously processed files to avoid duplication.\n            orch_process (OrchestrationProcess): Instance used for logging process events.\n            data_acquisition_detail (ctlDataAcquisitionDetail): Configuration for this acquisition step.\n\n        Raises:\n            Exception: If the file has already been processed or an error occurs during execution.\n        \"\"\"\n        connection_config: dict = json_loads(\n            data_acquisition_connection_master.connection_config\n        )\n\n        pre_ingestion_processed_files = [\n            x.inbound_file_location for x in pre_ingestion_logs\n        ]\n\n        file_name = file_name_generator(\n            data_acquisition_detail.outbound_source_file_pattern\n        )\n\n        path_s3 = path_to_s3(\n            location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n            env=env,\n        )\n        save_location_ = f\"{path_s3['s3_location']}/{file_name}\"\n\n        if save_location_ in pre_ingestion_processed_files:\n            raise Exception(\n                f\"The file {file_name} is already processed to Bronze Layer.\"\n            )\n\n        start_time = datetime.now()\n        batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n        try:\n            salesforce_extractor = SalesForce(\n                connection_config=connection_config,\n            )\n            columns = data_acquisition_detail.columns.split(\",\")\n            records = salesforce_extractor.query(\n                columns=columns,\n                dataset_name=data_acquisition_detail.pre_ingestion_dataset_name,\n            )\n\n            df = polars.DataFrame(records)\n            BronzeInboundWriter(\n                input_data=df,\n                save_location=save_location_,\n                outbound_file_delimiter=data_acquisition_detail.outbound_file_delimiter,\n            )\n            orch_process.insert_log_data_acquisition_detail(\n                log_data_acquisition=logDataAcquisitionDetail(\n                    batch_id=batch_id,\n                    run_date=start_time.date(),\n                    process_id=data_acquisition_detail.process_id,\n                    pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                    outbound_source_location=\"SALESFORCE/VEEVA\",\n                    inbound_file_location=save_location_,\n                    status=\"SUCCEEDED\",\n                    start_time=start_time,\n                    end_time=datetime.now(),\n                )\n            )\n        except Exception as e:\n            orch_process.insert_log_data_acquisition_detail(\n                log_data_acquisition=logDataAcquisitionDetail(\n                    batch_id=batch_id,\n                    run_date=start_time.date(),\n                    process_id=data_acquisition_detail.process_id,\n                    pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                    outbound_source_location=\"SALESFORCE/VEEVA\",\n                    inbound_file_location=None,\n                    status=\"FAILED\",\n                    exception_details=traceback.format_exc(),\n                    start_time=start_time,\n                    end_time=datetime.now(),\n                )\n            )\n            logger.error(traceback.format_exc())\n            raise\n</code></pre>"},{"location":"Extractors/SalesforceExtractor/#datacraft_framework.Extractors.SalesforceExtractor.SalesforceExtractor.__init__","title":"<code>__init__(data_acquisition_connection_master, pre_ingestion_logs, orch_process, data_acquisition_detail)</code>","text":"<p>Initialize and execute the Salesforce extraction process.</p> <p>Validates input configuration, connects to Salesforce, executes query, writes result to S3, and logs the ingestion status.</p> <p>Parameters:</p> Name Type Description Default <code>data_acquisition_connection_master</code> <code>ctlDataAcquisitionConnectionMaster</code> <p>Connection metadata object.</p> required <code>pre_ingestion_logs</code> <code>list[logDataAcquisitionDetail]</code> <p>List of previously processed files to avoid duplication.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging process events.</p> required <code>data_acquisition_detail</code> <code>ctlDataAcquisitionDetail</code> <p>Configuration for this acquisition step.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the file has already been processed or an error occurs during execution.</p> Source code in <code>src/datacraft_framework/Extractors/SalesforceExtractor.py</code> <pre><code>def __init__(\n    self,\n    data_acquisition_connection_master: ctlDataAcquisitionConnectionMaster,\n    pre_ingestion_logs: list[logDataAcquisitionDetail],\n    orch_process: OrchestrationProcess.OrchestrationProcess,\n    data_acquisition_detail: ctlDataAcquisitionDetail,\n):\n    \"\"\"\n    Initialize and execute the Salesforce extraction process.\n\n    Validates input configuration, connects to Salesforce, executes query, writes result to S3,\n    and logs the ingestion status.\n\n    Args:\n        data_acquisition_connection_master (ctlDataAcquisitionConnectionMaster): Connection metadata object.\n        pre_ingestion_logs (list[logDataAcquisitionDetail]): List of previously processed files to avoid duplication.\n        orch_process (OrchestrationProcess): Instance used for logging process events.\n        data_acquisition_detail (ctlDataAcquisitionDetail): Configuration for this acquisition step.\n\n    Raises:\n        Exception: If the file has already been processed or an error occurs during execution.\n    \"\"\"\n    connection_config: dict = json_loads(\n        data_acquisition_connection_master.connection_config\n    )\n\n    pre_ingestion_processed_files = [\n        x.inbound_file_location for x in pre_ingestion_logs\n    ]\n\n    file_name = file_name_generator(\n        data_acquisition_detail.outbound_source_file_pattern\n    )\n\n    path_s3 = path_to_s3(\n        location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n        env=env,\n    )\n    save_location_ = f\"{path_s3['s3_location']}/{file_name}\"\n\n    if save_location_ in pre_ingestion_processed_files:\n        raise Exception(\n            f\"The file {file_name} is already processed to Bronze Layer.\"\n        )\n\n    start_time = datetime.now()\n    batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n    try:\n        salesforce_extractor = SalesForce(\n            connection_config=connection_config,\n        )\n        columns = data_acquisition_detail.columns.split(\",\")\n        records = salesforce_extractor.query(\n            columns=columns,\n            dataset_name=data_acquisition_detail.pre_ingestion_dataset_name,\n        )\n\n        df = polars.DataFrame(records)\n        BronzeInboundWriter(\n            input_data=df,\n            save_location=save_location_,\n            outbound_file_delimiter=data_acquisition_detail.outbound_file_delimiter,\n        )\n        orch_process.insert_log_data_acquisition_detail(\n            log_data_acquisition=logDataAcquisitionDetail(\n                batch_id=batch_id,\n                run_date=start_time.date(),\n                process_id=data_acquisition_detail.process_id,\n                pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                outbound_source_location=\"SALESFORCE/VEEVA\",\n                inbound_file_location=save_location_,\n                status=\"SUCCEEDED\",\n                start_time=start_time,\n                end_time=datetime.now(),\n            )\n        )\n    except Exception as e:\n        orch_process.insert_log_data_acquisition_detail(\n            log_data_acquisition=logDataAcquisitionDetail(\n                batch_id=batch_id,\n                run_date=start_time.date(),\n                process_id=data_acquisition_detail.process_id,\n                pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                outbound_source_location=\"SALESFORCE/VEEVA\",\n                inbound_file_location=None,\n                status=\"FAILED\",\n                exception_details=traceback.format_exc(),\n                start_time=start_time,\n                end_time=datetime.now(),\n            )\n        )\n        logger.error(traceback.format_exc())\n        raise\n</code></pre>"},{"location":"Extractors/SftpExtractor/","title":"Documentation for <code>SftpExtractor</code>","text":""},{"location":"Extractors/SftpExtractor/#datacraft_framework.Extractors.SftpExtractor","title":"<code>datacraft_framework.Extractors.SftpExtractor</code>","text":""},{"location":"Extractors/SftpExtractor/#datacraft_framework.Extractors.SftpExtractor.SftpExtractor","title":"<code>SftpExtractor</code>","text":"<p>A class to extract data from an SFTP server and store it in cloud storage (e.g., S3).</p> <p>This class connects to a remote SFTP server, lists files in a specified directory, downloads matching files (based on file pattern), and uploads them to a target location (e.g., S3). It also logs each file transfer in a control table via <code>OrchestrationProcess</code>.</p> <p>Attributes:</p> Name Type Description <code>`None`</code> <p>All operations are stateless and executed during initialization.</p> Source code in <code>src/datacraft_framework/Extractors/SftpExtractor.py</code> <pre><code>class SftpExtractor:\n    \"\"\"\n    A class to extract data from an SFTP server and store it in cloud storage (e.g., S3).\n\n    This class connects to a remote SFTP server, lists files in a specified directory,\n    downloads matching files (based on file pattern), and uploads them to a target location (e.g., S3).\n    It also logs each file transfer in a control table via `OrchestrationProcess`.\n\n    Attributes:\n        `None`: All operations are stateless and executed during initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_acquisition_connection_master: ctlDataAcquisitionConnectionMaster,\n        pre_ingestion_logs: list[logDataAcquisitionDetail],\n        orch_process: OrchestrationProcess.OrchestrationProcess,\n        data_acquisition_detail: ctlDataAcquisitionDetail,\n    ):\n        \"\"\"\n        Initialize and execute the SFTP extraction process.\n\n        Validates input configuration, connects to the SFTP server, downloads matching files,\n        uploads them to cloud storage, and logs ingestion status. Skips already processed files.\n\n        Args:\n            data_acquisition_connection_master (ctlDataAcquisitionConnectionMaster): Connection metadata object.\n            pre_ingestion_logs (list[logDataAcquisitionDetail]): List of previously processed files to avoid duplication.\n            orch_process (OrchestrationProcess): Instance used for logging process events.\n            data_acquisition_detail (ctlDataAcquisitionDetail): Configuration for this acquisition step.\n\n        Raises:\n            Exception: If no unprocessed files are found or an error occurs during transfer.\n        \"\"\"\n        connection_config: dict = json_loads(\n            data_acquisition_connection_master.connection_config\n        )\n\n        ssh = paramiko.SSHClient()\n        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        if data_acquisition_connection_master.ssh_private_key:\n            privatekeyfile = StringIO(\n                data_acquisition_connection_master.ssh_private_key\n            )\n            mykey = paramiko.Ed25519Key.from_private_key(privatekeyfile)\n        else:\n            mykey = None\n\n        ssh.connect(\n            hostname=connection_config.get(\"host\"),\n            password=connection_config.get(\"password\"),\n            username=connection_config.get(\"user\"),\n            allow_agent=True,\n            pkey=mykey,\n        )\n\n        sftp = ssh.open_sftp()\n        files = sftp.listdir(data_acquisition_detail.outbound_source_location)\n\n        pre_ingestion_processed_files = [\n            x.inbound_file_location for x in pre_ingestion_logs\n        ]\n\n        new_files = list()\n\n        for file_ in files:\n            path_s3 = path_to_s3(\n                location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n                env=env,\n            )\n\n            save_location_ = f\"{path_s3['s3_location']}/{file_}\"\n\n            if save_location_ not in pre_ingestion_processed_files:\n\n                try:\n                    if PatternValidator.validate_pattern(\n                        file_pattern=data_acquisition_detail.outbound_source_file_pattern,\n                        file_name=file_,\n                        custom=(\n                            True\n                            if data_acquisition_detail.outbound_source_file_pattern_static\n                            == \"Y\"\n                            else False\n                        ),\n                    ):\n                        new_files.append(save_location_)\n\n                        start_time = datetime.now()\n                        batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n                        remote_file = sftp.file(\n                            data_acquisition_detail.outbound_source_location + file_,\n                            mode=\"r\",\n                        )\n\n                        buffer = BytesIO()\n\n                        while True:\n                            data = remote_file.read(524288000)\n                            if not data:\n                                break\n                            buffer.write(data)\n                        buffer.seek(0)\n\n                        splited_ = data_acquisition_detail.inbound_location.split(\"/\")\n                        bucket_name = path_s3[\"bucket\"]\n                        splited_.pop(0)\n                        aws_file_key = \"/\".join(splited_) + file_.split(\"/\")[-1]\n\n                        S3Process.S3Process().s3_raw_file_write(\n                            file_object=buffer,\n                            bucket=bucket_name,\n                            file_name=aws_file_key,\n                        )\n                        orch_process.insert_log_data_acquisition_detail(\n                            log_data_acquisition=logDataAcquisitionDetail(\n                                batch_id=batch_id,\n                                run_date=start_time.date(),\n                                process_id=data_acquisition_detail.process_id,\n                                pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                                outbound_source_location=data_acquisition_detail.outbound_source_location,\n                                inbound_file_location=save_location_,\n                                status=\"SUCCEEDED\",\n                                start_time=start_time,\n                                end_time=datetime.now(),\n                            )\n                        )\n                except Exception as e:\n                    orch_process.insert_log_data_acquisition_detail(\n                        log_data_acquisition=logDataAcquisitionDetail(\n                            batch_id=batch_id,\n                            run_date=start_time.date(),\n                            process_id=data_acquisition_detail.process_id,\n                            pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                            outbound_source_location=data_acquisition_detail.outbound_source_location,\n                            inbound_file_location=None,\n                            status=\"FAILED\",\n                            exception_details=traceback.format_exc(),\n                            start_time=start_time,\n                            end_time=datetime.now(),\n                        )\n                    )\n                    logger.error(traceback.format_exc())\n                    raise\n        if len(new_files) == 0:\n            logger.error(f\"No unprocessed files are found.\")\n            raise Exception(f\"No unprocessed files are found.\")\n</code></pre>"},{"location":"Extractors/SftpExtractor/#datacraft_framework.Extractors.SftpExtractor.SftpExtractor.__init__","title":"<code>__init__(data_acquisition_connection_master, pre_ingestion_logs, orch_process, data_acquisition_detail)</code>","text":"<p>Initialize and execute the SFTP extraction process.</p> <p>Validates input configuration, connects to the SFTP server, downloads matching files, uploads them to cloud storage, and logs ingestion status. Skips already processed files.</p> <p>Parameters:</p> Name Type Description Default <code>data_acquisition_connection_master</code> <code>ctlDataAcquisitionConnectionMaster</code> <p>Connection metadata object.</p> required <code>pre_ingestion_logs</code> <code>list[logDataAcquisitionDetail]</code> <p>List of previously processed files to avoid duplication.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging process events.</p> required <code>data_acquisition_detail</code> <code>ctlDataAcquisitionDetail</code> <p>Configuration for this acquisition step.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no unprocessed files are found or an error occurs during transfer.</p> Source code in <code>src/datacraft_framework/Extractors/SftpExtractor.py</code> <pre><code>def __init__(\n    self,\n    data_acquisition_connection_master: ctlDataAcquisitionConnectionMaster,\n    pre_ingestion_logs: list[logDataAcquisitionDetail],\n    orch_process: OrchestrationProcess.OrchestrationProcess,\n    data_acquisition_detail: ctlDataAcquisitionDetail,\n):\n    \"\"\"\n    Initialize and execute the SFTP extraction process.\n\n    Validates input configuration, connects to the SFTP server, downloads matching files,\n    uploads them to cloud storage, and logs ingestion status. Skips already processed files.\n\n    Args:\n        data_acquisition_connection_master (ctlDataAcquisitionConnectionMaster): Connection metadata object.\n        pre_ingestion_logs (list[logDataAcquisitionDetail]): List of previously processed files to avoid duplication.\n        orch_process (OrchestrationProcess): Instance used for logging process events.\n        data_acquisition_detail (ctlDataAcquisitionDetail): Configuration for this acquisition step.\n\n    Raises:\n        Exception: If no unprocessed files are found or an error occurs during transfer.\n    \"\"\"\n    connection_config: dict = json_loads(\n        data_acquisition_connection_master.connection_config\n    )\n\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n    if data_acquisition_connection_master.ssh_private_key:\n        privatekeyfile = StringIO(\n            data_acquisition_connection_master.ssh_private_key\n        )\n        mykey = paramiko.Ed25519Key.from_private_key(privatekeyfile)\n    else:\n        mykey = None\n\n    ssh.connect(\n        hostname=connection_config.get(\"host\"),\n        password=connection_config.get(\"password\"),\n        username=connection_config.get(\"user\"),\n        allow_agent=True,\n        pkey=mykey,\n    )\n\n    sftp = ssh.open_sftp()\n    files = sftp.listdir(data_acquisition_detail.outbound_source_location)\n\n    pre_ingestion_processed_files = [\n        x.inbound_file_location for x in pre_ingestion_logs\n    ]\n\n    new_files = list()\n\n    for file_ in files:\n        path_s3 = path_to_s3(\n            location=data_acquisition_detail.inbound_location.rstrip(\"/\"),\n            env=env,\n        )\n\n        save_location_ = f\"{path_s3['s3_location']}/{file_}\"\n\n        if save_location_ not in pre_ingestion_processed_files:\n\n            try:\n                if PatternValidator.validate_pattern(\n                    file_pattern=data_acquisition_detail.outbound_source_file_pattern,\n                    file_name=file_,\n                    custom=(\n                        True\n                        if data_acquisition_detail.outbound_source_file_pattern_static\n                        == \"Y\"\n                        else False\n                    ),\n                ):\n                    new_files.append(save_location_)\n\n                    start_time = datetime.now()\n                    batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n                    remote_file = sftp.file(\n                        data_acquisition_detail.outbound_source_location + file_,\n                        mode=\"r\",\n                    )\n\n                    buffer = BytesIO()\n\n                    while True:\n                        data = remote_file.read(524288000)\n                        if not data:\n                            break\n                        buffer.write(data)\n                    buffer.seek(0)\n\n                    splited_ = data_acquisition_detail.inbound_location.split(\"/\")\n                    bucket_name = path_s3[\"bucket\"]\n                    splited_.pop(0)\n                    aws_file_key = \"/\".join(splited_) + file_.split(\"/\")[-1]\n\n                    S3Process.S3Process().s3_raw_file_write(\n                        file_object=buffer,\n                        bucket=bucket_name,\n                        file_name=aws_file_key,\n                    )\n                    orch_process.insert_log_data_acquisition_detail(\n                        log_data_acquisition=logDataAcquisitionDetail(\n                            batch_id=batch_id,\n                            run_date=start_time.date(),\n                            process_id=data_acquisition_detail.process_id,\n                            pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                            outbound_source_location=data_acquisition_detail.outbound_source_location,\n                            inbound_file_location=save_location_,\n                            status=\"SUCCEEDED\",\n                            start_time=start_time,\n                            end_time=datetime.now(),\n                        )\n                    )\n            except Exception as e:\n                orch_process.insert_log_data_acquisition_detail(\n                    log_data_acquisition=logDataAcquisitionDetail(\n                        batch_id=batch_id,\n                        run_date=start_time.date(),\n                        process_id=data_acquisition_detail.process_id,\n                        pre_ingestion_dataset_id=data_acquisition_detail.pre_ingestion_dataset_id,\n                        outbound_source_location=data_acquisition_detail.outbound_source_location,\n                        inbound_file_location=None,\n                        status=\"FAILED\",\n                        exception_details=traceback.format_exc(),\n                        start_time=start_time,\n                        end_time=datetime.now(),\n                    )\n                )\n                logger.error(traceback.format_exc())\n                raise\n    if len(new_files) == 0:\n        logger.error(f\"No unprocessed files are found.\")\n        raise Exception(f\"No unprocessed files are found.\")\n</code></pre>"},{"location":"GoldLayerScripts/Transformation/","title":"Documentation for <code>Transformation</code>","text":""},{"location":"GoldLayerScripts/Transformation/#datacraft_framework.GoldLayerScripts.Transformation.Transformation","title":"<code>datacraft_framework.GoldLayerScripts.Transformation.Transformation</code>","text":"<p>A class to orchestrate and execute data transformations from SILVER to Gold layers.</p> This class supports multiple transformation strategies <ul> <li>Direct mapping of source to target</li> <li>Union of multiple datasets</li> <li>Join-based transformations</li> <li>Custom SQL queries executed via Polars SQLContext</li> </ul> The class also handles <ul> <li>Adding system fields (<code>sys_checksum</code>, <code>eff_strt_dt</code>, etc.)</li> <li>Writing transformed data to Delta Lake</li> <li>Logging transformation status</li> </ul> <p>Attributes:</p> Name Type Description <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>The master details of a dataset.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/Transformation.py</code> <pre><code>class Transformation:\n    \"\"\"\n    A class to orchestrate and execute data transformations from SILVER to Gold layers.\n\n    This class supports multiple transformation strategies:\n        - Direct mapping of source to target\n        - Union of multiple datasets\n        - Join-based transformations\n        - Custom SQL queries executed via Polars SQLContext\n\n    The class also handles:\n        - Adding system fields (`sys_checksum`, `eff_strt_dt`, etc.)\n        - Writing transformed data to Delta Lake\n        - Logging transformation status\n\n    Attributes:\n        dataset_master (ctlDatasetMaster): The master details of a dataset.\n    \"\"\"\n\n    def __init__(self, dataset_master: ctlDatasetMaster):\n        \"\"\"\n        Initialize and dispatch transformation based on configuration.\n\n        Args:\n            dataset_master (ctlDatasetMaster): Configuration object containing transformation rules.\n\n        Raises:\n            Exception: If an unsupported transformation type is requested.\n        \"\"\"\n        self.dataset_master = dataset_master\n\n        with OrchestrationProcess() as orch_process:\n            transformation_depedencies = (\n                orch_process.get_transformation_dependency_master(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                )\n            )\n\n            if transformation_depedencies[0].transformation_type.lower() == \"direct\":\n                self.direct_transformation()\n            elif transformation_depedencies[0].transformation_type.lower() == \"join\":\n                self.join_transformation()\n            elif transformation_depedencies[0].transformation_type.lower() == \"union\":\n                self.union_transformation()\n            elif transformation_depedencies[0].transformation_type.lower() == \"custom\":\n                self.custom_transformation()\n            else:\n                raise Exception(\n                    f\"Unsupported Transformation: {transformation_depedencies[0].transformation_type}\"\n                )\n\n    def direct_transformation(self):\n        \"\"\"\n        Perform a direct transformation: read from one SILVER table and write to a Gold table.\n\n        Applies system columns and writes to Delta Lake. Uses SCD Type 2 upserts if the target exists.\n        \"\"\"\n        dataset_master = self.dataset_master\n\n        with OrchestrationProcess() as orch_process:\n            transformation_dependency = (\n                orch_process.get_transformation_dependency_master(\n                    dataset_id=dataset_master.dataset_id,\n                    process_id=dataset_master.process_id,\n                )[0]\n            )\n\n            dependent_dataset_id = transformation_dependency.depedent_dataset_id\n            primary_keys = transformation_dependency.primary_keys.split(\",\")\n            primary_key_conditions = \" AND \".join(\n                [f\"target.{key} = staging.{key}\" for key in primary_keys]\n            )\n\n            columns = orch_process.get_ctl_column_metadata(\n                dataset_id=dataset_master.dataset_id\n            )\n            columns = [x.column_name for x in columns]\n\n            target_table_details = orch_process.get_dataset_master(\n                process_id=dataset_master.process_id,\n                dataset_type=\"GOLD\",\n                dataset_id=dataset_master.dataset_id,\n            )\n            dependent_table_details = orch_process.get_dataset_master(\n                process_id=dataset_master.process_id,\n                dataset_type=\"BRONZE\",\n                dataset_id=dependent_dataset_id,\n            )\n\n            unprocessed_transformation_files = (\n                orch_process.get_unprocessed_transformation_files(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dependent_dataset_id,\n                )\n            )\n\n            dependent_table_details_s3 = path_to_s3(\n                location=dependent_table_details.staging_location,\n                env=env,\n            )\n            target_table_details_s3 = path_to_s3(\n                location=target_table_details.transformation_location,\n                env=env,\n            )\n\n            if len(unprocessed_transformation_files) == 0:\n                raise Exception(\n                    f\"No unprocess files found for Dataset ID {dataset_master.dataset_id}\"\n                )\n            else:\n                for unprocessed_file in unprocessed_transformation_files:\n                    start_time = datetime.now()\n\n                    batch_id = unprocessed_file.batch_id\n\n                    try:\n                        staging_df = DeltaTableRead(\n                            delta_path=dependent_table_details_s3[\"s3_location\"],\n                            batch_id=batch_id,\n                        ).read()\n\n                        staging_df = staging_df.drop(polars.col(\"batch_id\"))\n                        staging_df = staging_df.select(columns)\n\n                        staging_df = staging_df.with_columns(\n                            [\n                                polars.lit(datetime.now().date()).alias(\"data_date\"),\n                                polars.lit(batch_id).alias(\"batch_id\"),\n                                polars.lit(datetime.now().date()).alias(\"eff_strt_dt\"),\n                                polars.lit(\"N\").alias(\"sys_del_flg\"),\n                                polars.lit(\n                                    datetime(year=9999, month=12, day=31).date()\n                                ).alias(\"eff_end_dt\"),\n                                polars.lit(datetime.now()).alias(\"sys_created_ts\"),\n                                polars.lit(datetime.now()).alias(\"sys_modified_ts\"),\n                                plh.concat_str(*columns)\n                                .chash.sha256()\n                                .alias(\"sys_checksum\"),\n                            ]\n                        )\n\n                        if S3Process().s3_list_files(\n                            bucket=target_table_details_s3[\"bucket\"],\n                            file_name=target_table_details_s3[\"key\"],\n                        ):\n                            # Upsert logic here\n                            DeltaTableWriterScdType2(\n                                staging_df=staging_df,\n                                primary_keys=primary_key_conditions,\n                                delta_path=target_table_details_s3[\"s3_location\"],\n                            )\n                        else:\n                            DeltaTableWriter(\n                                input_data=staging_df,\n                                save_location=target_table_details_s3[\"s3_location\"],\n                                batch_id=batch_id,\n                                partition_columns=target_table_details.transformation_partition_columns,\n                            )\n                        orch_process.insert_log_transformation(\n                            log_transformation=logTransformationDtl(\n                                batch_id=batch_id,\n                                data_date=start_time.date(),\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                status=\"SUCCEEDED\",\n                                exception_details=None,\n                                transformation_start_time=start_time,\n                                transformation_end_time=datetime.now(),\n                            )\n                        )\n                    except Exception as e:\n                        orch_process.insert_log_transformation(\n                            log_transformation=logTransformationDtl(\n                                batch_id=batch_id,\n                                data_date=start_time.date(),\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                status=\"FAILED\",\n                                exception_details=traceback.format_exc(),\n                                transformation_start_time=start_time,\n                                transformation_end_time=datetime.now(),\n                            )\n                        )\n                        logger.error(traceback.format_exc())\n                        raise\n\n    def union_transformation(self):\n        \"\"\"\n        Perform a union transformation across multiple source datasets.\n\n        Combines records from multiple sources into a single Gold table.\n        Adds system columns and writes result using Delta Lake.\n        \"\"\"\n        dataset_master = self.dataset_master\n\n        with OrchestrationProcess() as orch_process:\n            transformation_depedencies = (\n                orch_process.get_transformation_dependency_master(\n                    dataset_id=dataset_master.dataset_id,\n                    process_id=dataset_master.process_id,\n                )\n            )\n\n            primary_keys = transformation_depedencies[0].primary_keys.split(\",\")\n            primary_key_conditions = \" AND \".join(\n                [f\"target.{key} = staging.{key}\" for key in primary_keys]\n            )\n\n            columns = orch_process.get_ctl_column_metadata(\n                dataset_id=dataset_master.dataset_id\n            )\n            columns = [x.column_name for x in columns]\n\n            target_table_details = orch_process.get_dataset_master(\n                process_id=dataset_master.process_id,\n                dataset_type=\"GOLD\",\n                dataset_id=dataset_master.dataset_id,\n            )\n\n            target_table_details_s3 = path_to_s3(\n                location=target_table_details.transformation_location,\n                env=env,\n            )\n\n            unprocessed_transformation_files = (\n                orch_process.get_unprocessed_transformation_files(\n                    process_id=dataset_master.process_id,\n                    dataset_id=transformation_depedencies[0].depedent_dataset_id,\n                )\n            )\n\n            if len(unprocessed_transformation_files) == 0:\n                raise Exception(\n                    f\"No unprocess files found for Dataset ID {dataset_master.dataset_id}\"\n                )\n            else:\n                source_details = list()\n\n                for transformation_depedency in transformation_depedencies:\n                    dependent_dataset_details = orch_process.get_dataset_master(\n                        process_id=dataset_master.process_id,\n                        dataset_id=transformation_depedency.depedent_dataset_id,\n                        dataset_type=\"BRONZE\",\n                    )\n\n                    source_details.append(\n                        {\n                            \"source_table_location\": dependent_dataset_details.staging_location,\n                            \"source_table_name\": dependent_dataset_details.staging_table,\n                            \"extra_value\": transformation_depedency.extra_values,\n                        }\n                    )\n\n                for unprocessed_file in unprocessed_transformation_files:\n                    start_time = datetime.now()\n                    batch_id = unprocessed_file.batch_id\n\n                    try:\n                        source_dfs: list[polars.DataFrame] = list()\n\n                        for source_detail in source_details:\n                            source_table_location = source_detail[\n                                \"source_table_location\"\n                            ]\n                            source_table_location_s3 = path_to_s3(\n                                location=source_table_location, env=env\n                            )\n\n                            if source_detail.get(\"extra_values\"):\n                                new_values = [\n                                    polars.lit(value.strip(\"'\")).alias(column_name)\n                                    for column_name, value in (\n                                        item.split(\"=\")\n                                        for item in source_detail[\"extra_values\"].split(\n                                            \",\"\n                                        )\n                                    )\n                                ]\n\n                                source_dfs.append(\n                                    DeltaTableRead(\n                                        delta_path=source_table_location_s3[\n                                            \"s3_location\"\n                                        ],\n                                        latest=True,\n                                    )\n                                    .read()\n                                    .with_columns(new_values)\n                                )\n                            else:\n                                source_dfs.append(\n                                    DeltaTableRead(\n                                        delta_path=source_table_location_s3[\n                                            \"s3_location\"\n                                        ],\n                                        latest=True,\n                                    ).read()\n                                )\n\n                        result_df = polars.concat(source_dfs).select(columns)\n                        final_df = result_df.with_columns(\n                            [\n                                polars.lit(datetime.now().date()).alias(\"data_date\"),\n                                polars.lit(batch_id).alias(\"batch_id\"),\n                                polars.lit(datetime.now().date()).alias(\"eff_strt_dt\"),\n                                polars.lit(\"N\").alias(\"sys_del_flg\"),\n                                polars.lit(\n                                    datetime(year=9999, month=12, day=31).date()\n                                ).alias(\"eff_end_dt\"),\n                                polars.lit(datetime.now()).alias(\"sys_created_ts\"),\n                                polars.lit(datetime.now()).alias(\"sys_modified_ts\"),\n                                plh.concat_str(*columns)\n                                .chash.sha256()\n                                .alias(\"sys_checksum\"),\n                            ]\n                        )\n\n                        if S3Process().s3_list_files(\n                            bucket=target_table_details_s3[\"bucket\"],\n                            file_name=target_table_details_s3[\"key\"],\n                        ):\n                            # Upsert logic here\n                            DeltaTableWriterScdType2(\n                                staging_df=final_df,\n                                primary_keys=primary_key_conditions,\n                                delta_path=target_table_details_s3[\"s3_location\"],\n                            )\n                        else:\n                            DeltaTableWriter(\n                                input_data=final_df,\n                                save_location=target_table_details_s3[\"s3_location\"],\n                                batch_id=batch_id,\n                                partition_columns=target_table_details.transformation_partition_columns,\n                            )\n                        orch_process.insert_log_transformation(\n                            log_transformation=logTransformationDtl(\n                                batch_id=batch_id,\n                                data_date=start_time.date(),\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                status=\"SUCCEEDED\",\n                                exception_details=None,\n                                transformation_start_time=start_time,\n                                transformation_end_time=datetime.now(),\n                            )\n                        )\n                    except Exception as e:\n                        orch_process.insert_log_transformation(\n                            log_transformation=logTransformationDtl(\n                                batch_id=batch_id,\n                                data_date=start_time.date(),\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                status=\"FAILED\",\n                                exception_details=traceback.format_exc(),\n                                transformation_start_time=start_time,\n                                transformation_end_time=datetime.now(),\n                            )\n                        )\n                        logger.error(traceback.format_exc())\n                        raise\n\n    def join_transformation(self):\n        \"\"\"\n        Perform a join-based transformation between multiple SILVER datasets.\n\n        Executes SQL-like joins between datasets and writes the result to Gold layer.\n        \"\"\"\n        dataset_master = self.dataset_master\n\n        with OrchestrationProcess() as orch_process:\n            transformation_depedencies = (\n                orch_process.get_transformation_dependency_master(\n                    dataset_id=dataset_master.dataset_id,\n                    process_id=dataset_master.process_id,\n                )\n            )\n\n            primary_keys = transformation_depedencies[0].primary_keys.split(\",\")\n            primary_key_conditions = \" AND \".join(\n                [f\"target.{key} = staging.{key}\" for key in primary_keys]\n            )\n\n            columns = orch_process.get_ctl_column_metadata(\n                dataset_id=dataset_master.dataset_id\n            )\n            columns = [x.column_name for x in columns]\n\n            target_table_details = orch_process.get_dataset_master(\n                process_id=dataset_master.process_id,\n                dataset_type=\"GOLD\",\n                dataset_id=dataset_master.dataset_id,\n            )\n\n            target_table_details_s3 = path_to_s3(\n                location=target_table_details.transformation_location,\n                env=env,\n            )\n\n            unprocessed_transformation_files = (\n                orch_process.get_unprocessed_transformation_files(\n                    process_id=dataset_master.process_id,\n                    dataset_id=transformation_depedencies[0].depedent_dataset_id,\n                )\n            )\n\n            if len(unprocessed_transformation_files) == 0:\n                raise Exception(\n                    f\"No unprocess files found for Dataset ID {dataset_master.dataset_id}\"\n                )\n            else:\n                source_details = list()\n\n                for transformation_depedency in transformation_depedencies:\n                    dependent_dataset_details = orch_process.get_dataset_master(\n                        process_id=dataset_master.process_id,\n                        dataset_id=transformation_depedency.depedent_dataset_id,\n                        dataset_type=\"BRONZE\",\n                    )\n                    df_source = (\n                        DeltaTableRead(\n                            path_to_s3(\n                                location=dependent_dataset_details.staging_location,\n                                env=env,\n                            )[\"s3_location\"],\n                            latest=True,\n                        )\n                        .read()\n                        .drop(\"batch_id\")\n                    )\n\n                    source_details.append(\n                        {\n                            \"table_name\": dependent_dataset_details.staging_table,\n                            \"dataframe\": df_source,\n                            \"how\": transformation_depedency.join_how,\n                            \"left_join_columns\": transformation_depedency.left_table_columns,\n                            \"right_join_columns\": transformation_depedency.right_table_columns,\n                            \"dependent_dataset_id\": transformation_depedency.depedent_dataset_id,\n                        }\n                    )\n\n                for unprocessed_file in unprocessed_transformation_files:\n                    start_time = datetime.now()\n                    batch_id = unprocessed_file.batch_id\n\n                    try:\n                        base_df = source_details[0][\"dataframe\"]\n\n                        for entry in source_details[1:]:\n                            df_right = entry[\"dataframe\"]\n                            how = entry[\"how\"].lower()\n                            left_on = entry[\"left_join_columns\"].split(\",\")\n                            right_on = entry[\"right_join_columns\"].split(\",\")\n\n                            # Safety check\n                            if len(left_on) != len(right_on):\n                                raise ValueError(\n                                    f\"Join key count mismatch: {left_on} vs {right_on}\"\n                                )\n\n                            base_df = base_df.join(\n                                df_right, left_on=left_on, right_on=right_on, how=how\n                            )\n\n                        final_df = base_df.with_columns(\n                            [\n                                polars.lit(datetime.now().date()).alias(\"data_date\"),\n                                polars.lit(batch_id).alias(\"batch_id\"),\n                                polars.lit(datetime.now().date()).alias(\"eff_strt_dt\"),\n                                polars.lit(\"N\").alias(\"sys_del_flg\"),\n                                polars.lit(\n                                    datetime(year=9999, month=12, day=31).date()\n                                ).alias(\"eff_end_dt\"),\n                                polars.lit(datetime.now()).alias(\"sys_created_ts\"),\n                                polars.lit(datetime.now()).alias(\"sys_modified_ts\"),\n                                plh.concat_str(*columns)\n                                .chash.sha256()\n                                .alias(\"sys_checksum\"),\n                            ]\n                        )\n\n                        if S3Process().s3_list_files(\n                            bucket=target_table_details_s3[\"bucket\"],\n                            file_name=target_table_details_s3[\"key\"],\n                        ):\n                            # Upsert logic here\n                            DeltaTableWriterScdType2(\n                                staging_df=final_df,\n                                primary_keys=primary_key_conditions,\n                                delta_path=target_table_details_s3[\"s3_location\"],\n                            )\n                        else:\n                            DeltaTableWriter(\n                                input_data=final_df,\n                                save_location=target_table_details_s3[\"s3_location\"],\n                                batch_id=batch_id,\n                                partition_columns=target_table_details.transformation_partition_columns,\n                            )\n\n                        orch_process.insert_log_transformation(\n                            log_transformation=logTransformationDtl(\n                                batch_id=batch_id,\n                                data_date=start_time.date(),\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                status=\"SUCCEEDED\",\n                                exception_details=None,\n                                transformation_start_time=start_time,\n                                transformation_end_time=datetime.now(),\n                            )\n                        )\n\n                    except Exception as e:\n                        orch_process.insert_log_transformation(\n                            log_transformation=logTransformationDtl(\n                                batch_id=batch_id,\n                                data_date=start_time.date(),\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                status=\"FAILED\",\n                                exception_details=traceback.format_exc(),\n                                transformation_start_time=start_time,\n                                transformation_end_time=datetime.now(),\n                            )\n                        )\n                        logger.error(traceback.format_exc())\n                        raise\n\n    def custom_transformation(self):\n        \"\"\"\n        Perform a custom transformation using SQL queries.\n\n        Reads data from one or more SILVER datasets and executes a user-defined SQL query\n        to produce the final Gold dataset.\n        \"\"\"\n        dataset_master = self.dataset_master\n\n        with OrchestrationProcess() as orch_process:\n            transformation_depedencies = (\n                orch_process.get_transformation_dependency_master(\n                    dataset_id=dataset_master.dataset_id,\n                    process_id=dataset_master.process_id,\n                )\n            )\n\n            primary_keys = transformation_depedencies[0].primary_keys.split(\",\")\n            primary_key_conditions = \" AND \".join(\n                [f\"target.{key} = staging.{key}\" for key in primary_keys]\n            )\n\n            columns = orch_process.get_ctl_column_metadata(\n                dataset_id=dataset_master.dataset_id\n            )\n            columns = [x.column_name for x in columns]\n\n            target_table_details = orch_process.get_dataset_master(\n                process_id=dataset_master.process_id,\n                dataset_type=\"GOLD\",\n                dataset_id=dataset_master.dataset_id,\n            )\n\n            target_table_details_s3 = path_to_s3(\n                location=target_table_details.transformation_location,\n                env=env,\n            )\n\n            unprocessed_transformation_files = (\n                orch_process.get_unprocessed_transformation_files(\n                    process_id=dataset_master.process_id,\n                    dataset_id=transformation_depedencies[0].depedent_dataset_id,\n                )\n            )\n\n            if len(unprocessed_transformation_files) == 0:\n                raise Exception(\n                    f\"No unprocess files found for Dataset ID {dataset_master.dataset_id}\"\n                )\n            else:\n                source_details = list()\n\n                for transformation_depedency in transformation_depedencies:\n                    dependent_dataset_details = orch_process.get_dataset_master(\n                        process_id=dataset_master.process_id,\n                        dataset_id=transformation_depedency.depedent_dataset_id,\n                        dataset_type=\"BRONZE\",\n                    )\n                    df_source = (\n                        DeltaTableRead(\n                            path_to_s3(\n                                location=dependent_dataset_details.staging_location,\n                                env=env,\n                            )[\"s3_location\"],\n                            latest=True,\n                        )\n                        .read()\n                        .drop(\"batch_id\")\n                    )\n\n                    source_details.append(\n                        {\n                            \"table_name\": dependent_dataset_details.staging_table,\n                            \"dataframe\": df_source,\n                        }\n                    )\n\n                for unprocessed_file in unprocessed_transformation_files:\n                    start_time = datetime.now()\n                    try:\n                        batch_id = unprocessed_file.batch_id\n\n                        sql_context = polars.SQLContext()\n                        for source_detail in source_details:\n                            sql_context.register(\n                                name=source_detail[\"table_name\"],\n                                frame=source_detail[\"dataframe\"],\n                            )\n\n                        result_df = sql_context.execute(\n                            transformation_depedencies[-1].custom_transformation_query\n                        ).collect()\n                        final_df = result_df.with_columns(\n                            [\n                                polars.lit(datetime.now().date()).alias(\"data_date\"),\n                                polars.lit(batch_id).alias(\"batch_id\"),\n                                polars.lit(datetime.now().date()).alias(\"eff_strt_dt\"),\n                                polars.lit(\"N\").alias(\"sys_del_flg\"),\n                                polars.lit(\n                                    datetime(year=9999, month=12, day=31).date()\n                                ).alias(\"eff_end_dt\"),\n                                polars.lit(datetime.now()).alias(\"sys_created_ts\"),\n                                polars.lit(datetime.now()).alias(\"sys_modified_ts\"),\n                                plh.concat_str(*columns)\n                                .chash.sha256()\n                                .alias(\"sys_checksum\"),\n                            ]\n                        )\n\n                        if S3Process().s3_list_files(\n                            bucket=target_table_details_s3[\"bucket\"],\n                            file_name=target_table_details_s3[\"key\"],\n                        ):\n                            # Upsert logic here\n                            DeltaTableWriterScdType2(\n                                staging_df=final_df,\n                                primary_keys=primary_key_conditions,\n                                delta_path=target_table_details_s3[\"s3_location\"],\n                            )\n                        else:\n                            DeltaTableWriter(\n                                input_data=final_df,\n                                save_location=target_table_details_s3[\"s3_location\"],\n                                batch_id=batch_id,\n                                partition_columns=target_table_details.transformation_partition_columns,\n                            )\n\n                        orch_process.insert_log_transformation(\n                            log_transformation=logTransformationDtl(\n                                batch_id=batch_id,\n                                data_date=start_time.date(),\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                status=\"SUCCEEDED\",\n                                exception_details=None,\n                                transformation_start_time=start_time,\n                                transformation_end_time=datetime.now(),\n                            )\n                        )\n                    except Exception as e:\n                        orch_process.insert_log_transformation(\n                            log_transformation=logTransformationDtl(\n                                batch_id=batch_id,\n                                data_date=start_time.date(),\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                status=\"FAILED\",\n                                exception_details=traceback.format_exc(),\n                                transformation_start_time=start_time,\n                                transformation_end_time=datetime.now(),\n                            )\n                        )\n                        logger.error(traceback.format_exc())\n                        raise\n</code></pre>"},{"location":"GoldLayerScripts/Transformation/#datacraft_framework.GoldLayerScripts.Transformation.Transformation.__init__","title":"<code>__init__(dataset_master)</code>","text":"<p>Initialize and dispatch transformation based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Configuration object containing transformation rules.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If an unsupported transformation type is requested.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/Transformation.py</code> <pre><code>def __init__(self, dataset_master: ctlDatasetMaster):\n    \"\"\"\n    Initialize and dispatch transformation based on configuration.\n\n    Args:\n        dataset_master (ctlDatasetMaster): Configuration object containing transformation rules.\n\n    Raises:\n        Exception: If an unsupported transformation type is requested.\n    \"\"\"\n    self.dataset_master = dataset_master\n\n    with OrchestrationProcess() as orch_process:\n        transformation_depedencies = (\n            orch_process.get_transformation_dependency_master(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n            )\n        )\n\n        if transformation_depedencies[0].transformation_type.lower() == \"direct\":\n            self.direct_transformation()\n        elif transformation_depedencies[0].transformation_type.lower() == \"join\":\n            self.join_transformation()\n        elif transformation_depedencies[0].transformation_type.lower() == \"union\":\n            self.union_transformation()\n        elif transformation_depedencies[0].transformation_type.lower() == \"custom\":\n            self.custom_transformation()\n        else:\n            raise Exception(\n                f\"Unsupported Transformation: {transformation_depedencies[0].transformation_type}\"\n            )\n</code></pre>"},{"location":"GoldLayerScripts/Transformation/#datacraft_framework.GoldLayerScripts.Transformation.Transformation.direct_transformation","title":"<code>direct_transformation()</code>","text":"<p>Perform a direct transformation: read from one SILVER table and write to a Gold table.</p> <p>Applies system columns and writes to Delta Lake. Uses SCD Type 2 upserts if the target exists.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/Transformation.py</code> <pre><code>def direct_transformation(self):\n    \"\"\"\n    Perform a direct transformation: read from one SILVER table and write to a Gold table.\n\n    Applies system columns and writes to Delta Lake. Uses SCD Type 2 upserts if the target exists.\n    \"\"\"\n    dataset_master = self.dataset_master\n\n    with OrchestrationProcess() as orch_process:\n        transformation_dependency = (\n            orch_process.get_transformation_dependency_master(\n                dataset_id=dataset_master.dataset_id,\n                process_id=dataset_master.process_id,\n            )[0]\n        )\n\n        dependent_dataset_id = transformation_dependency.depedent_dataset_id\n        primary_keys = transformation_dependency.primary_keys.split(\",\")\n        primary_key_conditions = \" AND \".join(\n            [f\"target.{key} = staging.{key}\" for key in primary_keys]\n        )\n\n        columns = orch_process.get_ctl_column_metadata(\n            dataset_id=dataset_master.dataset_id\n        )\n        columns = [x.column_name for x in columns]\n\n        target_table_details = orch_process.get_dataset_master(\n            process_id=dataset_master.process_id,\n            dataset_type=\"GOLD\",\n            dataset_id=dataset_master.dataset_id,\n        )\n        dependent_table_details = orch_process.get_dataset_master(\n            process_id=dataset_master.process_id,\n            dataset_type=\"BRONZE\",\n            dataset_id=dependent_dataset_id,\n        )\n\n        unprocessed_transformation_files = (\n            orch_process.get_unprocessed_transformation_files(\n                process_id=dataset_master.process_id,\n                dataset_id=dependent_dataset_id,\n            )\n        )\n\n        dependent_table_details_s3 = path_to_s3(\n            location=dependent_table_details.staging_location,\n            env=env,\n        )\n        target_table_details_s3 = path_to_s3(\n            location=target_table_details.transformation_location,\n            env=env,\n        )\n\n        if len(unprocessed_transformation_files) == 0:\n            raise Exception(\n                f\"No unprocess files found for Dataset ID {dataset_master.dataset_id}\"\n            )\n        else:\n            for unprocessed_file in unprocessed_transformation_files:\n                start_time = datetime.now()\n\n                batch_id = unprocessed_file.batch_id\n\n                try:\n                    staging_df = DeltaTableRead(\n                        delta_path=dependent_table_details_s3[\"s3_location\"],\n                        batch_id=batch_id,\n                    ).read()\n\n                    staging_df = staging_df.drop(polars.col(\"batch_id\"))\n                    staging_df = staging_df.select(columns)\n\n                    staging_df = staging_df.with_columns(\n                        [\n                            polars.lit(datetime.now().date()).alias(\"data_date\"),\n                            polars.lit(batch_id).alias(\"batch_id\"),\n                            polars.lit(datetime.now().date()).alias(\"eff_strt_dt\"),\n                            polars.lit(\"N\").alias(\"sys_del_flg\"),\n                            polars.lit(\n                                datetime(year=9999, month=12, day=31).date()\n                            ).alias(\"eff_end_dt\"),\n                            polars.lit(datetime.now()).alias(\"sys_created_ts\"),\n                            polars.lit(datetime.now()).alias(\"sys_modified_ts\"),\n                            plh.concat_str(*columns)\n                            .chash.sha256()\n                            .alias(\"sys_checksum\"),\n                        ]\n                    )\n\n                    if S3Process().s3_list_files(\n                        bucket=target_table_details_s3[\"bucket\"],\n                        file_name=target_table_details_s3[\"key\"],\n                    ):\n                        # Upsert logic here\n                        DeltaTableWriterScdType2(\n                            staging_df=staging_df,\n                            primary_keys=primary_key_conditions,\n                            delta_path=target_table_details_s3[\"s3_location\"],\n                        )\n                    else:\n                        DeltaTableWriter(\n                            input_data=staging_df,\n                            save_location=target_table_details_s3[\"s3_location\"],\n                            batch_id=batch_id,\n                            partition_columns=target_table_details.transformation_partition_columns,\n                        )\n                    orch_process.insert_log_transformation(\n                        log_transformation=logTransformationDtl(\n                            batch_id=batch_id,\n                            data_date=start_time.date(),\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            status=\"SUCCEEDED\",\n                            exception_details=None,\n                            transformation_start_time=start_time,\n                            transformation_end_time=datetime.now(),\n                        )\n                    )\n                except Exception as e:\n                    orch_process.insert_log_transformation(\n                        log_transformation=logTransformationDtl(\n                            batch_id=batch_id,\n                            data_date=start_time.date(),\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            status=\"FAILED\",\n                            exception_details=traceback.format_exc(),\n                            transformation_start_time=start_time,\n                            transformation_end_time=datetime.now(),\n                        )\n                    )\n                    logger.error(traceback.format_exc())\n                    raise\n</code></pre>"},{"location":"GoldLayerScripts/Transformation/#datacraft_framework.GoldLayerScripts.Transformation.Transformation.union_transformation","title":"<code>union_transformation()</code>","text":"<p>Perform a union transformation across multiple source datasets.</p> <p>Combines records from multiple sources into a single Gold table. Adds system columns and writes result using Delta Lake.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/Transformation.py</code> <pre><code>def union_transformation(self):\n    \"\"\"\n    Perform a union transformation across multiple source datasets.\n\n    Combines records from multiple sources into a single Gold table.\n    Adds system columns and writes result using Delta Lake.\n    \"\"\"\n    dataset_master = self.dataset_master\n\n    with OrchestrationProcess() as orch_process:\n        transformation_depedencies = (\n            orch_process.get_transformation_dependency_master(\n                dataset_id=dataset_master.dataset_id,\n                process_id=dataset_master.process_id,\n            )\n        )\n\n        primary_keys = transformation_depedencies[0].primary_keys.split(\",\")\n        primary_key_conditions = \" AND \".join(\n            [f\"target.{key} = staging.{key}\" for key in primary_keys]\n        )\n\n        columns = orch_process.get_ctl_column_metadata(\n            dataset_id=dataset_master.dataset_id\n        )\n        columns = [x.column_name for x in columns]\n\n        target_table_details = orch_process.get_dataset_master(\n            process_id=dataset_master.process_id,\n            dataset_type=\"GOLD\",\n            dataset_id=dataset_master.dataset_id,\n        )\n\n        target_table_details_s3 = path_to_s3(\n            location=target_table_details.transformation_location,\n            env=env,\n        )\n\n        unprocessed_transformation_files = (\n            orch_process.get_unprocessed_transformation_files(\n                process_id=dataset_master.process_id,\n                dataset_id=transformation_depedencies[0].depedent_dataset_id,\n            )\n        )\n\n        if len(unprocessed_transformation_files) == 0:\n            raise Exception(\n                f\"No unprocess files found for Dataset ID {dataset_master.dataset_id}\"\n            )\n        else:\n            source_details = list()\n\n            for transformation_depedency in transformation_depedencies:\n                dependent_dataset_details = orch_process.get_dataset_master(\n                    process_id=dataset_master.process_id,\n                    dataset_id=transformation_depedency.depedent_dataset_id,\n                    dataset_type=\"BRONZE\",\n                )\n\n                source_details.append(\n                    {\n                        \"source_table_location\": dependent_dataset_details.staging_location,\n                        \"source_table_name\": dependent_dataset_details.staging_table,\n                        \"extra_value\": transformation_depedency.extra_values,\n                    }\n                )\n\n            for unprocessed_file in unprocessed_transformation_files:\n                start_time = datetime.now()\n                batch_id = unprocessed_file.batch_id\n\n                try:\n                    source_dfs: list[polars.DataFrame] = list()\n\n                    for source_detail in source_details:\n                        source_table_location = source_detail[\n                            \"source_table_location\"\n                        ]\n                        source_table_location_s3 = path_to_s3(\n                            location=source_table_location, env=env\n                        )\n\n                        if source_detail.get(\"extra_values\"):\n                            new_values = [\n                                polars.lit(value.strip(\"'\")).alias(column_name)\n                                for column_name, value in (\n                                    item.split(\"=\")\n                                    for item in source_detail[\"extra_values\"].split(\n                                        \",\"\n                                    )\n                                )\n                            ]\n\n                            source_dfs.append(\n                                DeltaTableRead(\n                                    delta_path=source_table_location_s3[\n                                        \"s3_location\"\n                                    ],\n                                    latest=True,\n                                )\n                                .read()\n                                .with_columns(new_values)\n                            )\n                        else:\n                            source_dfs.append(\n                                DeltaTableRead(\n                                    delta_path=source_table_location_s3[\n                                        \"s3_location\"\n                                    ],\n                                    latest=True,\n                                ).read()\n                            )\n\n                    result_df = polars.concat(source_dfs).select(columns)\n                    final_df = result_df.with_columns(\n                        [\n                            polars.lit(datetime.now().date()).alias(\"data_date\"),\n                            polars.lit(batch_id).alias(\"batch_id\"),\n                            polars.lit(datetime.now().date()).alias(\"eff_strt_dt\"),\n                            polars.lit(\"N\").alias(\"sys_del_flg\"),\n                            polars.lit(\n                                datetime(year=9999, month=12, day=31).date()\n                            ).alias(\"eff_end_dt\"),\n                            polars.lit(datetime.now()).alias(\"sys_created_ts\"),\n                            polars.lit(datetime.now()).alias(\"sys_modified_ts\"),\n                            plh.concat_str(*columns)\n                            .chash.sha256()\n                            .alias(\"sys_checksum\"),\n                        ]\n                    )\n\n                    if S3Process().s3_list_files(\n                        bucket=target_table_details_s3[\"bucket\"],\n                        file_name=target_table_details_s3[\"key\"],\n                    ):\n                        # Upsert logic here\n                        DeltaTableWriterScdType2(\n                            staging_df=final_df,\n                            primary_keys=primary_key_conditions,\n                            delta_path=target_table_details_s3[\"s3_location\"],\n                        )\n                    else:\n                        DeltaTableWriter(\n                            input_data=final_df,\n                            save_location=target_table_details_s3[\"s3_location\"],\n                            batch_id=batch_id,\n                            partition_columns=target_table_details.transformation_partition_columns,\n                        )\n                    orch_process.insert_log_transformation(\n                        log_transformation=logTransformationDtl(\n                            batch_id=batch_id,\n                            data_date=start_time.date(),\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            status=\"SUCCEEDED\",\n                            exception_details=None,\n                            transformation_start_time=start_time,\n                            transformation_end_time=datetime.now(),\n                        )\n                    )\n                except Exception as e:\n                    orch_process.insert_log_transformation(\n                        log_transformation=logTransformationDtl(\n                            batch_id=batch_id,\n                            data_date=start_time.date(),\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            status=\"FAILED\",\n                            exception_details=traceback.format_exc(),\n                            transformation_start_time=start_time,\n                            transformation_end_time=datetime.now(),\n                        )\n                    )\n                    logger.error(traceback.format_exc())\n                    raise\n</code></pre>"},{"location":"GoldLayerScripts/Transformation/#datacraft_framework.GoldLayerScripts.Transformation.Transformation.join_transformation","title":"<code>join_transformation()</code>","text":"<p>Perform a join-based transformation between multiple SILVER datasets.</p> <p>Executes SQL-like joins between datasets and writes the result to Gold layer.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/Transformation.py</code> <pre><code>def join_transformation(self):\n    \"\"\"\n    Perform a join-based transformation between multiple SILVER datasets.\n\n    Executes SQL-like joins between datasets and writes the result to Gold layer.\n    \"\"\"\n    dataset_master = self.dataset_master\n\n    with OrchestrationProcess() as orch_process:\n        transformation_depedencies = (\n            orch_process.get_transformation_dependency_master(\n                dataset_id=dataset_master.dataset_id,\n                process_id=dataset_master.process_id,\n            )\n        )\n\n        primary_keys = transformation_depedencies[0].primary_keys.split(\",\")\n        primary_key_conditions = \" AND \".join(\n            [f\"target.{key} = staging.{key}\" for key in primary_keys]\n        )\n\n        columns = orch_process.get_ctl_column_metadata(\n            dataset_id=dataset_master.dataset_id\n        )\n        columns = [x.column_name for x in columns]\n\n        target_table_details = orch_process.get_dataset_master(\n            process_id=dataset_master.process_id,\n            dataset_type=\"GOLD\",\n            dataset_id=dataset_master.dataset_id,\n        )\n\n        target_table_details_s3 = path_to_s3(\n            location=target_table_details.transformation_location,\n            env=env,\n        )\n\n        unprocessed_transformation_files = (\n            orch_process.get_unprocessed_transformation_files(\n                process_id=dataset_master.process_id,\n                dataset_id=transformation_depedencies[0].depedent_dataset_id,\n            )\n        )\n\n        if len(unprocessed_transformation_files) == 0:\n            raise Exception(\n                f\"No unprocess files found for Dataset ID {dataset_master.dataset_id}\"\n            )\n        else:\n            source_details = list()\n\n            for transformation_depedency in transformation_depedencies:\n                dependent_dataset_details = orch_process.get_dataset_master(\n                    process_id=dataset_master.process_id,\n                    dataset_id=transformation_depedency.depedent_dataset_id,\n                    dataset_type=\"BRONZE\",\n                )\n                df_source = (\n                    DeltaTableRead(\n                        path_to_s3(\n                            location=dependent_dataset_details.staging_location,\n                            env=env,\n                        )[\"s3_location\"],\n                        latest=True,\n                    )\n                    .read()\n                    .drop(\"batch_id\")\n                )\n\n                source_details.append(\n                    {\n                        \"table_name\": dependent_dataset_details.staging_table,\n                        \"dataframe\": df_source,\n                        \"how\": transformation_depedency.join_how,\n                        \"left_join_columns\": transformation_depedency.left_table_columns,\n                        \"right_join_columns\": transformation_depedency.right_table_columns,\n                        \"dependent_dataset_id\": transformation_depedency.depedent_dataset_id,\n                    }\n                )\n\n            for unprocessed_file in unprocessed_transformation_files:\n                start_time = datetime.now()\n                batch_id = unprocessed_file.batch_id\n\n                try:\n                    base_df = source_details[0][\"dataframe\"]\n\n                    for entry in source_details[1:]:\n                        df_right = entry[\"dataframe\"]\n                        how = entry[\"how\"].lower()\n                        left_on = entry[\"left_join_columns\"].split(\",\")\n                        right_on = entry[\"right_join_columns\"].split(\",\")\n\n                        # Safety check\n                        if len(left_on) != len(right_on):\n                            raise ValueError(\n                                f\"Join key count mismatch: {left_on} vs {right_on}\"\n                            )\n\n                        base_df = base_df.join(\n                            df_right, left_on=left_on, right_on=right_on, how=how\n                        )\n\n                    final_df = base_df.with_columns(\n                        [\n                            polars.lit(datetime.now().date()).alias(\"data_date\"),\n                            polars.lit(batch_id).alias(\"batch_id\"),\n                            polars.lit(datetime.now().date()).alias(\"eff_strt_dt\"),\n                            polars.lit(\"N\").alias(\"sys_del_flg\"),\n                            polars.lit(\n                                datetime(year=9999, month=12, day=31).date()\n                            ).alias(\"eff_end_dt\"),\n                            polars.lit(datetime.now()).alias(\"sys_created_ts\"),\n                            polars.lit(datetime.now()).alias(\"sys_modified_ts\"),\n                            plh.concat_str(*columns)\n                            .chash.sha256()\n                            .alias(\"sys_checksum\"),\n                        ]\n                    )\n\n                    if S3Process().s3_list_files(\n                        bucket=target_table_details_s3[\"bucket\"],\n                        file_name=target_table_details_s3[\"key\"],\n                    ):\n                        # Upsert logic here\n                        DeltaTableWriterScdType2(\n                            staging_df=final_df,\n                            primary_keys=primary_key_conditions,\n                            delta_path=target_table_details_s3[\"s3_location\"],\n                        )\n                    else:\n                        DeltaTableWriter(\n                            input_data=final_df,\n                            save_location=target_table_details_s3[\"s3_location\"],\n                            batch_id=batch_id,\n                            partition_columns=target_table_details.transformation_partition_columns,\n                        )\n\n                    orch_process.insert_log_transformation(\n                        log_transformation=logTransformationDtl(\n                            batch_id=batch_id,\n                            data_date=start_time.date(),\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            status=\"SUCCEEDED\",\n                            exception_details=None,\n                            transformation_start_time=start_time,\n                            transformation_end_time=datetime.now(),\n                        )\n                    )\n\n                except Exception as e:\n                    orch_process.insert_log_transformation(\n                        log_transformation=logTransformationDtl(\n                            batch_id=batch_id,\n                            data_date=start_time.date(),\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            status=\"FAILED\",\n                            exception_details=traceback.format_exc(),\n                            transformation_start_time=start_time,\n                            transformation_end_time=datetime.now(),\n                        )\n                    )\n                    logger.error(traceback.format_exc())\n                    raise\n</code></pre>"},{"location":"GoldLayerScripts/Transformation/#datacraft_framework.GoldLayerScripts.Transformation.Transformation.custom_transformation","title":"<code>custom_transformation()</code>","text":"<p>Perform a custom transformation using SQL queries.</p> <p>Reads data from one or more SILVER datasets and executes a user-defined SQL query to produce the final Gold dataset.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/Transformation.py</code> <pre><code>def custom_transformation(self):\n    \"\"\"\n    Perform a custom transformation using SQL queries.\n\n    Reads data from one or more SILVER datasets and executes a user-defined SQL query\n    to produce the final Gold dataset.\n    \"\"\"\n    dataset_master = self.dataset_master\n\n    with OrchestrationProcess() as orch_process:\n        transformation_depedencies = (\n            orch_process.get_transformation_dependency_master(\n                dataset_id=dataset_master.dataset_id,\n                process_id=dataset_master.process_id,\n            )\n        )\n\n        primary_keys = transformation_depedencies[0].primary_keys.split(\",\")\n        primary_key_conditions = \" AND \".join(\n            [f\"target.{key} = staging.{key}\" for key in primary_keys]\n        )\n\n        columns = orch_process.get_ctl_column_metadata(\n            dataset_id=dataset_master.dataset_id\n        )\n        columns = [x.column_name for x in columns]\n\n        target_table_details = orch_process.get_dataset_master(\n            process_id=dataset_master.process_id,\n            dataset_type=\"GOLD\",\n            dataset_id=dataset_master.dataset_id,\n        )\n\n        target_table_details_s3 = path_to_s3(\n            location=target_table_details.transformation_location,\n            env=env,\n        )\n\n        unprocessed_transformation_files = (\n            orch_process.get_unprocessed_transformation_files(\n                process_id=dataset_master.process_id,\n                dataset_id=transformation_depedencies[0].depedent_dataset_id,\n            )\n        )\n\n        if len(unprocessed_transformation_files) == 0:\n            raise Exception(\n                f\"No unprocess files found for Dataset ID {dataset_master.dataset_id}\"\n            )\n        else:\n            source_details = list()\n\n            for transformation_depedency in transformation_depedencies:\n                dependent_dataset_details = orch_process.get_dataset_master(\n                    process_id=dataset_master.process_id,\n                    dataset_id=transformation_depedency.depedent_dataset_id,\n                    dataset_type=\"BRONZE\",\n                )\n                df_source = (\n                    DeltaTableRead(\n                        path_to_s3(\n                            location=dependent_dataset_details.staging_location,\n                            env=env,\n                        )[\"s3_location\"],\n                        latest=True,\n                    )\n                    .read()\n                    .drop(\"batch_id\")\n                )\n\n                source_details.append(\n                    {\n                        \"table_name\": dependent_dataset_details.staging_table,\n                        \"dataframe\": df_source,\n                    }\n                )\n\n            for unprocessed_file in unprocessed_transformation_files:\n                start_time = datetime.now()\n                try:\n                    batch_id = unprocessed_file.batch_id\n\n                    sql_context = polars.SQLContext()\n                    for source_detail in source_details:\n                        sql_context.register(\n                            name=source_detail[\"table_name\"],\n                            frame=source_detail[\"dataframe\"],\n                        )\n\n                    result_df = sql_context.execute(\n                        transformation_depedencies[-1].custom_transformation_query\n                    ).collect()\n                    final_df = result_df.with_columns(\n                        [\n                            polars.lit(datetime.now().date()).alias(\"data_date\"),\n                            polars.lit(batch_id).alias(\"batch_id\"),\n                            polars.lit(datetime.now().date()).alias(\"eff_strt_dt\"),\n                            polars.lit(\"N\").alias(\"sys_del_flg\"),\n                            polars.lit(\n                                datetime(year=9999, month=12, day=31).date()\n                            ).alias(\"eff_end_dt\"),\n                            polars.lit(datetime.now()).alias(\"sys_created_ts\"),\n                            polars.lit(datetime.now()).alias(\"sys_modified_ts\"),\n                            plh.concat_str(*columns)\n                            .chash.sha256()\n                            .alias(\"sys_checksum\"),\n                        ]\n                    )\n\n                    if S3Process().s3_list_files(\n                        bucket=target_table_details_s3[\"bucket\"],\n                        file_name=target_table_details_s3[\"key\"],\n                    ):\n                        # Upsert logic here\n                        DeltaTableWriterScdType2(\n                            staging_df=final_df,\n                            primary_keys=primary_key_conditions,\n                            delta_path=target_table_details_s3[\"s3_location\"],\n                        )\n                    else:\n                        DeltaTableWriter(\n                            input_data=final_df,\n                            save_location=target_table_details_s3[\"s3_location\"],\n                            batch_id=batch_id,\n                            partition_columns=target_table_details.transformation_partition_columns,\n                        )\n\n                    orch_process.insert_log_transformation(\n                        log_transformation=logTransformationDtl(\n                            batch_id=batch_id,\n                            data_date=start_time.date(),\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            status=\"SUCCEEDED\",\n                            exception_details=None,\n                            transformation_start_time=start_time,\n                            transformation_end_time=datetime.now(),\n                        )\n                    )\n                except Exception as e:\n                    orch_process.insert_log_transformation(\n                        log_transformation=logTransformationDtl(\n                            batch_id=batch_id,\n                            data_date=start_time.date(),\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            status=\"FAILED\",\n                            exception_details=traceback.format_exc(),\n                            transformation_start_time=start_time,\n                            transformation_end_time=datetime.now(),\n                        )\n                    )\n                    logger.error(traceback.format_exc())\n                    raise\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/","title":"Documentation for <code>TransformationDataQualityCheck</code>","text":""},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck","title":"<code>datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck</code>","text":"<p>A class to perform data quality checks (DQM) on transformed data before publishing.</p> <p>Supports multiple QC types like null check, unique check, domain validation, etc. Integrates with orchestration process to log DQM results and optionally raise errors based on failure percentage thresholds.</p> <p>Attributes:</p> Name Type Description <code>None</code> <p>All operations are stateless and executed during initialization.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>class TransformationDataQualityCheck:\n    \"\"\"\n    A class to perform data quality checks (DQM) on transformed data before publishing.\n\n    Supports multiple QC types like null check, unique check, domain validation, etc.\n    Integrates with orchestration process to log DQM results and optionally raise errors\n    based on failure percentage thresholds.\n\n    Attributes:\n        None: All operations are stateless and executed during initialization.\n    \"\"\"\n\n    def null_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Perform null value check on specified column.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Orchestration utility for logging.\n\n        Returns:\n            polars.DataFrame: Filtered DataFrame containing only non-null rows.\n\n        Raises:\n            Exception: If criticality threshold is crossed and rule is marked 'C' (Critical).\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n            null_check_dqm_ = df.filter(\n                polars.col(dqm_detail.column_name).is_not_null()\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n        else:\n            null_check_dqm_ = df.filter(\n                polars.col(dqm_detail.column_name).is_not_null()\n            )\n\n        failure_count = total_count - len(null_check_dqm_)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(null_check_dqm_, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return null_check_dqm_\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return null_check_dqm_\n\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return null_check_dqm_\n\n    def unique_dqm(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Check that all values in a column (or set of columns) are unique.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Orchestration utility for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with unique values only.\n\n        Raises:\n            Exception: If criticality threshold is crossed and rule is marked 'C' (Critical).\n        \"\"\"\n        start_time = datetime.now()\n        total_count = len(df)\n\n        unique_dqm_df = df.unique(subset=dqm_detail.column_name.split(\",\"))\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            unique_dqm_df = unique_dqm_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(unique_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(unique_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=0,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return unique_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return unique_dqm_df\n\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return unique_dqm_df\n\n    def length_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Validate string length against a user-defined expression.\n\n        Supports conditions like \"&gt;\", \"&lt;\", \"==\", etc., applied to string length.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid-length strings.\n\n        Raises:\n            Exception: If invalid lengths exceed criticality threshold and marked 'C'.\n        \"\"\"\n        start_time = datetime.now()\n        total_count = len(df)\n\n        parsed_qc_param = json_loads(dqm_detail.qc_param)\n        param_exp = parsed_qc_param[\"expression\"]\n        param_value = parsed_qc_param[\"value\"]\n\n        length_validation_df = df.sql(\n            f\"SELECT * FROM self WHERE length({dqm_detail.column_name}) {param_exp} {param_value}\"\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            length_validation_df = length_validation_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(length_validation_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(length_validation_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return length_validation_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return length_validation_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return length_validation_df\n\n    def date_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Validate column values match expected date format using regex patterns.\n\n        Uses `get_date_regex()` to determine acceptable date formats based on config.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid date-formatted values.\n\n        Raises:\n            Exception: If invalid dates exceed criticality threshold and marked 'C'.\n        \"\"\"\n        start_time = datetime.now()\n        total_count = len(df)\n\n        date_regex_pattern = get_date_regex(qc_param=dqm_detail.qc_param)\n\n        date_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).str.contains(date_regex_pattern)\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            date_dqm_df = date_dqm_df.filter(polars.col(dqm_detail.column_name)).sql(\n                f\"SELECT * FROM self WHERE {filter_cols}\"\n            )\n\n        failure_count = total_count - len(date_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(date_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return date_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return date_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return date_dqm_df\n\n    def integer_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        r\"\"\"\n        Validate column values are integers.\n\n        Uses regex pattern `(^\\d+$)` to identify valid integers.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid integer values.\n\n        Raises:\n            Exception: If invalid values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        integer_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).str.contains(r\"(^-?\\d+$)\")\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            integer_dqm_df = integer_dqm_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(integer_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(integer_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return integer_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return integer_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return integer_dqm_df\n\n    def decimal_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        r\"\"\"\n        Validate column values are numeric (including decimals).\n\n        Uses regex pattern `(^\\d*[.]?\\d+$)` to identify valid numeric values.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid decimal values.\n\n        Raises:\n            Exception: If invalid values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        decimal_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).str.contains(r\"(^-?\\d+$)\")\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            decimal_dqm_df = decimal_dqm_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(decimal_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(decimal_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return decimal_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return decimal_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return decimal_dqm_df\n\n    def domain_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Validate column values fall within a defined domain.\n\n        Accepts comma-separated allowed values from `qc_param`.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid domain values.\n\n        Raises:\n            Exception: If invalid values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        domain_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).is_in(dqm_detail.qc_param.split(\",\"))\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            domain_dqm_df = domain_dqm_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(domain_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(domain_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return domain_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return domain_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return domain_dqm_df\n\n    def custom_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Apply custom SQL-like filter to perform flexible quality checks.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only records passing the custom filter.\n\n        Raises:\n            Exception: If invalid values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        custom_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).sql(dqm_detail.qc_param)\n        )\n\n        failure_count = total_count - len(custom_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(custom_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return custom_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return custom_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return custom_dqm_df\n\n    def __init__(\n        self,\n        dqm_details: list[ctlDqmMasterDtl],\n        dataset_master: ctlDatasetMaster,\n    ):\n        \"\"\"\n        Initialize and execute data quality management (DQM) checks on transformation files.\n\n        This constructor:\n        - Retrieves unprocessed transformation files for DQM validation\n        - Applies each DQM rule defined in `dqm_details`\n        - Filters, transforms, and overwrites active valid data to the publish layer using Delta Lake\n        - Logs results into `logDqmDtl` table via OrchestrationProcess\n\n        If no DQM rules are provided, it directly casts schema and writes data without validation.\n\n        Args:\n            dqm_details (List[ctlDqmMasterDtl]): A list of DQM rule definitions.\n                Each rule defines the type, condition, and criticality level of the check.\n            dataset_master (ctlDatasetMaster): Dataset metadata object containing configuration like paths,\n                partitioning info, process ID, etc.\n\n        Raises:\n            Exception: If no unprocessed files are found or any critical DQM check fails.\n        \"\"\"\n\n        with OrchestrationProcess() as orch_process:\n            unprocessed_files = orch_process.get_transformation_dqm_unprocessed_files(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n            )\n\n            ctl_column_metadata = orch_process.get_ctl_column_metadata(\n                dataset_id=dataset_master.dataset_id\n            )\n\n            if len(unprocessed_files) == 0:\n                raise Exception(\n                    f\"No unprocess files found for Data Quality Checks for dataset id: {dataset_master.dataset_id}\"\n                )\n\n            for unprocessed_file in unprocessed_files:\n                transformation_path = path_to_s3(\n                    location=dataset_master.transformation_location,\n                    env=env,\n                )\n\n                publish_dqm_path = path_to_s3(\n                    location=dataset_master.publish_location, env=env\n                )\n\n                if len(dqm_details) != 0:\n                    # First read data standard data.\n                    original_df = DeltaTableRead(\n                        delta_path=transformation_path[\"s3_location\"],\n                    ).read()\n\n                    dqm_check_df = original_df\n\n                    for dqm_detail in dqm_details:\n                        if dqm_detail.qc_type.lower() == \"null\":\n                            dqm_check_df = self.null_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"unique\":\n                            dqm_check_df = self.unique_dqm(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"decimal\":\n                            dqm_check_df = self.decimal_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"integer\":\n                            dqm_check_df = self.integer_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"length\":\n                            dqm_check_df = self.length_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"date\":\n                            dqm_check_df = self.date_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"domain\":\n                            dqm_check_df = self.domain_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"custom\":\n                            dqm_check_df = self.custom_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                    schame_casted_df = SchemaCaster(\n                        df=dqm_check_df.filter(polars.col(\"sys_del_flg\") == \"N\"),\n                        column_metadata=ctl_column_metadata,\n                    ).start()\n\n                    DeltaTablePublishWrite(\n                        input_data=schame_casted_df,\n                        save_location=publish_dqm_path[\"s3_location\"],\n                        partition_columns=dataset_master.publish_partition_columns,\n                        batch_id=unprocessed_file.batch_id,\n                    )\n\n                else:\n                    start_time = datetime.now()\n                    original_df = DeltaTableRead(\n                        delta_path=transformation_path[\"s3_location\"],\n                    ).read()\n                    schame_casted_df = SchemaCaster(\n                        df=original_df.filter(polars.col(\"sys_del_flg\") == \"N\"),\n                        column_metadata=ctl_column_metadata,\n                    ).start()\n                    DeltaTablePublishWrite(\n                        input_data=schame_casted_df,\n                        save_location=publish_dqm_path[\"s3_location\"],\n                        partition_columns=dataset_master.publish_partition_columns,\n                        batch_id=unprocessed_file.batch_id,\n                    )\n                    orch_process.insert_log_dqm(\n                        log_dqm=logDqmDtl(\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            column_name=None,\n                            qc_type=None,\n                            qc_param=None,\n                            qc_filter=None,\n                            criticality=None,\n                            criticality_threshold_pct=None,\n                            error_count=0,\n                            error_pct=0,\n                            status=\"SUCCEEDED\",\n                            dqm_start_time=start_time,\n                            dqm_end_time=datetime.now(),\n                        )\n                    )\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.null_check","title":"<code>null_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Perform null value check on specified column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Orchestration utility for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: Filtered DataFrame containing only non-null rows.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If criticality threshold is crossed and rule is marked 'C' (Critical).</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def null_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Perform null value check on specified column.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Orchestration utility for logging.\n\n    Returns:\n        polars.DataFrame: Filtered DataFrame containing only non-null rows.\n\n    Raises:\n        Exception: If criticality threshold is crossed and rule is marked 'C' (Critical).\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n        null_check_dqm_ = df.filter(\n            polars.col(dqm_detail.column_name).is_not_null()\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n    else:\n        null_check_dqm_ = df.filter(\n            polars.col(dqm_detail.column_name).is_not_null()\n        )\n\n    failure_count = total_count - len(null_check_dqm_)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(null_check_dqm_, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return null_check_dqm_\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return null_check_dqm_\n\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=failure_percentage,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return null_check_dqm_\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.unique_dqm","title":"<code>unique_dqm(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Check that all values in a column (or set of columns) are unique.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Orchestration utility for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with unique values only.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If criticality threshold is crossed and rule is marked 'C' (Critical).</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def unique_dqm(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Check that all values in a column (or set of columns) are unique.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Orchestration utility for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with unique values only.\n\n    Raises:\n        Exception: If criticality threshold is crossed and rule is marked 'C' (Critical).\n    \"\"\"\n    start_time = datetime.now()\n    total_count = len(df)\n\n    unique_dqm_df = df.unique(subset=dqm_detail.column_name.split(\",\"))\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        unique_dqm_df = unique_dqm_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(unique_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(unique_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return unique_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return unique_dqm_df\n\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return unique_dqm_df\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.length_dqm_check","title":"<code>length_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate string length against a user-defined expression.</p> <p>Supports conditions like \"&gt;\", \"&lt;\", \"==\", etc., applied to string length.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid-length strings.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid lengths exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def length_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Validate string length against a user-defined expression.\n\n    Supports conditions like \"&gt;\", \"&lt;\", \"==\", etc., applied to string length.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid-length strings.\n\n    Raises:\n        Exception: If invalid lengths exceed criticality threshold and marked 'C'.\n    \"\"\"\n    start_time = datetime.now()\n    total_count = len(df)\n\n    parsed_qc_param = json_loads(dqm_detail.qc_param)\n    param_exp = parsed_qc_param[\"expression\"]\n    param_value = parsed_qc_param[\"value\"]\n\n    length_validation_df = df.sql(\n        f\"SELECT * FROM self WHERE length({dqm_detail.column_name}) {param_exp} {param_value}\"\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        length_validation_df = length_validation_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(length_validation_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(length_validation_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return length_validation_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return length_validation_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return length_validation_df\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.date_dqm_check","title":"<code>date_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate column values match expected date format using regex patterns.</p> <p>Uses <code>get_date_regex()</code> to determine acceptable date formats based on config.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid date-formatted values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid dates exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def date_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Validate column values match expected date format using regex patterns.\n\n    Uses `get_date_regex()` to determine acceptable date formats based on config.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid date-formatted values.\n\n    Raises:\n        Exception: If invalid dates exceed criticality threshold and marked 'C'.\n    \"\"\"\n    start_time = datetime.now()\n    total_count = len(df)\n\n    date_regex_pattern = get_date_regex(qc_param=dqm_detail.qc_param)\n\n    date_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).str.contains(date_regex_pattern)\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        date_dqm_df = date_dqm_df.filter(polars.col(dqm_detail.column_name)).sql(\n            f\"SELECT * FROM self WHERE {filter_cols}\"\n        )\n\n    failure_count = total_count - len(date_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(date_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return date_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return date_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return date_dqm_df\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.integer_dqm_check","title":"<code>integer_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate column values are integers.</p> <p>Uses regex pattern <code>(^\\d+$)</code> to identify valid integers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid integer values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def integer_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    r\"\"\"\n    Validate column values are integers.\n\n    Uses regex pattern `(^\\d+$)` to identify valid integers.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid integer values.\n\n    Raises:\n        Exception: If invalid values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    integer_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).str.contains(r\"(^-?\\d+$)\")\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        integer_dqm_df = integer_dqm_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(integer_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(integer_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return integer_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return integer_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return integer_dqm_df\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.decimal_dqm_check","title":"<code>decimal_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate column values are numeric (including decimals).</p> <p>Uses regex pattern <code>(^\\d*[.]?\\d+$)</code> to identify valid numeric values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid decimal values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def decimal_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    r\"\"\"\n    Validate column values are numeric (including decimals).\n\n    Uses regex pattern `(^\\d*[.]?\\d+$)` to identify valid numeric values.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid decimal values.\n\n    Raises:\n        Exception: If invalid values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    decimal_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).str.contains(r\"(^-?\\d+$)\")\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        decimal_dqm_df = decimal_dqm_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(decimal_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(decimal_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return decimal_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return decimal_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return decimal_dqm_df\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.domain_dqm_check","title":"<code>domain_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate column values fall within a defined domain.</p> <p>Accepts comma-separated allowed values from <code>qc_param</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid domain values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def domain_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Validate column values fall within a defined domain.\n\n    Accepts comma-separated allowed values from `qc_param`.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid domain values.\n\n    Raises:\n        Exception: If invalid values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    domain_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).is_in(dqm_detail.qc_param.split(\",\"))\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        domain_dqm_df = domain_dqm_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(domain_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(domain_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return domain_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return domain_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return domain_dqm_df\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.custom_dqm_check","title":"<code>custom_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Apply custom SQL-like filter to perform flexible quality checks.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only records passing the custom filter.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def custom_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Apply custom SQL-like filter to perform flexible quality checks.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only records passing the custom filter.\n\n    Raises:\n        Exception: If invalid values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    custom_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).sql(dqm_detail.qc_param)\n    )\n\n    failure_count = total_count - len(custom_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(custom_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return custom_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.process_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return custom_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return custom_dqm_df\n</code></pre>"},{"location":"GoldLayerScripts/TransformationDataQualityCheck/#datacraft_framework.GoldLayerScripts.TransformationDataQualityCheck.TransformationDataQualityCheck.__init__","title":"<code>__init__(dqm_details, dataset_master)</code>","text":"<p>Initialize and execute data quality management (DQM) checks on transformation files.</p> <p>This constructor: - Retrieves unprocessed transformation files for DQM validation - Applies each DQM rule defined in <code>dqm_details</code> - Filters, transforms, and overwrites active valid data to the publish layer using Delta Lake - Logs results into <code>logDqmDtl</code> table via OrchestrationProcess</p> <p>If no DQM rules are provided, it directly casts schema and writes data without validation.</p> <p>Parameters:</p> Name Type Description Default <code>dqm_details</code> <code>List[ctlDqmMasterDtl]</code> <p>A list of DQM rule definitions. Each rule defines the type, condition, and criticality level of the check.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object containing configuration like paths, partitioning info, process ID, etc.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no unprocessed files are found or any critical DQM check fails.</p> Source code in <code>src/datacraft_framework/GoldLayerScripts/TransformationDataQualityCheck.py</code> <pre><code>def __init__(\n    self,\n    dqm_details: list[ctlDqmMasterDtl],\n    dataset_master: ctlDatasetMaster,\n):\n    \"\"\"\n    Initialize and execute data quality management (DQM) checks on transformation files.\n\n    This constructor:\n    - Retrieves unprocessed transformation files for DQM validation\n    - Applies each DQM rule defined in `dqm_details`\n    - Filters, transforms, and overwrites active valid data to the publish layer using Delta Lake\n    - Logs results into `logDqmDtl` table via OrchestrationProcess\n\n    If no DQM rules are provided, it directly casts schema and writes data without validation.\n\n    Args:\n        dqm_details (List[ctlDqmMasterDtl]): A list of DQM rule definitions.\n            Each rule defines the type, condition, and criticality level of the check.\n        dataset_master (ctlDatasetMaster): Dataset metadata object containing configuration like paths,\n            partitioning info, process ID, etc.\n\n    Raises:\n        Exception: If no unprocessed files are found or any critical DQM check fails.\n    \"\"\"\n\n    with OrchestrationProcess() as orch_process:\n        unprocessed_files = orch_process.get_transformation_dqm_unprocessed_files(\n            process_id=dataset_master.process_id,\n            dataset_id=dataset_master.dataset_id,\n        )\n\n        ctl_column_metadata = orch_process.get_ctl_column_metadata(\n            dataset_id=dataset_master.dataset_id\n        )\n\n        if len(unprocessed_files) == 0:\n            raise Exception(\n                f\"No unprocess files found for Data Quality Checks for dataset id: {dataset_master.dataset_id}\"\n            )\n\n        for unprocessed_file in unprocessed_files:\n            transformation_path = path_to_s3(\n                location=dataset_master.transformation_location,\n                env=env,\n            )\n\n            publish_dqm_path = path_to_s3(\n                location=dataset_master.publish_location, env=env\n            )\n\n            if len(dqm_details) != 0:\n                # First read data standard data.\n                original_df = DeltaTableRead(\n                    delta_path=transformation_path[\"s3_location\"],\n                ).read()\n\n                dqm_check_df = original_df\n\n                for dqm_detail in dqm_details:\n                    if dqm_detail.qc_type.lower() == \"null\":\n                        dqm_check_df = self.null_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"unique\":\n                        dqm_check_df = self.unique_dqm(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"decimal\":\n                        dqm_check_df = self.decimal_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"integer\":\n                        dqm_check_df = self.integer_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"length\":\n                        dqm_check_df = self.length_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"date\":\n                        dqm_check_df = self.date_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"domain\":\n                        dqm_check_df = self.domain_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"custom\":\n                        dqm_check_df = self.custom_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                schame_casted_df = SchemaCaster(\n                    df=dqm_check_df.filter(polars.col(\"sys_del_flg\") == \"N\"),\n                    column_metadata=ctl_column_metadata,\n                ).start()\n\n                DeltaTablePublishWrite(\n                    input_data=schame_casted_df,\n                    save_location=publish_dqm_path[\"s3_location\"],\n                    partition_columns=dataset_master.publish_partition_columns,\n                    batch_id=unprocessed_file.batch_id,\n                )\n\n            else:\n                start_time = datetime.now()\n                original_df = DeltaTableRead(\n                    delta_path=transformation_path[\"s3_location\"],\n                ).read()\n                schame_casted_df = SchemaCaster(\n                    df=original_df.filter(polars.col(\"sys_del_flg\") == \"N\"),\n                    column_metadata=ctl_column_metadata,\n                ).start()\n                DeltaTablePublishWrite(\n                    input_data=schame_casted_df,\n                    save_location=publish_dqm_path[\"s3_location\"],\n                    partition_columns=dataset_master.publish_partition_columns,\n                    batch_id=unprocessed_file.batch_id,\n                )\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=unprocessed_file.batch_id,\n                        source_file=unprocessed_file.source_file,\n                        column_name=None,\n                        qc_type=None,\n                        qc_param=None,\n                        qc_filter=None,\n                        criticality=None,\n                        criticality_threshold_pct=None,\n                        error_count=0,\n                        error_pct=0,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n</code></pre>"},{"location":"MedallionProcess/BronzeLayer/","title":"Documentation for <code>BronzeLayer</code>","text":""},{"location":"MedallionProcess/BronzeLayer/#datacraft_framework.MedallionProcess.BronzeLayer.BronzeLayer","title":"<code>datacraft_framework.MedallionProcess.BronzeLayer.BronzeLayer</code>","text":"<p>A class representing the Bronze Layer data ingestion process.</p> <p>This class orchestrates copying raw data from external systems (e.g., API, SFTP, S3, Database) into the framework\u2019s inbound location and creates a Delta Lake table in the Bronze layer.</p> <p>Attributes:</p> Name Type Description <code>process_id</code> <code>int</code> <p>The ID of the current orchestration process.</p> <code>bronze_datasets</code> <code>list[ctlDataAcquisitionDetail]</code> <p>List of dataset configurations for ingestion.</p> <code>bronze_dataset_masters</code> <code>list[ctlDatasetMaster]</code> <p>List of metadata objects describing datasets.</p> Source code in <code>src/datacraft_framework/MedallionProcess/BronzeLayer.py</code> <pre><code>class BronzeLayer:\n    \"\"\"\n    A class representing the Bronze Layer data ingestion process.\n\n    This class orchestrates copying raw data from external systems (e.g., API, SFTP, S3, Database)\n    into the framework\u2019s inbound location and creates a Delta Lake table in the Bronze layer.\n\n    Attributes:\n        process_id (int): The ID of the current orchestration process.\n        bronze_datasets (list[ctlDataAcquisitionDetail]): List of dataset configurations for ingestion.\n        bronze_dataset_masters (list[ctlDatasetMaster]): List of metadata objects describing datasets.\n    \"\"\"\n\n    def __init__(self, process_id):\n        \"\"\"\n        Initialize the BronzeLayer with the given process ID.\n\n        Retrieves acquisition and dataset metadata for this process.\n\n        Args:\n            process_id (int): The unique identifier of the orchestration process.\n        \"\"\"\n        self.process_id = process_id\n        LoggerManager(process_id=process_id)\n\n        with OrchestrationProcess() as orch_process:\n            self.bronze_datasets = orch_process.get_ctl_data_acquisition_detail(\n                process_id=process_id\n            )\n\n            self.bronze_dataset_masters = orch_process.get_dataset_master(\n                process_id=process_id,\n                dataset_type=\"BRONZE\",\n            )\n\n    def _handle_extraction(self, dataAcquisitionDetail: ctlDataAcquisitionDetail):\n        \"\"\"\n        Handle extraction of data from an external source to the inbound storage.\n\n        This method determines the correct extractor based on the source platform and executes it.\n        Supported platforms include: `SFTP`, `S3`, `DATABASE`, `SALESFORCE`, `VEEVA`, and `API`.\n\n        Args:\n            dataAcquisitionDetail (ctlDataAcquisitionDetail): Configuration object containing connection\n                and ingestion details for a specific dataset.\n\n        Raises:\n            Exception: If any error occurs during extraction; re-raised after logging.\n        \"\"\"\n\n        with OrchestrationProcess() as orch_process:\n            pre_ingestion_logs = orch_process.get_log_data_acquisition_detail(\n                process_id=dataAcquisitionDetail.process_id,\n                dataset_id=dataAcquisitionDetail.pre_ingestion_dataset_id,\n                status=\"SUCCEEDED\",\n            )\n\n            logger.info(\n                f\"Performaing Data Extraction for Dataset ID: {dataAcquisitionDetail.pre_ingestion_dataset_id} from Source: {dataAcquisitionDetail.outbound_source_platform}\"\n            )\n            logger.debug(\n                f\"dataAcquisitionDetail: {dataAcquisitionDetail.model_dump()}\",\n            )\n\n            if dataAcquisitionDetail.outbound_source_platform != \"API\":\n                connection_dtl = orch_process.get_ctl_data_acquisition_connection_master(\n                    outbound_source_platform=dataAcquisitionDetail.outbound_source_platform,\n                    outbound_source_system=dataAcquisitionDetail.outbound_source_system,\n                )\n                if dataAcquisitionDetail.outbound_source_platform == \"SFTP\":\n\n                    SftpExtractor.SftpExtractor(\n                        data_acquisition_connection_master=connection_dtl,\n                        pre_ingestion_logs=pre_ingestion_logs,\n                        data_acquisition_detail=dataAcquisitionDetail,\n                        orch_process=orch_process,\n                    )\n\n                if dataAcquisitionDetail.outbound_source_platform == \"S3\":\n\n                    S3Extractor.S3Extractor(\n                        data_acquisition_connection_master=connection_dtl,\n                        pre_ingestion_logs=pre_ingestion_logs,\n                        data_acquisition_detail=dataAcquisitionDetail,\n                        orch_process=orch_process,\n                    )\n\n                if dataAcquisitionDetail.outbound_source_platform == \"DATABASE\":\n\n                    DatabaseExtractor.DatabaseExtractor(\n                        data_acquisition_connection_master=connection_dtl,\n                        pre_ingestion_logs=pre_ingestion_logs,\n                        data_acquisition_detail=dataAcquisitionDetail,\n                        orch_process=orch_process,\n                    )\n\n                if (\n                    dataAcquisitionDetail.outbound_source_platform == \"VEEVA\"\n                    or dataAcquisitionDetail.outbound_source_platform == \"SALESFORCE\"\n                ):\n\n                    SalesforceExtractor.SalesforceExtractor(\n                        data_acquisition_connection_master=connection_dtl,\n                        pre_ingestion_logs=pre_ingestion_logs,\n                        data_acquisition_detail=dataAcquisitionDetail,\n                        orch_process=orch_process,\n                    )\n\n            elif dataAcquisitionDetail.outbound_source_platform == \"API\":\n\n                ApiExtractor.APIExtractor(\n                    api_connection_dtls=orch_process.get_ctl_api_connection_details(\n                        dataset_id=dataAcquisitionDetail.pre_ingestion_dataset_id\n                    ),\n                    column_meta_data=orch_process.get_ctl_column_metadata(\n                        dataset_id=dataAcquisitionDetail.pre_ingestion_dataset_id\n                    ),\n                    pre_ingestion_logs=pre_ingestion_logs,\n                    data_acquisition_detail=dataAcquisitionDetail,\n                    orch_process=orch_process,\n                )\n\n            logger.info(\n                f\"Performaing Data Extraction for Dataset ID: {dataAcquisitionDetail.pre_ingestion_dataset_id} from Source: {dataAcquisitionDetail.outbound_source_platform} Completed.\"\n            )\n\n    def _handle_raw_table_creation(self, dataset: ctlDatasetMaster):\n        \"\"\"\n        Create a raw Delta table in the Bronze layer for files that have been ingested.\n\n        Validates file patterns, filters out already processed files, and writes new ones to the landing zone.\n\n        Args:\n            dataset (ctlDatasetMaster): Dataset configuration including paths and partitioning info.\n\n        Raises:\n            Exception: If no new unprocessed files are found matching the pattern.\n        \"\"\"\n        logger.info(\n            f\"Creating landing delta table for Dataset ID: {dataset.dataset_id}\"\n        )\n        with OrchestrationProcess() as orch_process:\n            ingestion_logs = orch_process.get_log_raw_process_dtl(\n                process_id=dataset.process_id,\n                dataset_id=dataset.dataset_id,\n                status=\"SUCCEEDED\",\n            )\n\n            inbound_path = path_to_s3(location=dataset.inbound_location, env=env)\n            landing_path = path_to_s3(location=dataset.landing_location, env=env)\n            file_pattern = dataset.inbound_file_pattern\n\n            files_in_inbound = S3Process().s3_list_files(\n                bucket=inbound_path[\"bucket\"],\n                file_name=inbound_path[\"key\"],\n            )\n            files_in_inbound = [\n                f\"s3a://{inbound_path[\"bucket\"]}/{x}\" for x in files_in_inbound\n            ]\n            raw_completed_files = [x.source_file for x in ingestion_logs]\n\n            new_files = set(files_in_inbound) - set(raw_completed_files)\n            new_files = [\n                x\n                for x in new_files\n                if validate_pattern(\n                    file_pattern=file_pattern,\n                    file_name=x.split(\"/\")[-1],\n                )\n            ]\n            if len(new_files) == 0:\n                logger.info(\n                    f\"No new files found for Dataset ID: {dataset.dataset_id} hence failing.\"\n                )\n                raise Exception(\"No new files found to create raw delta table.\")\n\n            for new_file in new_files:\n\n                batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n                start_time = datetime.now()\n                try:\n                    DeltaTableWriter(\n                        input_data=new_file,\n                        save_location=landing_path[\"s3_location\"],\n                        batch_id=batch_id,\n                        partition_columns=dataset.landing_partition_columns,\n                        outbound_file_delimiter=dataset.inbound_file_delimiter,\n                    )\n                    orch_process.insert_log_raw_process_detail(\n                        log_raw_process_dtl=logRawProcessDtl(\n                            batch_id=batch_id,\n                            process_id=self.process_id,\n                            dataset_id=dataset.dataset_id,\n                            source_file=new_file,\n                            landing_location=dataset.landing_location,\n                            file_status=\"SUCCEEDED\",\n                            exception_details=None,\n                            file_process_start_time=start_time,\n                            file_process_end_time=datetime.now(),\n                        )\n                    )\n                    logger.info(\n                        f\"Creating landing delta table for Dataset ID: {dataset.dataset_id} completed.\"\n                    )\n\n                except Exception as e:\n                    orch_process.insert_log_raw_process_detail(\n                        log_raw_process_dtl=logRawProcessDtl(\n                            process_id=self.process_id,\n                            dataset_id=dataset.dataset_id,\n                            source_file=new_file,\n                            landing_location=dataset.landing_location,\n                            file_status=\"FAILED\",\n                            exception_details=traceback.format_exc(),\n                            file_process_start_time=start_time,\n                            file_process_end_time=datetime.now(),\n                        )\n                    )\n                    logger.error(traceback.format_exc())\n\n                    raise\n\n    def start_extraction(self):\n        \"\"\"\n        Start the Bronze layer ingestion process.\n\n        Orchestrates parallel execution of:\n        - Extraction from various sources (`SFTP`, `S3`, `API`, etc.)\n        - Creation of Delta tables for newly arrived files\n\n        Uses `ThreadPoolExecutor` to run multiple extractions concurrently.\n        \"\"\"\n        # Source To Inbound Process\n        with ThreadPoolExecutor(\n            max_workers=min(int(getenv(\"max_threads\")), len(self.bronze_datasets))\n        ) as executor:\n            futures = [\n                executor.submit(self._handle_extraction, bronze_dataset)\n                for bronze_dataset in self.bronze_datasets\n            ]\n\n            for future in futures:\n                try:\n                    future.result()\n                except Exception as e:\n                    raise\n\n        # Raw Inbound To Landing Table\n        with ThreadPoolExecutor(\n            max_workers=min(\n                int(getenv(\"max_threads\")), len(self.bronze_dataset_masters)\n            )\n        ) as executor:\n            futures = [\n                executor.submit(self._handle_raw_table_creation, bronze_dataset_master)\n                for bronze_dataset_master in self.bronze_dataset_masters\n            ]\n\n            for future in futures:\n                try:\n                    future.result()\n                except Exception as e:\n                    raise\n</code></pre>"},{"location":"MedallionProcess/BronzeLayer/#datacraft_framework.MedallionProcess.BronzeLayer.BronzeLayer.__init__","title":"<code>__init__(process_id)</code>","text":"<p>Initialize the BronzeLayer with the given process ID.</p> <p>Retrieves acquisition and dataset metadata for this process.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The unique identifier of the orchestration process.</p> required Source code in <code>src/datacraft_framework/MedallionProcess/BronzeLayer.py</code> <pre><code>def __init__(self, process_id):\n    \"\"\"\n    Initialize the BronzeLayer with the given process ID.\n\n    Retrieves acquisition and dataset metadata for this process.\n\n    Args:\n        process_id (int): The unique identifier of the orchestration process.\n    \"\"\"\n    self.process_id = process_id\n    LoggerManager(process_id=process_id)\n\n    with OrchestrationProcess() as orch_process:\n        self.bronze_datasets = orch_process.get_ctl_data_acquisition_detail(\n            process_id=process_id\n        )\n\n        self.bronze_dataset_masters = orch_process.get_dataset_master(\n            process_id=process_id,\n            dataset_type=\"BRONZE\",\n        )\n</code></pre>"},{"location":"MedallionProcess/BronzeLayer/#datacraft_framework.MedallionProcess.BronzeLayer.BronzeLayer._handle_extraction","title":"<code>_handle_extraction(dataAcquisitionDetail)</code>","text":"<p>Handle extraction of data from an external source to the inbound storage.</p> <p>This method determines the correct extractor based on the source platform and executes it. Supported platforms include: <code>SFTP</code>, <code>S3</code>, <code>DATABASE</code>, <code>SALESFORCE</code>, <code>VEEVA</code>, and <code>API</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataAcquisitionDetail</code> <code>ctlDataAcquisitionDetail</code> <p>Configuration object containing connection and ingestion details for a specific dataset.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during extraction; re-raised after logging.</p> Source code in <code>src/datacraft_framework/MedallionProcess/BronzeLayer.py</code> <pre><code>def _handle_extraction(self, dataAcquisitionDetail: ctlDataAcquisitionDetail):\n    \"\"\"\n    Handle extraction of data from an external source to the inbound storage.\n\n    This method determines the correct extractor based on the source platform and executes it.\n    Supported platforms include: `SFTP`, `S3`, `DATABASE`, `SALESFORCE`, `VEEVA`, and `API`.\n\n    Args:\n        dataAcquisitionDetail (ctlDataAcquisitionDetail): Configuration object containing connection\n            and ingestion details for a specific dataset.\n\n    Raises:\n        Exception: If any error occurs during extraction; re-raised after logging.\n    \"\"\"\n\n    with OrchestrationProcess() as orch_process:\n        pre_ingestion_logs = orch_process.get_log_data_acquisition_detail(\n            process_id=dataAcquisitionDetail.process_id,\n            dataset_id=dataAcquisitionDetail.pre_ingestion_dataset_id,\n            status=\"SUCCEEDED\",\n        )\n\n        logger.info(\n            f\"Performaing Data Extraction for Dataset ID: {dataAcquisitionDetail.pre_ingestion_dataset_id} from Source: {dataAcquisitionDetail.outbound_source_platform}\"\n        )\n        logger.debug(\n            f\"dataAcquisitionDetail: {dataAcquisitionDetail.model_dump()}\",\n        )\n\n        if dataAcquisitionDetail.outbound_source_platform != \"API\":\n            connection_dtl = orch_process.get_ctl_data_acquisition_connection_master(\n                outbound_source_platform=dataAcquisitionDetail.outbound_source_platform,\n                outbound_source_system=dataAcquisitionDetail.outbound_source_system,\n            )\n            if dataAcquisitionDetail.outbound_source_platform == \"SFTP\":\n\n                SftpExtractor.SftpExtractor(\n                    data_acquisition_connection_master=connection_dtl,\n                    pre_ingestion_logs=pre_ingestion_logs,\n                    data_acquisition_detail=dataAcquisitionDetail,\n                    orch_process=orch_process,\n                )\n\n            if dataAcquisitionDetail.outbound_source_platform == \"S3\":\n\n                S3Extractor.S3Extractor(\n                    data_acquisition_connection_master=connection_dtl,\n                    pre_ingestion_logs=pre_ingestion_logs,\n                    data_acquisition_detail=dataAcquisitionDetail,\n                    orch_process=orch_process,\n                )\n\n            if dataAcquisitionDetail.outbound_source_platform == \"DATABASE\":\n\n                DatabaseExtractor.DatabaseExtractor(\n                    data_acquisition_connection_master=connection_dtl,\n                    pre_ingestion_logs=pre_ingestion_logs,\n                    data_acquisition_detail=dataAcquisitionDetail,\n                    orch_process=orch_process,\n                )\n\n            if (\n                dataAcquisitionDetail.outbound_source_platform == \"VEEVA\"\n                or dataAcquisitionDetail.outbound_source_platform == \"SALESFORCE\"\n            ):\n\n                SalesforceExtractor.SalesforceExtractor(\n                    data_acquisition_connection_master=connection_dtl,\n                    pre_ingestion_logs=pre_ingestion_logs,\n                    data_acquisition_detail=dataAcquisitionDetail,\n                    orch_process=orch_process,\n                )\n\n        elif dataAcquisitionDetail.outbound_source_platform == \"API\":\n\n            ApiExtractor.APIExtractor(\n                api_connection_dtls=orch_process.get_ctl_api_connection_details(\n                    dataset_id=dataAcquisitionDetail.pre_ingestion_dataset_id\n                ),\n                column_meta_data=orch_process.get_ctl_column_metadata(\n                    dataset_id=dataAcquisitionDetail.pre_ingestion_dataset_id\n                ),\n                pre_ingestion_logs=pre_ingestion_logs,\n                data_acquisition_detail=dataAcquisitionDetail,\n                orch_process=orch_process,\n            )\n\n        logger.info(\n            f\"Performaing Data Extraction for Dataset ID: {dataAcquisitionDetail.pre_ingestion_dataset_id} from Source: {dataAcquisitionDetail.outbound_source_platform} Completed.\"\n        )\n</code></pre>"},{"location":"MedallionProcess/BronzeLayer/#datacraft_framework.MedallionProcess.BronzeLayer.BronzeLayer._handle_raw_table_creation","title":"<code>_handle_raw_table_creation(dataset)</code>","text":"<p>Create a raw Delta table in the Bronze layer for files that have been ingested.</p> <p>Validates file patterns, filters out already processed files, and writes new ones to the landing zone.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ctlDatasetMaster</code> <p>Dataset configuration including paths and partitioning info.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no new unprocessed files are found matching the pattern.</p> Source code in <code>src/datacraft_framework/MedallionProcess/BronzeLayer.py</code> <pre><code>def _handle_raw_table_creation(self, dataset: ctlDatasetMaster):\n    \"\"\"\n    Create a raw Delta table in the Bronze layer for files that have been ingested.\n\n    Validates file patterns, filters out already processed files, and writes new ones to the landing zone.\n\n    Args:\n        dataset (ctlDatasetMaster): Dataset configuration including paths and partitioning info.\n\n    Raises:\n        Exception: If no new unprocessed files are found matching the pattern.\n    \"\"\"\n    logger.info(\n        f\"Creating landing delta table for Dataset ID: {dataset.dataset_id}\"\n    )\n    with OrchestrationProcess() as orch_process:\n        ingestion_logs = orch_process.get_log_raw_process_dtl(\n            process_id=dataset.process_id,\n            dataset_id=dataset.dataset_id,\n            status=\"SUCCEEDED\",\n        )\n\n        inbound_path = path_to_s3(location=dataset.inbound_location, env=env)\n        landing_path = path_to_s3(location=dataset.landing_location, env=env)\n        file_pattern = dataset.inbound_file_pattern\n\n        files_in_inbound = S3Process().s3_list_files(\n            bucket=inbound_path[\"bucket\"],\n            file_name=inbound_path[\"key\"],\n        )\n        files_in_inbound = [\n            f\"s3a://{inbound_path[\"bucket\"]}/{x}\" for x in files_in_inbound\n        ]\n        raw_completed_files = [x.source_file for x in ingestion_logs]\n\n        new_files = set(files_in_inbound) - set(raw_completed_files)\n        new_files = [\n            x\n            for x in new_files\n            if validate_pattern(\n                file_pattern=file_pattern,\n                file_name=x.split(\"/\")[-1],\n            )\n        ]\n        if len(new_files) == 0:\n            logger.info(\n                f\"No new files found for Dataset ID: {dataset.dataset_id} hence failing.\"\n            )\n            raise Exception(\"No new files found to create raw delta table.\")\n\n        for new_file in new_files:\n\n            batch_id = int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-1])\n\n            start_time = datetime.now()\n            try:\n                DeltaTableWriter(\n                    input_data=new_file,\n                    save_location=landing_path[\"s3_location\"],\n                    batch_id=batch_id,\n                    partition_columns=dataset.landing_partition_columns,\n                    outbound_file_delimiter=dataset.inbound_file_delimiter,\n                )\n                orch_process.insert_log_raw_process_detail(\n                    log_raw_process_dtl=logRawProcessDtl(\n                        batch_id=batch_id,\n                        process_id=self.process_id,\n                        dataset_id=dataset.dataset_id,\n                        source_file=new_file,\n                        landing_location=dataset.landing_location,\n                        file_status=\"SUCCEEDED\",\n                        exception_details=None,\n                        file_process_start_time=start_time,\n                        file_process_end_time=datetime.now(),\n                    )\n                )\n                logger.info(\n                    f\"Creating landing delta table for Dataset ID: {dataset.dataset_id} completed.\"\n                )\n\n            except Exception as e:\n                orch_process.insert_log_raw_process_detail(\n                    log_raw_process_dtl=logRawProcessDtl(\n                        process_id=self.process_id,\n                        dataset_id=dataset.dataset_id,\n                        source_file=new_file,\n                        landing_location=dataset.landing_location,\n                        file_status=\"FAILED\",\n                        exception_details=traceback.format_exc(),\n                        file_process_start_time=start_time,\n                        file_process_end_time=datetime.now(),\n                    )\n                )\n                logger.error(traceback.format_exc())\n\n                raise\n</code></pre>"},{"location":"MedallionProcess/BronzeLayer/#datacraft_framework.MedallionProcess.BronzeLayer.BronzeLayer.start_extraction","title":"<code>start_extraction()</code>","text":"<p>Start the Bronze layer ingestion process.</p> <p>Orchestrates parallel execution of: - Extraction from various sources (<code>SFTP</code>, <code>S3</code>, <code>API</code>, etc.) - Creation of Delta tables for newly arrived files</p> <p>Uses <code>ThreadPoolExecutor</code> to run multiple extractions concurrently.</p> Source code in <code>src/datacraft_framework/MedallionProcess/BronzeLayer.py</code> <pre><code>def start_extraction(self):\n    \"\"\"\n    Start the Bronze layer ingestion process.\n\n    Orchestrates parallel execution of:\n    - Extraction from various sources (`SFTP`, `S3`, `API`, etc.)\n    - Creation of Delta tables for newly arrived files\n\n    Uses `ThreadPoolExecutor` to run multiple extractions concurrently.\n    \"\"\"\n    # Source To Inbound Process\n    with ThreadPoolExecutor(\n        max_workers=min(int(getenv(\"max_threads\")), len(self.bronze_datasets))\n    ) as executor:\n        futures = [\n            executor.submit(self._handle_extraction, bronze_dataset)\n            for bronze_dataset in self.bronze_datasets\n        ]\n\n        for future in futures:\n            try:\n                future.result()\n            except Exception as e:\n                raise\n\n    # Raw Inbound To Landing Table\n    with ThreadPoolExecutor(\n        max_workers=min(\n            int(getenv(\"max_threads\")), len(self.bronze_dataset_masters)\n        )\n    ) as executor:\n        futures = [\n            executor.submit(self._handle_raw_table_creation, bronze_dataset_master)\n            for bronze_dataset_master in self.bronze_dataset_masters\n        ]\n\n        for future in futures:\n            try:\n                future.result()\n            except Exception as e:\n                raise\n</code></pre>"},{"location":"MedallionProcess/GoldLayer/","title":"Documentation for <code>GoldLayer</code>","text":""},{"location":"MedallionProcess/GoldLayer/#datacraft_framework.MedallionProcess.GoldLayer.GoldLayer","title":"<code>datacraft_framework.MedallionProcess.GoldLayer.GoldLayer</code>","text":"<p>A class representing the Gold Layer data processing pipeline.</p> This class orchestrates two key stages of a dataset in the Gold layer <ul> <li>Transformation: Applies business logic like JOIN, UNION, or CUSTOM SQL to raw/transformed data.</li> <li>Data Quality Management (DQM): Validates transformed data against defined quality rules.</li> </ul> <p>The Gold Layer typically represents clean, curated, and structured data ready for analytics or downstream use.</p> <p>Attributes:</p> Name Type Description <code>process_id</code> <code>int</code> <p>The unique identifier of the orchestration process.</p> Source code in <code>src/datacraft_framework/MedallionProcess/GoldLayer.py</code> <pre><code>class GoldLayer:\n    \"\"\"\n    A class representing the Gold Layer data processing pipeline.\n\n    This class orchestrates two key stages of a dataset in the Gold layer:\n        - **Transformation**: Applies business logic like JOIN, UNION, or CUSTOM SQL to raw/transformed data.\n        -  **Data Quality Management (DQM)**: Validates transformed data against defined quality rules.\n\n    The Gold Layer typically represents clean, curated, and structured data ready for analytics or downstream use.\n\n    Attributes:\n        process_id (int): The unique identifier of the orchestration process.\n    \"\"\"\n\n    def _handle_gold_layer(self, dataset_master: ctlDatasetMaster):\n        \"\"\"\n        Process a single dataset through transformation and DQM into the Gold Layer.\n\n        Args:\n            dataset_master (ctlDatasetMaster): Dataset configuration object containing metadata,\n                paths, and partitioning info.\n        \"\"\"\n        logger.info(f\"Starting Gold Layer for Dataset ID: {dataset_master.dataset_id}\")\n        with OrchestrationProcess() as orch_process:\n            dqm_details = orch_process.get_dqm_detail(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n            )\n        logger.info(\n            f\"Starting Gold Layer Transformation for Dataset ID: {dataset_master.dataset_id}\"\n        )\n        Transformation(\n            dataset_master=dataset_master,\n        )\n        logger.info(\n            f\"Gold Layer Transformation for Dataset ID: {dataset_master.dataset_id} Completed.\"\n        )\n        logger.info(\n            f\"Starting Gold Layer Transformation DQM for Dataset ID: {dataset_master.dataset_id}\"\n        )\n        TransformationDataQualityCheck(\n            dqm_details=dqm_details,\n            dataset_master=dataset_master,\n        )\n        logger.info(\n            f\"Gold Layer Transformation DQM for Dataset ID: {dataset_master.dataset_id} Completed.\"\n        )\n\n    def __init__(self, process_id: int):\n        \"\"\"\n        Initialize and execute the Gold Layer pipeline for all datasets under the given process ID.\n\n        Orchestrates parallel execution using thread pools based on available workers.\n\n        Args:\n            process_id (int): The ID of the current process to execute.\n\n        Raises:\n            Exception: If any transformation or DQM step fails during execution.\n        \"\"\"\n        with OrchestrationProcess() as orch_process:\n            gold_datasets = orch_process.get_dataset_master(\n                process_id=process_id, dataset_type=\"GOLD\"\n            )\n\n            with ThreadPoolExecutor(\n                max_workers=min(int(getenv(\"max_threads\")), len(gold_datasets))\n            ) as executor:\n                futures = [\n                    executor.submit(self._handle_gold_layer, gold_dataset)\n                    for gold_dataset in gold_datasets\n                ]\n\n                for future in futures:\n                    try:\n                        future.result()\n                    except Exception as e:\n                        raise\n</code></pre>"},{"location":"MedallionProcess/GoldLayer/#datacraft_framework.MedallionProcess.GoldLayer.GoldLayer._handle_gold_layer","title":"<code>_handle_gold_layer(dataset_master)</code>","text":"<p>Process a single dataset through transformation and DQM into the Gold Layer.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset configuration object containing metadata, paths, and partitioning info.</p> required Source code in <code>src/datacraft_framework/MedallionProcess/GoldLayer.py</code> <pre><code>def _handle_gold_layer(self, dataset_master: ctlDatasetMaster):\n    \"\"\"\n    Process a single dataset through transformation and DQM into the Gold Layer.\n\n    Args:\n        dataset_master (ctlDatasetMaster): Dataset configuration object containing metadata,\n            paths, and partitioning info.\n    \"\"\"\n    logger.info(f\"Starting Gold Layer for Dataset ID: {dataset_master.dataset_id}\")\n    with OrchestrationProcess() as orch_process:\n        dqm_details = orch_process.get_dqm_detail(\n            process_id=dataset_master.process_id,\n            dataset_id=dataset_master.dataset_id,\n        )\n    logger.info(\n        f\"Starting Gold Layer Transformation for Dataset ID: {dataset_master.dataset_id}\"\n    )\n    Transformation(\n        dataset_master=dataset_master,\n    )\n    logger.info(\n        f\"Gold Layer Transformation for Dataset ID: {dataset_master.dataset_id} Completed.\"\n    )\n    logger.info(\n        f\"Starting Gold Layer Transformation DQM for Dataset ID: {dataset_master.dataset_id}\"\n    )\n    TransformationDataQualityCheck(\n        dqm_details=dqm_details,\n        dataset_master=dataset_master,\n    )\n    logger.info(\n        f\"Gold Layer Transformation DQM for Dataset ID: {dataset_master.dataset_id} Completed.\"\n    )\n</code></pre>"},{"location":"MedallionProcess/GoldLayer/#datacraft_framework.MedallionProcess.GoldLayer.GoldLayer.__init__","title":"<code>__init__(process_id)</code>","text":"<p>Initialize and execute the Gold Layer pipeline for all datasets under the given process ID.</p> <p>Orchestrates parallel execution using thread pools based on available workers.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The ID of the current process to execute.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If any transformation or DQM step fails during execution.</p> Source code in <code>src/datacraft_framework/MedallionProcess/GoldLayer.py</code> <pre><code>def __init__(self, process_id: int):\n    \"\"\"\n    Initialize and execute the Gold Layer pipeline for all datasets under the given process ID.\n\n    Orchestrates parallel execution using thread pools based on available workers.\n\n    Args:\n        process_id (int): The ID of the current process to execute.\n\n    Raises:\n        Exception: If any transformation or DQM step fails during execution.\n    \"\"\"\n    with OrchestrationProcess() as orch_process:\n        gold_datasets = orch_process.get_dataset_master(\n            process_id=process_id, dataset_type=\"GOLD\"\n        )\n\n        with ThreadPoolExecutor(\n            max_workers=min(int(getenv(\"max_threads\")), len(gold_datasets))\n        ) as executor:\n            futures = [\n                executor.submit(self._handle_gold_layer, gold_dataset)\n                for gold_dataset in gold_datasets\n            ]\n\n            for future in futures:\n                try:\n                    future.result()\n                except Exception as e:\n                    raise\n</code></pre>"},{"location":"MedallionProcess/SilverLayer/","title":"Documentation for <code>SilverLayer</code>","text":""},{"location":"MedallionProcess/SilverLayer/#datacraft_framework.MedallionProcess.SilverLayer.SilverLayer","title":"<code>datacraft_framework.MedallionProcess.SilverLayer.SilverLayer</code>","text":"<p>A class representing the Silver Layer data processing pipeline.</p> <p>The Silver Layer is responsible for:</p> <ul> <li>Data Standardization: Cleaning, transforming, and normalizing raw data.   Supported operations include padding, trimming, regex replacement, type conversion, etc.</li> <li>Data Quality Management (DQM): Validating standardized data against defined rules.</li> </ul> <p>This class orchestrates parallel execution of standardization and DQM processes across multiple datasets using thread pools.</p> <p>Attributes:</p> Name Type Description <code>process_id</code> <code>int</code> <p>The unique identifier of the orchestration process.</p> <code>datasets</code> <code>List[ctlDatasetMaster]</code> <p>List of dataset metadata objects for Silver layer processing.</p> Source code in <code>src/datacraft_framework/MedallionProcess/SilverLayer.py</code> <pre><code>class SilverLayer:\n    \"\"\"\n    A class representing the Silver Layer data processing pipeline.\n\n    The Silver Layer is responsible for:\n\n    - **Data Standardization**: Cleaning, transforming, and normalizing raw data.\n      Supported operations include padding, trimming, regex replacement, type conversion, etc.\n    - **Data Quality Management (DQM)**: Validating standardized data against defined rules.\n\n    This class orchestrates parallel execution of standardization and DQM processes\n    across multiple datasets using thread pools.\n\n    Attributes:\n        process_id (int): The unique identifier of the orchestration process.\n        datasets (List[ctlDatasetMaster]): List of dataset metadata objects for Silver layer processing.\n    \"\"\"\n\n    def _handle_silver_process(self, dataset_master: ctlDatasetMaster):\n        \"\"\"\n        Handle the Silver Layer process for a single dataset.\n\n        Orchestrates:\n\n            - Retrieval of data standardization and DQM rules from control tables.\n            - Execution of `DataStandardization` and `DataQualityCheck`.\n\n        Args:\n            dataset_master (ctlDatasetMaster): Dataset configuration object containing paths,\n                process ID, and transformation logic.\n\n        Raises:\n            Exception: If any error occurs during standardization or DQM steps.\n        \"\"\"\n        with OrchestrationProcess() as orch_process:\n            data_standard = orch_process.get_data_standard_dtl(\n                dataset_id=dataset_master.dataset_id\n            )\n            dqm_details = orch_process.get_dqm_detail(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n            )\n\n        logger.info(\n            f\"Started Silver Layer for Dataset ID: {dataset_master.dataset_id}.\"\n        )\n        logger.info(\n            f\"\\tPerforming Data DataStandardization for Dataset ID: {dataset_master.dataset_id}.\"\n        )\n        DataStandardization(\n            data_standard_detail=data_standard,\n            dataset_master=dataset_master,\n        )\n        logger.info(\n            f\"Completed Data DataStandardization for Dataset ID: {dataset_master.dataset_id}.\"\n        )\n\n        logger.info(\n            f\"\\tPerforming Data Quality Checks for Dataset ID: {dataset_master.dataset_id}.\"\n        )\n        DataQualityCheck(dqm_details=dqm_details, dataset_master=dataset_master)\n        logger.info(\n            f\"Completed Data DataStandardization for Dataset ID: {dataset_master.dataset_id}.\"\n        )\n\n    def __init__(self, process_id):\n        \"\"\"\n        Initialize and execute the Silver Layer pipeline for all datasets under the given process ID.\n\n        Retrieves Bronze layer datasets and spawns threads to handle standardization and DQM in parallel.\n\n        Args:\n            process_id (int): The ID of the current orchestration process.\n\n        Raises:\n            Exception: If no datasets are found for the given process ID or if any thread raises an exception.\n        \"\"\"\n        self.process_id = process_id\n\n        with OrchestrationProcess() as orch_process:\n\n            self.datasets = orch_process.get_dataset_master(\n                process_id=process_id,\n                dataset_type=\"BRONZE\",\n            )\n\n        with ThreadPoolExecutor(\n            max_workers=min(int(getenv(\"max_threads\")), len(self.datasets))\n        ) as executor:\n            futures = [\n                executor.submit(self._handle_silver_process, silver_dataset)\n                for silver_dataset in self.datasets\n            ]\n\n            for future in futures:\n                try:\n                    future.result()\n                except Exception as e:\n                    raise\n</code></pre>"},{"location":"MedallionProcess/SilverLayer/#datacraft_framework.MedallionProcess.SilverLayer.SilverLayer._handle_silver_process","title":"<code>_handle_silver_process(dataset_master)</code>","text":"<p>Handle the Silver Layer process for a single dataset.</p> <p>Orchestrates:</p> <pre><code>- Retrieval of data standardization and DQM rules from control tables.\n- Execution of `DataStandardization` and `DataQualityCheck`.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset configuration object containing paths, process ID, and transformation logic.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during standardization or DQM steps.</p> Source code in <code>src/datacraft_framework/MedallionProcess/SilverLayer.py</code> <pre><code>def _handle_silver_process(self, dataset_master: ctlDatasetMaster):\n    \"\"\"\n    Handle the Silver Layer process for a single dataset.\n\n    Orchestrates:\n\n        - Retrieval of data standardization and DQM rules from control tables.\n        - Execution of `DataStandardization` and `DataQualityCheck`.\n\n    Args:\n        dataset_master (ctlDatasetMaster): Dataset configuration object containing paths,\n            process ID, and transformation logic.\n\n    Raises:\n        Exception: If any error occurs during standardization or DQM steps.\n    \"\"\"\n    with OrchestrationProcess() as orch_process:\n        data_standard = orch_process.get_data_standard_dtl(\n            dataset_id=dataset_master.dataset_id\n        )\n        dqm_details = orch_process.get_dqm_detail(\n            process_id=dataset_master.process_id,\n            dataset_id=dataset_master.dataset_id,\n        )\n\n    logger.info(\n        f\"Started Silver Layer for Dataset ID: {dataset_master.dataset_id}.\"\n    )\n    logger.info(\n        f\"\\tPerforming Data DataStandardization for Dataset ID: {dataset_master.dataset_id}.\"\n    )\n    DataStandardization(\n        data_standard_detail=data_standard,\n        dataset_master=dataset_master,\n    )\n    logger.info(\n        f\"Completed Data DataStandardization for Dataset ID: {dataset_master.dataset_id}.\"\n    )\n\n    logger.info(\n        f\"\\tPerforming Data Quality Checks for Dataset ID: {dataset_master.dataset_id}.\"\n    )\n    DataQualityCheck(dqm_details=dqm_details, dataset_master=dataset_master)\n    logger.info(\n        f\"Completed Data DataStandardization for Dataset ID: {dataset_master.dataset_id}.\"\n    )\n</code></pre>"},{"location":"MedallionProcess/SilverLayer/#datacraft_framework.MedallionProcess.SilverLayer.SilverLayer.__init__","title":"<code>__init__(process_id)</code>","text":"<p>Initialize and execute the Silver Layer pipeline for all datasets under the given process ID.</p> <p>Retrieves Bronze layer datasets and spawns threads to handle standardization and DQM in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The ID of the current orchestration process.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no datasets are found for the given process ID or if any thread raises an exception.</p> Source code in <code>src/datacraft_framework/MedallionProcess/SilverLayer.py</code> <pre><code>def __init__(self, process_id):\n    \"\"\"\n    Initialize and execute the Silver Layer pipeline for all datasets under the given process ID.\n\n    Retrieves Bronze layer datasets and spawns threads to handle standardization and DQM in parallel.\n\n    Args:\n        process_id (int): The ID of the current orchestration process.\n\n    Raises:\n        Exception: If no datasets are found for the given process ID or if any thread raises an exception.\n    \"\"\"\n    self.process_id = process_id\n\n    with OrchestrationProcess() as orch_process:\n\n        self.datasets = orch_process.get_dataset_master(\n            process_id=process_id,\n            dataset_type=\"BRONZE\",\n        )\n\n    with ThreadPoolExecutor(\n        max_workers=min(int(getenv(\"max_threads\")), len(self.datasets))\n    ) as executor:\n        futures = [\n            executor.submit(self._handle_silver_process, silver_dataset)\n            for silver_dataset in self.datasets\n        ]\n\n        for future in futures:\n            try:\n                future.result()\n            except Exception as e:\n                raise\n</code></pre>"},{"location":"Models/Schema/","title":"Documentation for <code>Schema</code>","text":"<p>Here you can find the documentation for Datacraft Framework's Configuration Tables.</p> Fields Field Name Type Description Primary Key <code>outbound_source_platform</code> <code>str</code> The platform in which the source file is stored. * <code>outbound_source_system</code> <code>str</code> The unique identifier for the source credentials. * <code>connection_config</code> <code>Optional[str]</code> JSON containing the connection details like hostname, port, etc. <code>ssh_private_key</code> <code>Optional[str]</code> Private key file contents for SFTP connection authentication. Fields Field Name Type Description Primary Key <code>seq_no</code> <code>int</code> Auto-incremented sequence number. * <code>pre_ingestion_dataset_id</code> <code>Optional[int]</code> The dataset ID for RAW/Bronze layer associated with this API call. <code>outbound_source_system</code> <code>Optional[str]</code> The unique identifier for the source credentials used for the API. <code>type</code> <code>Optional[str]</code> The API request type. Expected values: ['TOKEN', 'RESPONSE', 'CUSTOM'] <code>token_url</code> <code>Optional[str]</code> URL to fetch a token if type == 'TOKEN'. <code>auth_type</code> <code>Optional[str]</code> Type of authentication used. Supported types: OAuth, JWT, Basic Auth, CUSTOM. <code>token_type</code> <code>Optional[str]</code> Type of token used (e.g., Bearer). <code>client_id</code> <code>Optional[str]</code> Client ID used to request a token. <code>client_secret</code> <code>Optional[str]</code> Client secret used to request a token. <code>username</code> <code>Optional[str]</code> Username used for basic authentication. <code>password</code> <code>Optional[str]</code> Password used for basic authentication. <code>issuer</code> <code>Optional[str]</code> Issuer required for JWT or service account token requests. <code>scope</code> <code>Optional[str]</code> Scope required for JWT or service account token requests. <code>private_key</code> <code>Optional[str]</code> Private key required for JWT or service account token requests. <code>token_path</code> <code>Optional[str]</code> JSON path where the token exists in the response body. <code>method</code> <code>Optional[str]</code> HTTP method used for the API request (e.g., GET, POST). <code>url</code> <code>Optional[str]</code> API endpoint URL to send the request to. <code>headers</code> <code>Optional[str]</code> HTTP headers to be sent with the request as JSON string. <code>params</code> <code>Optional[str]</code> Query parameters to be appended to the request URL. <code>data</code> <code>Optional[str]</code> Raw body content sent when making an API request. <code>json_body</code> <code>Optional[str]</code> JSON-formatted body sent when making an API request. <code>body_values</code> <code>Optional[str]</code> Placeholders in the request body that need dynamic replacement. Fields Field Name Type Description Primary Key <code>process_id</code> <code>int</code> The unique ID representing a specific data acquisition process. * <code>pre_ingestion_dataset_id</code> <code>int</code> The Bronze Layer dataset ID. Use Salesforce object name if source platform is Salesforce. * <code>pre_ingestion_dataset_name</code> <code>Optional[str]</code> The Bronze Layer dataset name. <code>outbound_source_platform</code> <code>Optional[str]</code> Platform where the source file is stored (e.g., SFTP, DB, Salesforce). <code>outbound_source_system</code> <code>Optional[str]</code> Unique identifier for the source system credentials. <code>outbound_source_location</code> <code>Optional[str]</code> Location where the source file is stored (e.g., directory path or database schema). <code>outbound_source_file_pattern_static</code> <code>Optional[str]</code> Flag indicating whether the source filename is static ('Y' or 'N'). <code>outbound_source_file_pattern</code> <code>Optional[str]</code> File name pattern of the source file. Supports YYYY-MM-DD and regex patterns. <code>outbound_source_file_format</code> <code>Optional[str]</code> Format of the source file (e.g., CSV, JSON, XML). <code>outbound_file_delimiter</code> <code>Optional[str]</code> Delimiter used in the source file (e.g., comma, tab). <code>query</code> <code>Optional[str]</code> SQL query to fetch data from a database. Use only when source platform is a database. <code>columns</code> <code>Optional[str]</code> Comma-separated list of columns to select from Salesforce. Use only when source platform is Salesforce. <code>inbound_location</code> <code>Optional[str]</code> Destination location where the acquired data will be saved. Fields Field Name Type Description Primary Key <code>seq_no</code> <code>int</code> Auto-incremented sequence number. * <code>batch_id</code> <code>Optional[int]</code> Batch ID of the running process. <code>run_date</code> <code>Optional[date]</code> Date on which the process was executed. <code>process_id</code> <code>Optional[int]</code> Unique ID of the process. <code>pre_ingestion_dataset_id</code> <code>Optional[int]</code> Dataset ID of the Bronze Layer. <code>outbound_source_location</code> <code>Optional[str]</code> Source file location. <code>inbound_file_location</code> <code>Optional[str]</code> Inbound file storage location. <code>status</code> <code>Optional[str]</code> Current status of the BRONZE dataset process. <code>exception_details</code> <code>Optional[str]</code> Details of any exceptions encountered during processing. <code>start_time</code> <code>Optional[datetime]</code> Start time of the process. <code>end_time</code> <code>Optional[datetime]</code> End time of the process. Fields Field Name Type Description Primary Key <code>file_id</code> <code>Optional[int]</code> Unique identifier for the processed file. * <code>run_date</code> <code>Optional[date]</code> Date when the file was processed. <code>batch_id</code> <code>Optional[int]</code> Batch ID associated with the file processing. <code>process_id</code> <code>Optional[int]</code> ID of the process responsible for file ingestion. <code>dataset_id</code> <code>Optional[int]</code> ID of the dataset being processed. <code>source_file</code> <code>Optional[str]</code> Name of the source file being processed. <code>landing_location</code> <code>Optional[str]</code> Path where the file was saved in Landing Layer. <code>file_status</code> <code>Optional[str]</code> Status of the file processing. <code>exception_details</code> <code>Optional[str]</code> Error message if file processing failed. <code>file_process_start_time</code> <code>Optional[datetime]</code> Start time of file processing. <code>file_process_end_time</code> <code>Optional[datetime]</code> End time of file processing. Fields Field Name Type Description Primary Key <code>process_id</code> <code>int</code> Unique identifier for the data pipeline process. * <code>dataset_id</code> <code>int</code> Unique identifier for the dataset. * <code>dataset_name</code> <code>Optional[str]</code> Name of the dataset. <code>dataset_type</code> <code>Optional[str]</code> Type of dataset (e.g., RAW, Bronze, Silver, Gold). <code>provider</code> <code>Optional[str]</code> Source system or provider of the dataset. <code>subject_area</code> <code>Optional[str]</code> Business domain or subject area this dataset belongs to. <code>inbound_location</code> <code>Optional[str]</code> File system path where raw data is initially received. <code>inbound_file_pattern</code> <code>Optional[str]</code> Pattern used to match incoming files (supports date patterns and regex). <code>inbound_static_file_pattern</code> <code>Optional[str]</code> Flag indicating if the inbound filename is static ('Y'/'N'). <code>inbound_file_format</code> <code>Optional[str]</code> Format of the source file (e.g., CSV, JSON, XML). <code>inbound_file_delimiter</code> <code>Optional[str]</code> Delimiter used in flat files (e.g., comma, tab). <code>header_available_flag</code> <code>Optional[str]</code> Indicates whether the file has a header row ('Y'/'N'). <code>file_number_of_fields</code> <code>Optional[int]</code> Expected number of columns in the source file. <code>landing_location</code> <code>Optional[str]</code> Path where data is stored after landing (RAW/Landing Layer). <code>landing_table</code> <code>Optional[str]</code> Target table name for structured storage in the Landing Layer. <code>landing_partition_columns</code> <code>Optional[str]</code> Partitioning strategy for the Landing Layer table. <code>data_standardisation_location</code> <code>Optional[str]</code> Path where standardized version of the dataset is stored. <code>data_standardisation_table</code> <code>Optional[str]</code> Table name for the standardized dataset. <code>data_standardisation_partition_columns</code> <code>Optional[str]</code> Partitioning strategy for the Standardized Layer. <code>dqm_location</code> <code>Optional[str]</code> Location where data quality checked version is stored. <code>dqm_table</code> <code>Optional[str]</code> Table name for the Data Quality Management (DQM) layer. <code>dqm_partition_columns</code> <code>Optional[str]</code> Partitioning strategy for the DQM Layer. <code>staging_location</code> <code>Optional[str]</code> Path where data is stored before transformation into final format. <code>staging_table</code> <code>Optional[str]</code> Table name for the Staging Layer. <code>staging_partition_columns</code> <code>Optional[str]</code> Partitioning strategy for the Staging Layer. <code>transformation_location</code> <code>Optional[str]</code> Path where transformed dataset is stored. <code>transformation_table</code> <code>Optional[str]</code> Table name for the Transformation Layer. <code>transformation_partition_columns</code> <code>Optional[str]</code> Partitioning strategy for the Transformation Layer. <code>archive_location</code> <code>Optional[str]</code> Archive path for historical versions of the dataset. <code>publish_location</code> <code>Optional[str]</code> Final destination where the dataset is published for consumption. <code>publish_table</code> <code>Optional[str]</code> Table name for the Published Layer. <code>publish_partition_columns</code> <code>Optional[str]</code> Partitioning strategy for the Published Layer. Fields Field Name Type Description Primary Key <code>column_id</code> <code>Optional[int]</code> Unique identifier for the column. * <code>table_name</code> <code>str</code> Name of the table for which the column metadata is defined. * <code>dataset_id</code> <code>int</code> ID of the dataset this column belongs to. * <code>column_name</code> <code>str</code> Name of the column. * <code>column_data_type</code> <code>Optional[str]</code> Data type of the column (e.g., string, integer, float, date). <code>column_date_format</code> <code>Optional[str]</code> Date format if column_data_type is Date (e.g., 'yyyy-MM-dd'). <code>column_description</code> <code>Optional[str]</code> Description explaining the meaning or business context of the column. <code>column_json_mapping</code> <code>Optional[str]</code> JSON path mapping used to extract value from raw JSON input. <code>source_column_name</code> <code>Optional[str]</code> Original column name from the source dataset. <code>column_sequence_number</code> <code>Optional[int]</code> Order of the column within the table structure. <code>column_tag</code> <code>Optional[str]</code> Tags for column usage in dashboards (e.g., KPI, KPI-Filter). Fields Field Name Type Description Primary Key <code>dataset_id</code> <code>int</code> ID of the dataset being standardized. * <code>column_name</code> <code>str</code> Name of the column to which the standardization rule applies. * <code>function_name</code> <code>Optional[str]</code> Name of the function used for standardizing the column (e.g., trim, cast, parse_date). <code>function_params</code> <code>Optional[str]</code> Parameters passed to the function (as JSON or string). Fields Field Name Type Description Primary Key <code>qc_id</code> <code>int</code> Unique ID for the data quality check. * <code>process_id</code> <code>Optional[int]</code> ID of the process associated with the QC rule. <code>dataset_id</code> <code>Optional[int]</code> ID of the dataset being validated. <code>column_name</code> <code>Optional[str]</code> Name of the column being validated. <code>qc_type</code> <code>Optional[str]</code> Type of quality check (e.g., not_null, unique, range, regex). <code>qc_param</code> <code>Optional[str]</code> Parameters for the QC rule (e.g., min/max values, regex pattern). <code>active_flag</code> <code>Optional[str]</code> Whether the QC rule is active ('Y'/'N'). <code>qc_filter</code> <code>Optional[str]</code> Optional filter condition for applying the QC rule. <code>criticality</code> <code>Optional[str]</code> Severity level of the QC failure (e.g., HIGH, MEDIUM, LOW). <code>criticality_threshold_pct</code> <code>Optional[int]</code> Threshold percentage for criticality violation (e.g., 5% nulls allowed). Fields Field Name Type Description Primary Key <code>seq_no</code> <code>Optional[int]</code> Auto-incremented sequence number for logging entries. * <code>batch_id</code> <code>Optional[int]</code> Batch ID associated with the standardization process. <code>process_id</code> <code>Optional[int]</code> ID of the data pipeline process involved. <code>dataset_id</code> <code>Optional[int]</code> ID of the dataset undergoing standardization. <code>source_file</code> <code>Optional[str]</code> Name of the source file processed during standardization. <code>data_standardisation_location</code> <code>Optional[str]</code> Path where standardized output was written. <code>status</code> <code>Optional[str]</code> Current status of the standardization process (e.g., SUCCESS, FAILED). <code>exception_details</code> <code>Optional[str]</code> Details of any exceptions encountered during processing. <code>start_datetime</code> <code>Optional[datetime]</code> Start time of the standardization process. <code>end_datetime</code> <code>Optional[datetime]</code> End time of the standardization process. Fields Field Name Type Description Primary Key <code>seq_no</code> <code>Optional[int]</code> Auto-incremented sequence number for logging entries. * <code>process_id</code> <code>Optional[int]</code> ID of the data pipeline process involved. <code>dataset_id</code> <code>Optional[int]</code> ID of the dataset being validated. <code>batch_id</code> <code>Optional[int]</code> Batch ID associated with the DQM run. <code>source_file</code> <code>Optional[str]</code> Name of the source file processed during DQM validation. <code>column_name</code> <code>Optional[str]</code> Name of the column being validated. <code>qc_type</code> <code>Optional[str]</code> Type of quality check performed (e.g., not_null, unique). <code>qc_param</code> <code>Optional[str]</code> Parameters used for the QC rule (e.g., regex, value ranges). <code>qc_filter</code> <code>Optional[str]</code> Filter condition applied during QC validation. <code>criticality</code> <code>Optional[str]</code> Severity level of the QC failure (e.g., HIGH, MEDIUM). <code>criticality_threshold_pct</code> <code>Optional[int]</code> Threshold percentage for criticality violation. <code>error_count</code> <code>Optional[int]</code> Number of records failing the QC rule. <code>error_pct</code> <code>Optional[int]</code> Percentage of records failing the QC rule. <code>status</code> <code>Optional[str]</code> Status of the QC validation (e.g., PASSED, FAILED). <code>dqm_start_time</code> <code>Optional[datetime]</code> Start time of the DQM validation process. <code>dqm_end_time</code> <code>Optional[datetime]</code> End time of the DQM validation process. Fields Field Name Type Description Primary Key <code>process_id</code> <code>int</code> ID of the transformation process. * <code>transformation_step</code> <code>Optional[str]</code> Order of transformation step (e.g., 'JOIN', 'AGGREGATE'). <code>dataset_id</code> <code>int</code> ID of the current dataset being transformed. * <code>depedent_dataset_id</code> <code>int</code> ID of the dependent dataset needed for transformation. * <code>transformation_type</code> <code>Optional[str]</code> Type of transformation logic (e.g., JOIN, UNION, CUSTOM). <code>join_how</code> <code>Optional[str]</code> Join type (e.g., inner, left, outer). <code>left_table_columns</code> <code>Optional[str]</code> Columns from the left table involved in the join. <code>right_table_columns</code> <code>Optional[str]</code> Columns from the right table involved in the join. <code>primary_keys</code> <code>Optional[str]</code> List of primary keys for the resulting transformed dataset. <code>custom_transformation_query</code> <code>Optional[str]</code> Custom SQL or transformation query when using custom logic. <code>extra_values</code> <code>Optional[str]</code> Additional parameters or metadata required for the transformation. Fields Field Name Type Description Primary Key <code>seq_no</code> <code>Optional[int]</code> Auto-incremented sequence number for logging entries. * <code>batch_id</code> <code>Optional[int]</code> Batch ID associated with the transformation process. <code>data_date</code> <code>Optional[date]</code> Date of the data being transformed (typically partitioning key). <code>process_id</code> <code>Optional[int]</code> ID of the transformation process. <code>dataset_id</code> <code>Optional[int]</code> ID of the dataset undergoing transformation. <code>source_file</code> <code>Optional[str]</code> Name of the source file processed during transformation. <code>status</code> <code>Optional[str]</code> Current status of the transformation job (e.g., SUCCESS, FAILED). <code>exception_details</code> <code>Optional[str]</code> Details of any exceptions encountered during transformation. <code>transformation_start_time</code> <code>Optional[datetime]</code> Start time of the transformation job. <code>transformation_end_time</code> <code>Optional[datetime]</code> End time of the transformation job."},{"location":"Models/Schema/#datacraft_framework.Models.schema.ctlDataAcquisitionConnectionMaster","title":"<code>datacraft_framework.Models.schema.ctlDataAcquisitionConnectionMaster</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Stores connection details for external data acquisition platforms (e.g., SFTP, databases). Used to securely store credentials and configuration for connecting to source systems.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class ctlDataAcquisitionConnectionMaster(SQLModel, table=True):\n    \"\"\"\n    Stores connection details for external data acquisition platforms (e.g., SFTP, databases).\n    Used to securely store credentials and configuration for connecting to source systems.\n    \"\"\"\n\n    outbound_source_platform: str = Field(\n        primary_key=True, description=\"The platform in which the source file is stored.\"\n    )\n    outbound_source_system: str = Field(\n        primary_key=True,\n        description=\"The unique identifier for the source credentials.\",\n    )\n    connection_config: Optional[str] = Field(\n        default=None,\n        description=\"JSON containing the connection details like hostname, port, etc.\",\n    )\n    ssh_private_key: Optional[str] = Field(\n        default=None,\n        description=\"Private key file contents for SFTP connection authentication.\",\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.ctlApiConnectionsDtl","title":"<code>datacraft_framework.Models.schema.ctlApiConnectionsDtl</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Stores detailed API connection configurations including authentication and request parameters.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class ctlApiConnectionsDtl(SQLModel, table=True):\n    \"\"\"\n    Stores detailed API connection configurations including authentication and request parameters.\n    \"\"\"\n\n    seq_no: int = Field(\n        primary_key=True, description=\"Auto-incremented sequence number.\"\n    )\n    pre_ingestion_dataset_id: Optional[int] = Field(\n        default=None,\n        description=\"The dataset ID for RAW/Bronze layer associated with this API call.\",\n    )\n    outbound_source_system: Optional[str] = Field(\n        default=None,\n        description=\"The unique identifier for the source credentials used for the API.\",\n    )\n    type: Optional[str] = Field(\n        default=None,\n        description=\"The API request type. Expected values: ['TOKEN', 'RESPONSE', 'CUSTOM']\",\n    )\n    token_url: Optional[str] = Field(\n        default=None, description=\"URL to fetch a token if type == 'TOKEN'.\"\n    )\n    auth_type: Optional[str] = Field(\n        default=None,\n        description=\"Type of authentication used. Supported types: OAuth, JWT, Basic Auth, CUSTOM.\",\n    )\n    token_type: Optional[str] = Field(\n        default=None, description=\"Type of token used (e.g., Bearer).\"\n    )\n    client_id: Optional[str] = Field(\n        default=None, description=\"Client ID used to request a token.\"\n    )\n    client_secret: Optional[str] = Field(\n        default=None, description=\"Client secret used to request a token.\"\n    )\n    username: Optional[str] = Field(\n        default=None, description=\"Username used for basic authentication.\"\n    )\n    password: Optional[str] = Field(\n        default=None, description=\"Password used for basic authentication.\"\n    )\n    issuer: Optional[str] = Field(\n        default=None,\n        description=\"Issuer required for JWT or service account token requests.\",\n    )\n    scope: Optional[str] = Field(\n        default=None,\n        description=\"Scope required for JWT or service account token requests.\",\n    )\n    private_key: Optional[str] = Field(\n        default=None,\n        description=\"Private key required for JWT or service account token requests.\",\n    )\n    token_path: Optional[str] = Field(\n        default=None,\n        description=\"JSON path where the token exists in the response body.\",\n    )\n    method: Optional[str] = Field(\n        default=None,\n        description=\"HTTP method used for the API request (e.g., GET, POST).\",\n    )\n    url: Optional[str] = Field(\n        default=None, description=\"API endpoint URL to send the request to.\"\n    )\n    headers: Optional[str] = Field(\n        default=None,\n        description=\"HTTP headers to be sent with the request as JSON string.\",\n    )\n    params: Optional[str] = Field(\n        default=None, description=\"Query parameters to be appended to the request URL.\"\n    )\n    data: Optional[str] = Field(\n        default=None, description=\"Raw body content sent when making an API request.\"\n    )\n    json_body: Optional[str] = Field(\n        default=None, description=\"JSON-formatted body sent when making an API request.\"\n    )\n    body_values: Optional[str] = Field(\n        default=None,\n        description=\"Placeholders in the request body that need dynamic replacement.\",\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.ctlDataAcquisitionDetail","title":"<code>datacraft_framework.Models.schema.ctlDataAcquisitionDetail</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Contains metadata about each data acquisition process such as source location, file format, query, and destination.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class ctlDataAcquisitionDetail(SQLModel, table=True):\n    \"\"\"\n    Contains metadata about each data acquisition process such as source location, file format, query, and destination.\n    \"\"\"\n\n    process_id: int = Field(\n        primary_key=True,\n        description=\"The unique ID representing a specific data acquisition process.\",\n    )\n    pre_ingestion_dataset_id: int = Field(\n        primary_key=True,\n        description=\"The Bronze Layer dataset ID. Use Salesforce object name if source platform is Salesforce.\",\n    )\n    pre_ingestion_dataset_name: Optional[str] = Field(\n        default=None, description=\"The Bronze Layer dataset name.\"\n    )\n    outbound_source_platform: Optional[str] = Field(\n        default=None,\n        description=\"Platform where the source file is stored (e.g., SFTP, DB, Salesforce).\",\n    )\n    outbound_source_system: Optional[str] = Field(\n        default=None, description=\"Unique identifier for the source system credentials.\"\n    )\n    outbound_source_location: Optional[str] = Field(\n        default=None,\n        description=\"Location where the source file is stored (e.g., directory path or database schema).\",\n    )\n    outbound_source_file_pattern_static: Optional[str] = Field(\n        default=None,\n        description=\"Flag indicating whether the source filename is static ('Y' or 'N').\",\n    )\n    outbound_source_file_pattern: Optional[str] = Field(\n        default=None,\n        description=\"File name pattern of the source file. Supports YYYY-MM-DD and regex patterns.\",\n    )\n    outbound_source_file_format: Optional[str] = Field(\n        default=None, description=\"Format of the source file (e.g., CSV, JSON, XML).\"\n    )\n    outbound_file_delimiter: Optional[str] = Field(\n        default=None,\n        description=\"Delimiter used in the source file (e.g., comma, tab).\",\n    )\n    query: Optional[str] = Field(\n        default=None,\n        description=\"SQL query to fetch data from a database. Use only when source platform is a database.\",\n    )\n    columns: Optional[str] = Field(\n        default=None,\n        description=\"Comma-separated list of columns to select from Salesforce. Use only when source platform is Salesforce.\",\n    )\n    inbound_location: Optional[str] = Field(\n        default=None,\n        description=\"Destination location where the acquired data will be saved.\",\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.logDataAcquisitionDetail","title":"<code>datacraft_framework.Models.schema.logDataAcquisitionDetail</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Logs execution details of data acquisition processes including status, timing, and exception information.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class logDataAcquisitionDetail(SQLModel, table=True):\n    \"\"\"\n    Logs execution details of data acquisition processes including status, timing, and exception information.\n    \"\"\"\n\n    seq_no: int = Field(\n        primary_key=True, description=\"Auto-incremented sequence number.\"\n    )\n    batch_id: Optional[int] = Field(\n        default=None, description=\"Batch ID of the running process.\"\n    )\n    run_date: Optional[date] = Field(\n        default=None, description=\"Date on which the process was executed.\"\n    )\n    process_id: Optional[int] = Field(\n        default=None, description=\"Unique ID of the process.\"\n    )\n    pre_ingestion_dataset_id: Optional[int] = Field(\n        default=None, description=\"Dataset ID of the Bronze Layer.\"\n    )\n    outbound_source_location: Optional[str] = Field(\n        default=None, description=\"Source file location.\"\n    )\n    inbound_file_location: Optional[str] = Field(\n        default=None, description=\"Inbound file storage location.\"\n    )\n    status: Optional[str] = Field(\n        default=\"IN-PROGRESS\",\n        description=\"Current status of the BRONZE dataset process.\",\n    )\n    exception_details: Optional[str] = Field(\n        default=None,\n        description=\"Details of any exceptions encountered during processing.\",\n    )\n    start_time: Optional[datetime] = Field(\n        default=None, description=\"Start time of the process.\"\n    )\n    end_time: Optional[datetime] = Field(\n        default=None, description=\"End time of the process.\"\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.logRawProcessDtl","title":"<code>datacraft_framework.Models.schema.logRawProcessDtl</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Logs processing details for files in the RAW/Landing layer including status and performance metrics.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class logRawProcessDtl(SQLModel, table=True):\n    \"\"\"\n    Logs processing details for files in the RAW/Landing layer including status and performance metrics.\n    \"\"\"\n\n    file_id: Optional[int] = Field(\n        primary_key=True, description=\"Unique identifier for the processed file.\"\n    )\n    run_date: Optional[date] = Field(\n        default_factory=date.today, description=\"Date when the file was processed.\"\n    )\n    batch_id: Optional[int] = Field(\n        default=None, description=\"Batch ID associated with the file processing.\"\n    )\n    process_id: Optional[int] = Field(\n        default=None, description=\"ID of the process responsible for file ingestion.\"\n    )\n    dataset_id: Optional[int] = Field(\n        default=None, description=\"ID of the dataset being processed.\"\n    )\n    source_file: Optional[str] = Field(\n        default=None, description=\"Name of the source file being processed.\"\n    )\n    landing_location: Optional[str] = Field(\n        default=None, description=\"Path where the file was saved in Landing Layer.\"\n    )\n    file_status: Optional[str] = Field(\n        default=\"IN-PROGRESS\", description=\"Status of the file processing.\"\n    )\n    exception_details: Optional[str] = Field(\n        default=None, description=\"Error message if file processing failed.\"\n    )\n    file_process_start_time: Optional[datetime] = Field(\n        default=None, description=\"Start time of file processing.\"\n    )\n    file_process_end_time: Optional[datetime] = Field(\n        default=None, description=\"End time of file processing.\"\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.ctlDatasetMaster","title":"<code>datacraft_framework.Models.schema.ctlDatasetMaster</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Master configuration table for datasets across all layers (RAW, Landing, Standardization, DQM, Staging, etc.). Defines locations, formats, and partitioning strategies for each dataset.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class ctlDatasetMaster(SQLModel, table=True):\n    \"\"\"\n    Master configuration table for datasets across all layers (RAW, Landing, Standardization, DQM, Staging, etc.).\n    Defines locations, formats, and partitioning strategies for each dataset.\n    \"\"\"\n\n    process_id: int = Field(\n        primary_key=True, description=\"Unique identifier for the data pipeline process.\"\n    )\n    dataset_id: int = Field(\n        primary_key=True, description=\"Unique identifier for the dataset.\"\n    )\n    dataset_name: Optional[str] = Field(\n        default=None, description=\"Name of the dataset.\"\n    )\n    dataset_type: Optional[str] = Field(\n        default=None, description=\"Type of dataset (e.g., RAW, Bronze, Silver, Gold).\"\n    )\n    provider: Optional[str] = Field(\n        default=None, description=\"Source system or provider of the dataset.\"\n    )\n    subject_area: Optional[str] = Field(\n        default=None,\n        description=\"Business domain or subject area this dataset belongs to.\",\n    )\n    inbound_location: Optional[str] = Field(\n        default=None,\n        description=\"File system path where raw data is initially received.\",\n    )\n    inbound_file_pattern: Optional[str] = Field(\n        default=None,\n        description=\"Pattern used to match incoming files (supports date patterns and regex).\",\n    )\n    inbound_static_file_pattern: Optional[str] = Field(\n        default=None,\n        description=\"Flag indicating if the inbound filename is static ('Y'/'N').\",\n    )\n    inbound_file_format: Optional[str] = Field(\n        default=None, description=\"Format of the source file (e.g., CSV, JSON, XML).\"\n    )\n    inbound_file_delimiter: Optional[str] = Field(\n        default=None, description=\"Delimiter used in flat files (e.g., comma, tab).\"\n    )\n    landing_location: Optional[str] = Field(\n        default=None,\n        description=\"Path where data is stored after landing (RAW/Landing Layer).\",\n    )\n    landing_table: Optional[str] = Field(\n        default=None,\n        description=\"Target table name for structured storage in the Landing Layer.\",\n    )\n    landing_partition_columns: Optional[str] = Field(\n        default=None, description=\"Partitioning strategy for the Landing Layer table.\"\n    )\n    data_standardisation_location: Optional[str] = Field(\n        default=None,\n        description=\"Path where standardized version of the dataset is stored.\",\n    )\n    data_standardisation_partition_columns: Optional[str] = Field(\n        default=None, description=\"Partitioning strategy for the Standardized Layer.\"\n    )\n    dqm_error_location: Optional[str] = Field(\n        default=None,\n        description=\"Location where data quality checked version is stored.\",\n    )\n    dqm_partition_columns: Optional[str] = Field(\n        default=None, description=\"Partitioning strategy for the DQM Layer.\"\n    )\n    staging_location: Optional[str] = Field(\n        default=None,\n        description=\"Path where data is stored before transformation into final format.\",\n    )\n    staging_table: Optional[str] = Field(\n        default=None, description=\"Table name for the Staging Layer.\"\n    )\n    staging_partition_columns: Optional[str] = Field(\n        default=None, description=\"Partitioning strategy for the Staging Layer.\"\n    )\n    transformation_location: Optional[str] = Field(\n        default=None, description=\"Path where transformed dataset is stored.\"\n    )\n    transformation_table: Optional[str] = Field(\n        default=None, description=\"Table name for the Transformation Layer.\"\n    )\n    transformation_partition_columns: Optional[str] = Field(\n        default=None, description=\"Partitioning strategy for the Transformation Layer.\"\n    )\n    archive_location: Optional[str] = Field(\n        default=None, description=\"Archive path for historical versions of the dataset.\"\n    )\n    publish_location: Optional[str] = Field(\n        default=None,\n        description=\"Final destination where the dataset is published for consumption.\",\n    )\n    publish_table: Optional[str] = Field(\n        default=None, description=\"Table name for the Published Layer.\"\n    )\n    publish_partition_columns: Optional[str] = Field(\n        default=None, description=\"Partitioning strategy for the Published Layer.\"\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.CtlColumnMetadata","title":"<code>datacraft_framework.Models.schema.CtlColumnMetadata</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Maintains metadata about individual columns in datasets including data types, descriptions, and mappings.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class CtlColumnMetadata(SQLModel, table=True):\n    \"\"\"\n    Maintains metadata about individual columns in datasets including data types, descriptions, and mappings.\n    \"\"\"\n\n    column_id: Optional[int] = Field(\n        primary_key=True, default=None, description=\"Unique identifier for the column.\"\n    )\n    table_name: str = Field(\n        primary_key=True,\n        description=\"Name of the table for which the column metadata is defined.\",\n    )\n    dataset_id: int = Field(\n        primary_key=True, description=\"ID of the dataset this column belongs to.\"\n    )\n    column_name: str = Field(primary_key=True, description=\"Name of the column.\")\n    column_data_type: Optional[str] = Field(\n        default=None,\n        description=\"Data type of the column (e.g., string, integer, float, date).\",\n    )\n    column_date_format: Optional[str] = Field(\n        default=None,\n        description=\"Date format if column_data_type is Date (e.g., 'yyyy-MM-dd').\",\n    )\n    column_description: Optional[str] = Field(\n        default=None,\n        description=\"Description explaining the meaning or business context of the column.\",\n    )\n    column_json_mapping: Optional[str] = Field(\n        default=None,\n        description=\"JSON path mapping used to extract value from raw JSON input.\",\n    )\n    source_column_name: Optional[str] = Field(\n        default=None, description=\"Original column name from the source dataset.\"\n    )\n    column_sequence_number: Optional[int] = Field(\n        default=None, description=\"Order of the column within the table structure.\"\n    )\n    column_tag: Optional[str] = Field(\n        default=None,\n        description=\"Tags for column usage in dashboards (e.g., KPI, KPI-Filter).\",\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.ctlDataStandardisationDtl","title":"<code>datacraft_framework.Models.schema.ctlDataStandardisationDtl</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Configuration table for data standardization rules applied to specific columns. Includes functions and parameters for transforming raw data into standardized formats.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class ctlDataStandardisationDtl(SQLModel, table=True):\n    \"\"\"\n    Configuration table for data standardization rules applied to specific columns.\n    Includes functions and parameters for transforming raw data into standardized formats.\n    \"\"\"\n\n    dataset_id: int = Field(\n        primary_key=True, description=\"ID of the dataset being standardized.\"\n    )\n    column_name: str = Field(\n        primary_key=True,\n        description=\"Name of the column to which the standardization rule applies.\",\n    )\n    function_name: Optional[str] = Field(\n        default=None,\n        description=\"Name of the function used for standardizing the column (e.g., trim, cast, parse_date).\",\n    )\n    function_params: Optional[str] = Field(\n        default=None,\n        description=\"Parameters passed to the function (as JSON or string).\",\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.ctlDqmMasterDtl","title":"<code>datacraft_framework.Models.schema.ctlDqmMasterDtl</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Configuration table for Data Quality Management (DQM) rules applied to datasets. Defines quality checks, thresholds, and criticality levels for validation.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class ctlDqmMasterDtl(SQLModel, table=True):\n    \"\"\"\n    Configuration table for Data Quality Management (DQM) rules applied to datasets.\n    Defines quality checks, thresholds, and criticality levels for validation.\n    \"\"\"\n\n    qc_id: int = Field(\n        primary_key=True, description=\"Unique ID for the data quality check.\"\n    )\n    process_id: Optional[int] = Field(\n        default=None, description=\"ID of the process associated with the QC rule.\"\n    )\n    dataset_id: Optional[int] = Field(\n        default=None, description=\"ID of the dataset being validated.\"\n    )\n    column_name: Optional[str] = Field(\n        default=None, description=\"Name of the column being validated.\"\n    )\n    qc_type: Optional[str] = Field(\n        default=None,\n        description=\"Type of quality check (e.g., not_null, unique, range, regex).\",\n    )\n    qc_param: Optional[str] = Field(\n        default=None,\n        description=\"Parameters for the QC rule (e.g., min/max values, regex pattern).\",\n    )\n    active_flag: Optional[str] = Field(\n        default=None, description=\"Whether the QC rule is active ('Y'/'N').\"\n    )\n    qc_filter: Optional[str] = Field(\n        default=None, description=\"Optional filter condition for applying the QC rule.\"\n    )\n    criticality: Optional[str] = Field(\n        default=None,\n        description=\"Severity level of the QC failure (e.g., HIGH, MEDIUM, LOW).\",\n    )\n    criticality_threshold_pct: Optional[int] = Field(\n        default=None,\n        description=\"Threshold percentage for criticality violation (e.g., 5% nulls allowed).\",\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.logDataStandardisationDtl","title":"<code>datacraft_framework.Models.schema.logDataStandardisationDtl</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Logs execution details of data standardization processes. Tracks transformation steps and any errors that occurred.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class logDataStandardisationDtl(SQLModel, table=True):\n    \"\"\"\n    Logs execution details of data standardization processes.\n    Tracks transformation steps and any errors that occurred.\n    \"\"\"\n\n    seq_no: Optional[int] = Field(\n        primary_key=True,\n        default=None,\n        description=\"Auto-incremented sequence number for logging entries.\",\n    )\n    batch_id: Optional[int] = Field(\n        default=None,\n        description=\"Batch ID associated with the standardization process.\",\n    )\n    process_id: Optional[int] = Field(\n        default=None, description=\"ID of the data pipeline process involved.\"\n    )\n    dataset_id: Optional[int] = Field(\n        default=None, description=\"ID of the dataset undergoing standardization.\"\n    )\n    source_file: Optional[str] = Field(\n        default=None,\n        description=\"Name of the source file processed during standardization.\",\n    )\n    data_standardisation_location: Optional[str] = Field(\n        default=None, description=\"Path where standardized output was written.\"\n    )\n    status: Optional[str] = Field(\n        default=None,\n        description=\"Current status of the standardization process (e.g., SUCCESS, FAILED).\",\n    )\n    exception_details: Optional[str] = Field(\n        default=None,\n        description=\"Details of any exceptions encountered during processing.\",\n    )\n    start_datetime: Optional[datetime] = Field(\n        default=None, description=\"Start time of the standardization process.\"\n    )\n    end_datetime: Optional[datetime] = Field(\n        default=None, description=\"End time of the standardization process.\"\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.logDqmDtl","title":"<code>datacraft_framework.Models.schema.logDqmDtl</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Logs results of data quality checks performed on datasets. Tracks error counts, failure thresholds, and execution times for DQM rules.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class logDqmDtl(SQLModel, table=True):\n    \"\"\"\n    Logs results of data quality checks performed on datasets.\n    Tracks error counts, failure thresholds, and execution times for DQM rules.\n    \"\"\"\n\n    seq_no: Optional[int] = Field(\n        primary_key=True,\n        default=None,\n        description=\"Auto-incremented sequence number for logging entries.\",\n    )\n    process_id: Optional[int] = Field(\n        default=None, description=\"ID of the data pipeline process involved.\"\n    )\n    dataset_id: Optional[int] = Field(\n        default=None, description=\"ID of the dataset being validated.\"\n    )\n    batch_id: Optional[int] = Field(\n        default=None, description=\"Batch ID associated with the DQM run.\"\n    )\n    source_file: Optional[str] = Field(\n        default=None,\n        description=\"Name of the source file processed during DQM validation.\",\n    )\n    column_name: Optional[str] = Field(\n        default=None, description=\"Name of the column being validated.\"\n    )\n    qc_type: Optional[str] = Field(\n        default=None,\n        description=\"Type of quality check performed (e.g., not_null, unique).\",\n    )\n    qc_param: Optional[str] = Field(\n        default=None,\n        description=\"Parameters used for the QC rule (e.g., regex, value ranges).\",\n    )\n    qc_filter: Optional[str] = Field(\n        default=None, description=\"Filter condition applied during QC validation.\"\n    )\n    criticality: Optional[str] = Field(\n        default=None,\n        description=\"Severity level of the QC failure (e.g., HIGH, MEDIUM).\",\n    )\n    criticality_threshold_pct: Optional[int] = Field(\n        default=None, description=\"Threshold percentage for criticality violation.\"\n    )\n    error_count: Optional[int] = Field(\n        default=None, description=\"Number of records failing the QC rule.\"\n    )\n    error_pct: Optional[int] = Field(\n        default=None, description=\"Percentage of records failing the QC rule.\"\n    )\n    status: Optional[str] = Field(\n        default=None, description=\"Status of the QC validation (e.g., PASSED, FAILED).\"\n    )\n    dqm_start_time: Optional[datetime] = Field(\n        default=None, description=\"Start time of the DQM validation process.\"\n    )\n    dqm_end_time: Optional[datetime] = Field(\n        default=None, description=\"End time of the DQM validation process.\"\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.ctlTransformationDependencyMaster","title":"<code>datacraft_framework.Models.schema.ctlTransformationDependencyMaster</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Stores transformation dependencies between datasets. Defines join logic, primary keys, and custom queries used during transformations.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class ctlTransformationDependencyMaster(SQLModel, table=True):\n    \"\"\"\n    Stores transformation dependencies between datasets.\n    Defines join logic, primary keys, and custom queries used during transformations.\n    \"\"\"\n\n    process_id: int = Field(\n        primary_key=True, description=\"ID of the transformation process.\"\n    )\n    transformation_step: Optional[str] = Field(\n        default=None,\n        description=\"Order of transformation step (e.g., 'JOIN', 'AGGREGATE').\",\n    )\n    dataset_id: int = Field(\n        primary_key=True, description=\"ID of the current dataset being transformed.\"\n    )\n    depedent_dataset_id: int = Field(\n        primary_key=True,\n        description=\"ID of the dependent dataset needed for transformation.\",\n    )\n    transformation_type: Optional[str] = Field(\n        default=None,\n        description=\"Type of transformation logic (e.g., JOIN, UNION, CUSTOM).\",\n    )\n    join_how: Optional[str] = Field(\n        default=None, description=\"Join type (e.g., inner, left, outer).\"\n    )\n    left_table_columns: Optional[str] = Field(\n        default=None, description=\"Columns from the left table involved in the join.\"\n    )\n    right_table_columns: Optional[str] = Field(\n        default=None, description=\"Columns from the right table involved in the join.\"\n    )\n    primary_keys: Optional[str] = Field(\n        default=None,\n        description=\"List of primary keys for the resulting transformed dataset.\",\n    )\n    custom_transformation_query: Optional[str] = Field(\n        default=None,\n        description=\"Custom SQL or transformation query when using custom logic.\",\n    )\n    extra_values: Optional[str] = Field(\n        default=None,\n        description=\"Additional parameters or metadata required for the transformation.\",\n    )\n</code></pre>"},{"location":"Models/Schema/#datacraft_framework.Models.schema.logTransformationDtl","title":"<code>datacraft_framework.Models.schema.logTransformationDtl</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Logs execution details of transformation jobs. Tracks performance metrics and failures related to dataset transformations.</p> Source code in <code>src/datacraft_framework/Models/schema.py</code> <pre><code>class logTransformationDtl(SQLModel, table=True):\n    \"\"\"\n    Logs execution details of transformation jobs.\n    Tracks performance metrics and failures related to dataset transformations.\n    \"\"\"\n\n    seq_no: Optional[int] = Field(\n        primary_key=True,\n        default=None,\n        description=\"Auto-incremented sequence number for logging entries.\",\n    )\n    batch_id: Optional[int] = Field(\n        default=None, description=\"Batch ID associated with the transformation process.\"\n    )\n    data_date: Optional[date] = Field(\n        default=None,\n        description=\"Date of the data being transformed (typically partitioning key).\",\n    )\n    process_id: Optional[int] = Field(\n        default=None, description=\"ID of the transformation process.\"\n    )\n    dataset_id: Optional[int] = Field(\n        default=None, description=\"ID of the dataset undergoing transformation.\"\n    )\n    source_file: Optional[str] = Field(\n        default=None,\n        description=\"Name of the source file processed during transformation.\",\n    )\n    status: Optional[str] = Field(\n        default=None,\n        description=\"Current status of the transformation job (e.g., SUCCESS, FAILED).\",\n    )\n    exception_details: Optional[str] = Field(\n        default=None,\n        description=\"Details of any exceptions encountered during transformation.\",\n    )\n    transformation_start_time: Optional[datetime] = Field(\n        default=None, description=\"Start time of the transformation job.\"\n    )\n    transformation_end_time: Optional[datetime] = Field(\n        default=None, description=\"End time of the transformation job.\"\n    )\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/","title":"Documentation for <code>DataQualityCheck</code>","text":""},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck","title":"<code>datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck</code>","text":"Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>class DataQualityCheck:\n    def null_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Perform null value check on specified column.\n\n        Filters rows where the target column contains non-null values.\n        Logs results into `logDqmDtl` table and optionally raises exception if criticality is breached.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: Filtered DataFrame containing only valid (non-null) records.\n\n        Raises:\n            Exception: If null values exceed criticality threshold and marked 'C' (Critical).\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n            null_check_dqm_ = df.filter(\n                polars.col(dqm_detail.column_name).is_not_null()\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n        else:\n            null_check_dqm_ = df.filter(\n                polars.col(dqm_detail.column_name).is_not_null()\n            )\n\n        failure_count = total_count - len(null_check_dqm_)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(null_check_dqm_, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return null_check_dqm_\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return null_check_dqm_\n\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=0,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return null_check_dqm_\n\n    def unique_dqm(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Check that all values in a column (or set of columns) are unique.\n\n        Uses `.unique()` to identify duplicates and logs them if criticality threshold is crossed.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only unique values.\n\n        Raises:\n            Exception: If duplicate values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        unique_dqm_df = df.unique(subset=dqm_detail.column_name.split(\",\"))\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            unique_dqm_df = unique_dqm_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(unique_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(unique_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return unique_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return unique_dqm_df\n\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return unique_dqm_df\n\n    def length_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Validate string length against an expected expression.\n\n        Supports conditions like \"&gt;\", \"&lt;\", \"==\", etc., applied to string length.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid-length strings.\n\n        Raises:\n            Exception: If invalid lengths exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        parsed_qc_param = json_loads(dqm_detail.qc_param)\n        param_exp = parsed_qc_param[\"expression\"]\n        param_value = parsed_qc_param[\"value\"]\n\n        length_validation_df = df.sql(\n            f\"SELECT * FROM self WHERE length({dqm_detail.column_name}) {param_exp} {param_value}\"\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            length_validation_df = length_validation_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(length_validation_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(length_validation_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return length_validation_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return length_validation_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return length_validation_df\n\n    def date_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Validate column values match expected date format using regex patterns.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid date-formatted values.\n\n        Raises:\n            Exception: If invalid dates exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        date_regex_pattern = get_date_regex(qc_param=dqm_detail.qc_param)\n\n        date_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).str.contains(date_regex_pattern)\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            date_dqm_df = date_dqm_df.filter(polars.col(dqm_detail.column_name)).sql(\n                f\"SELECT * FROM self WHERE {filter_cols}\"\n            )\n\n        failure_count = total_count - len(date_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(date_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return date_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return date_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return date_dqm_df\n\n    def integer_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        r\"\"\"\n        Validate column values are integers.\n\n        Uses regex pattern `(^\\d+$)` to identify valid integers.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid integer values.\n\n        Raises:\n            Exception: If invalid values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        integer_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).str.contains(r\"(^-?\\d+$)\")\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            integer_dqm_df = integer_dqm_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(integer_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(integer_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return integer_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return integer_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return integer_dqm_df\n\n    def decimal_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        r\"\"\"\n        Validate column values are numeric (including decimals).\n\n        Uses regex pattern `(^\\d*[.]?\\d+$)` to identify valid numeric values.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid decimal values.\n\n        Raises:\n            Exception: If invalid values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        decimal_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).str.contains(r\"(^-?\\d+$)\")\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            decimal_dqm_df = decimal_dqm_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(decimal_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(decimal_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return decimal_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return decimal_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return decimal_dqm_df\n\n    def domain_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Validate column values fall within a defined domain.\n\n        Accepts comma-separated allowed values from `qc_param`.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only valid domain values.\n\n        Raises:\n            Exception: If invalid values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        domain_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).is_in(dqm_detail.qc_param.split(\",\"))\n        )\n\n        if dqm_detail.qc_filter:\n            filter_cols = dqm_detail.qc_filter.split(\",\")\n            filter_cols = \" AND \".join(filter_cols)\n\n            domain_dqm_df = domain_dqm_df.filter(\n                polars.col(dqm_detail.column_name)\n            ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n        failure_count = total_count - len(domain_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(domain_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return domain_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return domain_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return domain_dqm_df\n\n    def custom_dqm_check(\n        self,\n        df: polars.DataFrame,\n        batch_id: int,\n        source_file: str,\n        dqm_detail: ctlDqmMasterDtl,\n        dataset_master: ctlDatasetMaster,\n        orch_process: OrchestrationProcess,\n    ) -&gt; polars.DataFrame:\n        \"\"\"\n        Apply custom SQL-like filter to perform flexible quality checks.\n\n        Args:\n            df (polars.DataFrame): Input DataFrame to validate.\n            batch_id (int): Batch ID associated with this data.\n            source_file (str): Source file name for logging purposes.\n            dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n            dataset_master (ctlDatasetMaster): Dataset metadata object.\n            orch_process (OrchestrationProcess): Instance used for logging.\n\n        Returns:\n            polars.DataFrame: DataFrame with only records passing the custom filter.\n\n        Raises:\n            Exception: If invalid values exceed criticality threshold and marked 'C'.\n        \"\"\"\n\n        start_time = datetime.now()\n        total_count = len(df)\n\n        custom_dqm_df = df.filter(\n            polars.col(dqm_detail.column_name).sql(dqm_detail.qc_param)\n        )\n\n        failure_count = total_count - len(custom_dqm_df)\n\n        if failure_count != 0:\n            failure_percentage = (failure_count / total_count) * 100\n            failed_records = df.join(custom_dqm_df, on=df.columns, how=\"anti\")\n\n            if (dqm_detail.criticality == \"C\") and (\n                failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n            ):\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"FAILED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.error(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n                raise Exception(\n                    f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n                )\n            elif (dqm_detail.criticality == \"C\") and (\n                failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n            ):\n                # Write Passed Records only\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return custom_dqm_df\n            elif dqm_detail.criticality == \"NC\":\n                # As its NC no exception is raised and only passed records are saved.\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.dataset_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=batch_id,\n                        source_file=source_file,\n                        column_name=dqm_detail.column_name,\n                        qc_type=dqm_detail.qc_type,\n                        qc_param=dqm_detail.qc_param,\n                        qc_filter=dqm_detail.qc_param,\n                        criticality=dqm_detail.criticality,\n                        criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                        error_count=failure_count,\n                        error_pct=failure_percentage,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n                logger.warning(\n                    f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n                )\n                return custom_dqm_df\n        else:\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=0,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.info(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n            )\n            return custom_dqm_df\n\n    def __init__(\n        self,\n        dqm_details: list[ctlDqmMasterDtl],\n        dataset_master: ctlDatasetMaster,\n    ):\n        \"\"\"\n        Initialize and execute DQM checks for unprocessed transformation files.\n\n        Reads transformation data from Delta, applies DQM rules, logs results,\n        and writes clean data to publish layer.\n\n        Args:\n            dqm_details (list[ctlDqmMasterDtl]): List of DQM rule definitions.\n            dataset_master (ctlDatasetMaster): Metadata about dataset and paths.\n\n        Raises:\n            Exception: If no unprocessed files found or any critical DQM check fails.\n        \"\"\"\n\n        with OrchestrationProcess() as orch_process:\n            unprocessed_files = orch_process.get_dqm_unprocessed_files(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n            )\n\n            if len(unprocessed_files) == 0:\n                raise Exception(\n                    f\"No unprocess files found for Data Quality Checks for dataset id: {dataset_master.dataset_id}\"\n                )\n\n            for unprocessed_file in unprocessed_files:\n                data_standard_path = path_to_s3(\n                    location=dataset_master.data_standardisation_location,\n                    env=env,\n                )\n                compute_dqm_path = path_to_s3(\n                    location=dataset_master.staging_location, env=env\n                )\n\n                if len(dqm_details) != 0:\n                    # First read data standard data.\n                    original_df = DeltaTableRead(\n                        delta_path=data_standard_path[\"s3_location\"],\n                        batch_id=unprocessed_file.batch_id,\n                    ).read()\n\n                    dqm_check_df = original_df\n\n                    for dqm_detail in dqm_details:\n                        if dqm_detail.qc_type.lower() == \"null\":\n                            dqm_check_df = self.null_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"unique\":\n                            dqm_check_df = self.unique_dqm(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"decimal\":\n                            dqm_check_df = self.decimal_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"integer\":\n                            dqm_check_df = self.integer_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"length\":\n                            dqm_check_df = self.length_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"date\":\n                            dqm_check_df = self.date_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"domain\":\n                            dqm_check_df = self.domain_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n\n                        if dqm_detail.qc_type.lower() == \"custom\":\n                            dqm_check_df = self.custom_dqm_check(\n                                df=dqm_check_df,\n                                batch_id=unprocessed_file.batch_id,\n                                source_file=unprocessed_file.source_file,\n                                dqm_detail=dqm_detail,\n                                dataset_master=dataset_master,\n                                orch_process=orch_process,\n                            )\n                    DeltaTableWriter(\n                        input_data=dqm_check_df,\n                        save_location=compute_dqm_path[\"s3_location\"],\n                        batch_id=unprocessed_file.batch_id,\n                        partition_columns=dataset_master.staging_partition_columns,\n                    )\n\n                else:\n                    start_time = datetime.now()\n                    original_df = DeltaTableRead(\n                        delta_path=data_standard_path[\"s3_location\"],\n                        batch_id=unprocessed_file.batch_id,\n                    ).read()\n                    DeltaTableWriter(\n                        input_data=original_df,\n                        save_location=compute_dqm_path[\"s3_location\"],\n                        batch_id=unprocessed_file.batch_id,\n                        partition_columns=dataset_master.staging_partition_columns,\n                    )\n                    orch_process.insert_log_dqm(\n                        log_dqm=logDqmDtl(\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            column_name=None,\n                            qc_type=None,\n                            qc_param=None,\n                            qc_filter=None,\n                            criticality=None,\n                            criticality_threshold_pct=None,\n                            error_count=0,\n                            error_pct=0,\n                            status=\"SUCCEEDED\",\n                            dqm_start_time=start_time,\n                            dqm_end_time=datetime.now(),\n                        )\n                    )\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.null_check","title":"<code>null_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Perform null value check on specified column.</p> <p>Filters rows where the target column contains non-null values. Logs results into <code>logDqmDtl</code> table and optionally raises exception if criticality is breached.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: Filtered DataFrame containing only valid (non-null) records.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If null values exceed criticality threshold and marked 'C' (Critical).</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def null_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Perform null value check on specified column.\n\n    Filters rows where the target column contains non-null values.\n    Logs results into `logDqmDtl` table and optionally raises exception if criticality is breached.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: Filtered DataFrame containing only valid (non-null) records.\n\n    Raises:\n        Exception: If null values exceed criticality threshold and marked 'C' (Critical).\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n        null_check_dqm_ = df.filter(\n            polars.col(dqm_detail.column_name).is_not_null()\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n    else:\n        null_check_dqm_ = df.filter(\n            polars.col(dqm_detail.column_name).is_not_null()\n        )\n\n    failure_count = total_count - len(null_check_dqm_)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(null_check_dqm_, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return null_check_dqm_\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return null_check_dqm_\n\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.dataset_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=0,\n                error_pct=failure_percentage,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return null_check_dqm_\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.unique_dqm","title":"<code>unique_dqm(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Check that all values in a column (or set of columns) are unique.</p> <p>Uses <code>.unique()</code> to identify duplicates and logs them if criticality threshold is crossed.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only unique values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If duplicate values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def unique_dqm(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Check that all values in a column (or set of columns) are unique.\n\n    Uses `.unique()` to identify duplicates and logs them if criticality threshold is crossed.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only unique values.\n\n    Raises:\n        Exception: If duplicate values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    unique_dqm_df = df.unique(subset=dqm_detail.column_name.split(\",\"))\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        unique_dqm_df = unique_dqm_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(unique_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(unique_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return unique_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return unique_dqm_df\n\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.dataset_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return unique_dqm_df\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.length_dqm_check","title":"<code>length_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate string length against an expected expression.</p> <p>Supports conditions like \"&gt;\", \"&lt;\", \"==\", etc., applied to string length.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid-length strings.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid lengths exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def length_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Validate string length against an expected expression.\n\n    Supports conditions like \"&gt;\", \"&lt;\", \"==\", etc., applied to string length.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid-length strings.\n\n    Raises:\n        Exception: If invalid lengths exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    parsed_qc_param = json_loads(dqm_detail.qc_param)\n    param_exp = parsed_qc_param[\"expression\"]\n    param_value = parsed_qc_param[\"value\"]\n\n    length_validation_df = df.sql(\n        f\"SELECT * FROM self WHERE length({dqm_detail.column_name}) {param_exp} {param_value}\"\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        length_validation_df = length_validation_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(length_validation_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(length_validation_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return length_validation_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return length_validation_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.dataset_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return length_validation_df\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.date_dqm_check","title":"<code>date_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate column values match expected date format using regex patterns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid date-formatted values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid dates exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def date_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Validate column values match expected date format using regex patterns.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid date-formatted values.\n\n    Raises:\n        Exception: If invalid dates exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    date_regex_pattern = get_date_regex(qc_param=dqm_detail.qc_param)\n\n    date_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).str.contains(date_regex_pattern)\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        date_dqm_df = date_dqm_df.filter(polars.col(dqm_detail.column_name)).sql(\n            f\"SELECT * FROM self WHERE {filter_cols}\"\n        )\n\n    failure_count = total_count - len(date_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(date_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return date_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return date_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.dataset_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return date_dqm_df\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.integer_dqm_check","title":"<code>integer_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate column values are integers.</p> <p>Uses regex pattern <code>(^\\d+$)</code> to identify valid integers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid integer values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def integer_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    r\"\"\"\n    Validate column values are integers.\n\n    Uses regex pattern `(^\\d+$)` to identify valid integers.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid integer values.\n\n    Raises:\n        Exception: If invalid values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    integer_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).str.contains(r\"(^-?\\d+$)\")\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        integer_dqm_df = integer_dqm_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(integer_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(integer_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return integer_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return integer_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.dataset_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return integer_dqm_df\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.decimal_dqm_check","title":"<code>decimal_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate column values are numeric (including decimals).</p> <p>Uses regex pattern <code>(^\\d*[.]?\\d+$)</code> to identify valid numeric values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid decimal values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def decimal_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    r\"\"\"\n    Validate column values are numeric (including decimals).\n\n    Uses regex pattern `(^\\d*[.]?\\d+$)` to identify valid numeric values.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid decimal values.\n\n    Raises:\n        Exception: If invalid values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    decimal_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).str.contains(r\"(^-?\\d+$)\")\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        decimal_dqm_df = decimal_dqm_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(decimal_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(decimal_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return decimal_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return decimal_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.dataset_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return decimal_dqm_df\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.domain_dqm_check","title":"<code>domain_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Validate column values fall within a defined domain.</p> <p>Accepts comma-separated allowed values from <code>qc_param</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only valid domain values.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def domain_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Validate column values fall within a defined domain.\n\n    Accepts comma-separated allowed values from `qc_param`.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only valid domain values.\n\n    Raises:\n        Exception: If invalid values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    domain_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).is_in(dqm_detail.qc_param.split(\",\"))\n    )\n\n    if dqm_detail.qc_filter:\n        filter_cols = dqm_detail.qc_filter.split(\",\")\n        filter_cols = \" AND \".join(filter_cols)\n\n        domain_dqm_df = domain_dqm_df.filter(\n            polars.col(dqm_detail.column_name)\n        ).sql(f\"SELECT * FROM self WHERE {filter_cols}\")\n\n    failure_count = total_count - len(domain_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(domain_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return domain_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return domain_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.dataset_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return domain_dqm_df\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.custom_dqm_check","title":"<code>custom_dqm_check(df, batch_id, source_file, dqm_detail, dataset_master, orch_process)</code>","text":"<p>Apply custom SQL-like filter to perform flexible quality checks.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to validate.</p> required <code>batch_id</code> <code>int</code> <p>Batch ID associated with this data.</p> required <code>source_file</code> <code>str</code> <p>Source file name for logging purposes.</p> required <code>dqm_detail</code> <code>ctlDqmMasterDtl</code> <p>DQM rule definition object.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Dataset metadata object.</p> required <code>orch_process</code> <code>OrchestrationProcess</code> <p>Instance used for logging.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>polars.DataFrame: DataFrame with only records passing the custom filter.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If invalid values exceed criticality threshold and marked 'C'.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def custom_dqm_check(\n    self,\n    df: polars.DataFrame,\n    batch_id: int,\n    source_file: str,\n    dqm_detail: ctlDqmMasterDtl,\n    dataset_master: ctlDatasetMaster,\n    orch_process: OrchestrationProcess,\n) -&gt; polars.DataFrame:\n    \"\"\"\n    Apply custom SQL-like filter to perform flexible quality checks.\n\n    Args:\n        df (polars.DataFrame): Input DataFrame to validate.\n        batch_id (int): Batch ID associated with this data.\n        source_file (str): Source file name for logging purposes.\n        dqm_detail (ctlDqmMasterDtl): DQM rule definition object.\n        dataset_master (ctlDatasetMaster): Dataset metadata object.\n        orch_process (OrchestrationProcess): Instance used for logging.\n\n    Returns:\n        polars.DataFrame: DataFrame with only records passing the custom filter.\n\n    Raises:\n        Exception: If invalid values exceed criticality threshold and marked 'C'.\n    \"\"\"\n\n    start_time = datetime.now()\n    total_count = len(df)\n\n    custom_dqm_df = df.filter(\n        polars.col(dqm_detail.column_name).sql(dqm_detail.qc_param)\n    )\n\n    failure_count = total_count - len(custom_dqm_df)\n\n    if failure_count != 0:\n        failure_percentage = (failure_count / total_count) * 100\n        failed_records = df.join(custom_dqm_df, on=df.columns, how=\"anti\")\n\n        if (dqm_detail.criticality == \"C\") and (\n            failure_percentage &gt;= dqm_detail.criticality_threshold_pct\n        ):\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"FAILED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.error(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check Failed For: {dqm_detail.qc_type} as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n            raise Exception(\n                f\"DQM Checks failed for dataset ID: {dataset_master.dataset_id} for {dqm_detail.qc_type}-Check as it crossed Criticality Threshold {failure_percentage}%.\"\n            )\n        elif (dqm_detail.criticality == \"C\") and (\n            failure_percentage &lt;= dqm_detail.criticality_threshold_pct\n        ):\n            # Write Passed Records only\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return custom_dqm_df\n        elif dqm_detail.criticality == \"NC\":\n            # As its NC no exception is raised and only passed records are saved.\n            orch_process.insert_log_dqm(\n                log_dqm=logDqmDtl(\n                    process_id=dataset_master.dataset_id,\n                    dataset_id=dataset_master.dataset_id,\n                    batch_id=batch_id,\n                    source_file=source_file,\n                    column_name=dqm_detail.column_name,\n                    qc_type=dqm_detail.qc_type,\n                    qc_param=dqm_detail.qc_param,\n                    qc_filter=dqm_detail.qc_param,\n                    criticality=dqm_detail.criticality,\n                    criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                    error_count=failure_count,\n                    error_pct=failure_percentage,\n                    status=\"SUCCEEDED\",\n                    dqm_start_time=start_time,\n                    dqm_end_time=datetime.now(),\n                )\n            )\n            logger.warning(\n                f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} has some failed records of {failure_count} out of {total_count}.\"\n            )\n            return custom_dqm_df\n    else:\n        orch_process.insert_log_dqm(\n            log_dqm=logDqmDtl(\n                process_id=dataset_master.dataset_id,\n                dataset_id=dataset_master.dataset_id,\n                batch_id=batch_id,\n                source_file=source_file,\n                column_name=dqm_detail.column_name,\n                qc_type=dqm_detail.qc_type,\n                qc_param=dqm_detail.qc_param,\n                qc_filter=dqm_detail.qc_param,\n                criticality=dqm_detail.criticality,\n                criticality_threshold_pct=dqm_detail.criticality_threshold_pct,\n                error_count=failure_count,\n                error_pct=0,\n                status=\"SUCCEEDED\",\n                dqm_start_time=start_time,\n                dqm_end_time=datetime.now(),\n            )\n        )\n        logger.info(\n            f\"Dataset ID: {dataset_master.dataset_id} DQM check : {dqm_detail.qc_type} Has passed.\"\n        )\n        return custom_dqm_df\n</code></pre>"},{"location":"SilverLayerScripts/DataQualityCheck/#datacraft_framework.SilverLayerScripts.DataQualityCheck.DataQualityCheck.__init__","title":"<code>__init__(dqm_details, dataset_master)</code>","text":"<p>Initialize and execute DQM checks for unprocessed transformation files.</p> <p>Reads transformation data from Delta, applies DQM rules, logs results, and writes clean data to publish layer.</p> <p>Parameters:</p> Name Type Description Default <code>dqm_details</code> <code>list[ctlDqmMasterDtl]</code> <p>List of DQM rule definitions.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Metadata about dataset and paths.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no unprocessed files found or any critical DQM check fails.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataQualityCheck.py</code> <pre><code>def __init__(\n    self,\n    dqm_details: list[ctlDqmMasterDtl],\n    dataset_master: ctlDatasetMaster,\n):\n    \"\"\"\n    Initialize and execute DQM checks for unprocessed transformation files.\n\n    Reads transformation data from Delta, applies DQM rules, logs results,\n    and writes clean data to publish layer.\n\n    Args:\n        dqm_details (list[ctlDqmMasterDtl]): List of DQM rule definitions.\n        dataset_master (ctlDatasetMaster): Metadata about dataset and paths.\n\n    Raises:\n        Exception: If no unprocessed files found or any critical DQM check fails.\n    \"\"\"\n\n    with OrchestrationProcess() as orch_process:\n        unprocessed_files = orch_process.get_dqm_unprocessed_files(\n            process_id=dataset_master.process_id,\n            dataset_id=dataset_master.dataset_id,\n        )\n\n        if len(unprocessed_files) == 0:\n            raise Exception(\n                f\"No unprocess files found for Data Quality Checks for dataset id: {dataset_master.dataset_id}\"\n            )\n\n        for unprocessed_file in unprocessed_files:\n            data_standard_path = path_to_s3(\n                location=dataset_master.data_standardisation_location,\n                env=env,\n            )\n            compute_dqm_path = path_to_s3(\n                location=dataset_master.staging_location, env=env\n            )\n\n            if len(dqm_details) != 0:\n                # First read data standard data.\n                original_df = DeltaTableRead(\n                    delta_path=data_standard_path[\"s3_location\"],\n                    batch_id=unprocessed_file.batch_id,\n                ).read()\n\n                dqm_check_df = original_df\n\n                for dqm_detail in dqm_details:\n                    if dqm_detail.qc_type.lower() == \"null\":\n                        dqm_check_df = self.null_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"unique\":\n                        dqm_check_df = self.unique_dqm(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"decimal\":\n                        dqm_check_df = self.decimal_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"integer\":\n                        dqm_check_df = self.integer_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"length\":\n                        dqm_check_df = self.length_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"date\":\n                        dqm_check_df = self.date_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"domain\":\n                        dqm_check_df = self.domain_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n\n                    if dqm_detail.qc_type.lower() == \"custom\":\n                        dqm_check_df = self.custom_dqm_check(\n                            df=dqm_check_df,\n                            batch_id=unprocessed_file.batch_id,\n                            source_file=unprocessed_file.source_file,\n                            dqm_detail=dqm_detail,\n                            dataset_master=dataset_master,\n                            orch_process=orch_process,\n                        )\n                DeltaTableWriter(\n                    input_data=dqm_check_df,\n                    save_location=compute_dqm_path[\"s3_location\"],\n                    batch_id=unprocessed_file.batch_id,\n                    partition_columns=dataset_master.staging_partition_columns,\n                )\n\n            else:\n                start_time = datetime.now()\n                original_df = DeltaTableRead(\n                    delta_path=data_standard_path[\"s3_location\"],\n                    batch_id=unprocessed_file.batch_id,\n                ).read()\n                DeltaTableWriter(\n                    input_data=original_df,\n                    save_location=compute_dqm_path[\"s3_location\"],\n                    batch_id=unprocessed_file.batch_id,\n                    partition_columns=dataset_master.staging_partition_columns,\n                )\n                orch_process.insert_log_dqm(\n                    log_dqm=logDqmDtl(\n                        process_id=dataset_master.process_id,\n                        dataset_id=dataset_master.dataset_id,\n                        batch_id=unprocessed_file.batch_id,\n                        source_file=unprocessed_file.source_file,\n                        column_name=None,\n                        qc_type=None,\n                        qc_param=None,\n                        qc_filter=None,\n                        criticality=None,\n                        criticality_threshold_pct=None,\n                        error_count=0,\n                        error_pct=0,\n                        status=\"SUCCEEDED\",\n                        dqm_start_time=start_time,\n                        dqm_end_time=datetime.now(),\n                    )\n                )\n</code></pre>"},{"location":"SilverLayerScripts/DataStandardization/","title":"Documentation for <code>DataStandardization</code>","text":""},{"location":"SilverLayerScripts/DataStandardization/#datacraft_framework.SilverLayerScripts.DataStandardization.DataStandardization","title":"<code>datacraft_framework.SilverLayerScripts.DataStandardization.DataStandardization</code>","text":"<p>Handles the data standardization process for unprocessed files in the SILVER layer.</p> <p>This class reads data from the landing location, applies transformations based on data standardization rules, and writes the standardized data to a specified S3 location. It also logs the status and any exceptions encountered during the process.</p> <p>Attributes:</p> Name Type Description <code>data_standard_detail</code> <code>list[ctlDataStandardisationDtl]</code> <p>List of standardization rules to apply.</p> <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Metadata for the dataset being processed.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataStandardization.py</code> <pre><code>class DataStandardization:\n    \"\"\"\n    Handles the data standardization process for unprocessed files in the SILVER layer.\n\n    This class reads data from the landing location, applies transformations based on\n    data standardization rules, and writes the standardized data to a specified S3 location.\n    It also logs the status and any exceptions encountered during the process.\n\n    Attributes:\n        data_standard_detail (list[ctlDataStandardisationDtl]): List of standardization rules to apply.\n        dataset_master (ctlDatasetMaster): Metadata for the dataset being processed.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_standard_detail: list[ctlDataStandardisationDtl],\n        dataset_master: ctlDatasetMaster,\n    ):\n        \"\"\"\n        Initializes the DataStandardization process and executes it.\n\n        Args:\n            data_standard_detail (list[ctlDataStandardisationDtl]):\n                A list of standardization rules such as padding, trimming, and type conversion.\n            dataset_master (ctlDatasetMaster):\n                Master metadata of the dataset containing details like process ID, dataset ID,\n                and storage locations.\n\n        Raises:\n            Exception: If no unprocessed files are found for the given dataset.\n            Exception: If an unsupported function or padding type is encountered.\n            Exception: For any failure during the standardization process, details are logged.\n        \"\"\"\n\n        with OrchestrationProcess() as orch_process:\n\n            unprocessed_files = orch_process.get_data_standardisation_unprocessed_files(\n                process_id=dataset_master.process_id,\n                dataset_id=dataset_master.dataset_id,\n            )\n            data_standard_location = path_to_s3(\n                location=dataset_master.data_standardisation_location, env=env\n            )\n            landing_location_ = path_to_s3(\n                location=dataset_master.landing_location,\n                env=env,\n            )\n\n            if len(unprocessed_files) != 0:\n\n                for unprocessed_file in unprocessed_files:\n                    start_time = datetime.now()\n                    column_meta = orch_process.get_ctl_column_metadata(\n                        dataset_id=dataset_master.dataset_id,\n                    )\n                    rename_mapping = {\n                        x.source_column_name: x.column_name for x in column_meta\n                    }\n\n                    df = DeltaTableRead(\n                        delta_path=landing_location_[\"s3_location\"],\n                        batch_id=unprocessed_file.batch_id,\n                    ).read()\n                    df = df.rename(rename_mapping)\n                    df = SchemaCaster(df=df, column_metadata=column_meta).start()\n\n                    if len(data_standard_detail) != 0:\n                        for data_standard in data_standard_detail:\n                            try:\n                                if data_standard.function_name == \"padding\":\n                                    parsed_json = json_loads(\n                                        data_standard.function_params\n                                    )\n                                    padding_type = parsed_json[\"type\"]\n                                    padding_length = int(parsed_json[\"length\"])\n                                    padding_value = parsed_json[\"padding_value\"]\n\n                                    if padding_type == \"left\":\n                                        df = df.with_columns(\n                                            polars.col(\n                                                data_standard.column_name\n                                            ).str.pad_start(\n                                                fill_char=padding_value,\n                                                length=padding_length,\n                                            )\n                                        )\n                                    elif padding_type == \"right\":\n                                        df = df.with_columns(\n                                            polars.col(\n                                                data_standard.column_name\n                                            ).str.pad_end(\n                                                fill_char=padding_value,\n                                                length=padding_length,\n                                            )\n                                        )\n                                    else:\n                                        raise Exception(\n                                            f\"Unsuppored Padding type: {padding_value}\"\n                                        )\n\n                                elif data_standard.function_name == \"trim\":\n                                    df = df.with_columns(\n                                        polars.col(\n                                            data_standard.column_name\n                                        ).str.strip_chars()\n                                    )\n\n                                elif data_standard.function_name == \"blank_conversion\":\n                                    df = df.with_columns(\n                                        polars.col(data_standard.column_name)\n                                        .str.strip_chars()\n                                        .replace(pattern=r\"\\s+\", value=\" \")\n                                    )\n\n                                elif data_standard.function_name == \"replace\":\n                                    parsed_json = json_loads(\n                                        data_standard.function_params\n                                    )\n                                    to_replace_pattern = parsed_json[\"value\"]\n                                    replacement = parsed_json[\"value\"]\n\n                                    df = df.with_columns(\n                                        polars.col(\n                                            data_standard.column_name\n                                        ).str.replace(\n                                            pattern=to_replace_pattern,\n                                            value=replacement,\n                                        )\n                                    )\n\n                                elif data_standard.function_name == \"type_conversion\":\n                                    parsed_json = json_loads(\n                                        data_standard.function_params\n                                    )\n                                    type_conversion_type = parsed_json[\"type\"]\n\n                                    if type_conversion_type == \"lower\":\n                                        df = df.with_columns(\n                                            polars.col(\n                                                data_standard.column_name\n                                            ).str.to_lowercase()\n                                        )\n                                    elif type_conversion_type == \"upper\":\n                                        df = df.with_columns(\n                                            polars.col(\n                                                data_standard.column_name\n                                            ).str.to_lowercase()\n                                        )\n\n                                elif data_standard.function_name == \"sub_string\":\n                                    parsed_json = json_loads(\n                                        data_standard.function_params\n                                    )\n\n                                    start_index = int(parsed_json[\"start_index\"])\n                                    length = int(parsed_json[\"length\"])\n\n                                    df = df.with_columns(\n                                        polars.col(data_standard.column_name).str.slice(\n                                            offset=start_index,\n                                            length=length,\n                                        )\n                                    )\n                                else:\n                                    raise Exception(\n                                        f\"Unknown Data Standardization Function: {data_standard.function_name}\"\n                                    )\n                            except Exception as e:\n                                orch_process.insert_data_standardisation_log(\n                                    log_data_standardisation=logDataStandardisationDtl(\n                                        batch_id=unprocessed_file.batch_id,\n                                        process_id=dataset_master.process_id,\n                                        dataset_id=dataset_master.dataset_id,\n                                        source_file=unprocessed_file.source_file,\n                                        data_standardisation_location=dataset_master.data_standardisation_location,\n                                        status=\"FAILED\",\n                                        exception_details=traceback.format_exc(),\n                                        start_datetime=start_time,\n                                        end_datetime=datetime.now(),\n                                    )\n                                )\n\n                        DeltaTableWriter(\n                            input_data=df,\n                            save_location=data_standard_location[\"s3_location\"],\n                            batch_id=unprocessed_file.batch_id,\n                            partition_columns=dataset_master.data_standardisation_partition_columns,\n                        )\n                        orch_process.insert_data_standardisation_log(\n                            log_data_standardisation=logDataStandardisationDtl(\n                                batch_id=unprocessed_file.batch_id,\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                data_standardisation_location=dataset_master.data_standardisation_location,\n                                status=\"SUCCEEDED\",\n                                start_datetime=start_time,\n                                end_datetime=datetime.now(),\n                            )\n                        )\n                    else:\n\n                        DeltaTableWriter(\n                            input_data=df,\n                            save_location=data_standard_location[\"s3_location\"],\n                            batch_id=unprocessed_file.batch_id,\n                            partition_columns=dataset_master.data_standardisation_partition_columns,\n                        )\n                        orch_process.insert_data_standardisation_log(\n                            log_data_standardisation=logDataStandardisationDtl(\n                                batch_id=unprocessed_file.batch_id,\n                                process_id=dataset_master.process_id,\n                                dataset_id=dataset_master.dataset_id,\n                                source_file=unprocessed_file.source_file,\n                                data_standardisation_location=dataset_master.data_standardisation_location,\n                                status=\"SUCCEEDED\",\n                                start_datetime=start_time,\n                                end_datetime=datetime.now(),\n                            )\n                        )\n\n            else:\n                raise Exception(\n                    f\"No Unprocessed files found for SILVER Layer for DatasetID: {dataset_master.dataset_id}\"\n                )\n</code></pre>"},{"location":"SilverLayerScripts/DataStandardization/#datacraft_framework.SilverLayerScripts.DataStandardization.DataStandardization.__init__","title":"<code>__init__(data_standard_detail, dataset_master)</code>","text":"<p>Initializes the DataStandardization process and executes it.</p> <p>Parameters:</p> Name Type Description Default <code>data_standard_detail</code> <code>list[ctlDataStandardisationDtl]</code> <p>A list of standardization rules such as padding, trimming, and type conversion.</p> required <code>dataset_master</code> <code>ctlDatasetMaster</code> <p>Master metadata of the dataset containing details like process ID, dataset ID, and storage locations.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If no unprocessed files are found for the given dataset.</p> <code>Exception</code> <p>If an unsupported function or padding type is encountered.</p> <code>Exception</code> <p>For any failure during the standardization process, details are logged.</p> Source code in <code>src/datacraft_framework/SilverLayerScripts/DataStandardization.py</code> <pre><code>def __init__(\n    self,\n    data_standard_detail: list[ctlDataStandardisationDtl],\n    dataset_master: ctlDatasetMaster,\n):\n    \"\"\"\n    Initializes the DataStandardization process and executes it.\n\n    Args:\n        data_standard_detail (list[ctlDataStandardisationDtl]):\n            A list of standardization rules such as padding, trimming, and type conversion.\n        dataset_master (ctlDatasetMaster):\n            Master metadata of the dataset containing details like process ID, dataset ID,\n            and storage locations.\n\n    Raises:\n        Exception: If no unprocessed files are found for the given dataset.\n        Exception: If an unsupported function or padding type is encountered.\n        Exception: For any failure during the standardization process, details are logged.\n    \"\"\"\n\n    with OrchestrationProcess() as orch_process:\n\n        unprocessed_files = orch_process.get_data_standardisation_unprocessed_files(\n            process_id=dataset_master.process_id,\n            dataset_id=dataset_master.dataset_id,\n        )\n        data_standard_location = path_to_s3(\n            location=dataset_master.data_standardisation_location, env=env\n        )\n        landing_location_ = path_to_s3(\n            location=dataset_master.landing_location,\n            env=env,\n        )\n\n        if len(unprocessed_files) != 0:\n\n            for unprocessed_file in unprocessed_files:\n                start_time = datetime.now()\n                column_meta = orch_process.get_ctl_column_metadata(\n                    dataset_id=dataset_master.dataset_id,\n                )\n                rename_mapping = {\n                    x.source_column_name: x.column_name for x in column_meta\n                }\n\n                df = DeltaTableRead(\n                    delta_path=landing_location_[\"s3_location\"],\n                    batch_id=unprocessed_file.batch_id,\n                ).read()\n                df = df.rename(rename_mapping)\n                df = SchemaCaster(df=df, column_metadata=column_meta).start()\n\n                if len(data_standard_detail) != 0:\n                    for data_standard in data_standard_detail:\n                        try:\n                            if data_standard.function_name == \"padding\":\n                                parsed_json = json_loads(\n                                    data_standard.function_params\n                                )\n                                padding_type = parsed_json[\"type\"]\n                                padding_length = int(parsed_json[\"length\"])\n                                padding_value = parsed_json[\"padding_value\"]\n\n                                if padding_type == \"left\":\n                                    df = df.with_columns(\n                                        polars.col(\n                                            data_standard.column_name\n                                        ).str.pad_start(\n                                            fill_char=padding_value,\n                                            length=padding_length,\n                                        )\n                                    )\n                                elif padding_type == \"right\":\n                                    df = df.with_columns(\n                                        polars.col(\n                                            data_standard.column_name\n                                        ).str.pad_end(\n                                            fill_char=padding_value,\n                                            length=padding_length,\n                                        )\n                                    )\n                                else:\n                                    raise Exception(\n                                        f\"Unsuppored Padding type: {padding_value}\"\n                                    )\n\n                            elif data_standard.function_name == \"trim\":\n                                df = df.with_columns(\n                                    polars.col(\n                                        data_standard.column_name\n                                    ).str.strip_chars()\n                                )\n\n                            elif data_standard.function_name == \"blank_conversion\":\n                                df = df.with_columns(\n                                    polars.col(data_standard.column_name)\n                                    .str.strip_chars()\n                                    .replace(pattern=r\"\\s+\", value=\" \")\n                                )\n\n                            elif data_standard.function_name == \"replace\":\n                                parsed_json = json_loads(\n                                    data_standard.function_params\n                                )\n                                to_replace_pattern = parsed_json[\"value\"]\n                                replacement = parsed_json[\"value\"]\n\n                                df = df.with_columns(\n                                    polars.col(\n                                        data_standard.column_name\n                                    ).str.replace(\n                                        pattern=to_replace_pattern,\n                                        value=replacement,\n                                    )\n                                )\n\n                            elif data_standard.function_name == \"type_conversion\":\n                                parsed_json = json_loads(\n                                    data_standard.function_params\n                                )\n                                type_conversion_type = parsed_json[\"type\"]\n\n                                if type_conversion_type == \"lower\":\n                                    df = df.with_columns(\n                                        polars.col(\n                                            data_standard.column_name\n                                        ).str.to_lowercase()\n                                    )\n                                elif type_conversion_type == \"upper\":\n                                    df = df.with_columns(\n                                        polars.col(\n                                            data_standard.column_name\n                                        ).str.to_lowercase()\n                                    )\n\n                            elif data_standard.function_name == \"sub_string\":\n                                parsed_json = json_loads(\n                                    data_standard.function_params\n                                )\n\n                                start_index = int(parsed_json[\"start_index\"])\n                                length = int(parsed_json[\"length\"])\n\n                                df = df.with_columns(\n                                    polars.col(data_standard.column_name).str.slice(\n                                        offset=start_index,\n                                        length=length,\n                                    )\n                                )\n                            else:\n                                raise Exception(\n                                    f\"Unknown Data Standardization Function: {data_standard.function_name}\"\n                                )\n                        except Exception as e:\n                            orch_process.insert_data_standardisation_log(\n                                log_data_standardisation=logDataStandardisationDtl(\n                                    batch_id=unprocessed_file.batch_id,\n                                    process_id=dataset_master.process_id,\n                                    dataset_id=dataset_master.dataset_id,\n                                    source_file=unprocessed_file.source_file,\n                                    data_standardisation_location=dataset_master.data_standardisation_location,\n                                    status=\"FAILED\",\n                                    exception_details=traceback.format_exc(),\n                                    start_datetime=start_time,\n                                    end_datetime=datetime.now(),\n                                )\n                            )\n\n                    DeltaTableWriter(\n                        input_data=df,\n                        save_location=data_standard_location[\"s3_location\"],\n                        batch_id=unprocessed_file.batch_id,\n                        partition_columns=dataset_master.data_standardisation_partition_columns,\n                    )\n                    orch_process.insert_data_standardisation_log(\n                        log_data_standardisation=logDataStandardisationDtl(\n                            batch_id=unprocessed_file.batch_id,\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            data_standardisation_location=dataset_master.data_standardisation_location,\n                            status=\"SUCCEEDED\",\n                            start_datetime=start_time,\n                            end_datetime=datetime.now(),\n                        )\n                    )\n                else:\n\n                    DeltaTableWriter(\n                        input_data=df,\n                        save_location=data_standard_location[\"s3_location\"],\n                        batch_id=unprocessed_file.batch_id,\n                        partition_columns=dataset_master.data_standardisation_partition_columns,\n                    )\n                    orch_process.insert_data_standardisation_log(\n                        log_data_standardisation=logDataStandardisationDtl(\n                            batch_id=unprocessed_file.batch_id,\n                            process_id=dataset_master.process_id,\n                            dataset_id=dataset_master.dataset_id,\n                            source_file=unprocessed_file.source_file,\n                            data_standardisation_location=dataset_master.data_standardisation_location,\n                            status=\"SUCCEEDED\",\n                            start_datetime=start_time,\n                            end_datetime=datetime.now(),\n                        )\n                    )\n\n        else:\n            raise Exception(\n                f\"No Unprocessed files found for SILVER Layer for DatasetID: {dataset_master.dataset_id}\"\n            )\n</code></pre>"}]}